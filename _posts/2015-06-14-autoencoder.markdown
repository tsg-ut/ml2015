---
layout: post
title:  "#09 オートエンコーダ"
date:   2015-06-14
categories: ml
---

## オートエンコーダ

\#07，\#08で扱った多層パーセプトロンは，このような形状をしていた．

![多層パーセプトロン]({{ site.baseurl }}/images/09/perceptron.001.png)

入力ベクトルとして\\(\boldsymbol{x}\\)を与え，教師ベクトルとして\\(\boldsymbol{z}\\)を与えて，誤差関数を最小化することで学習を行った．
（\#07での説明では教師信号は\\(z\\)というスカラーで説明したが，これをベクトルに拡張するのは難しくない）

ここで，教師ベクトルを次のようにとってみる．

![オートエンコーダ]({{ site.baseurl }}/images/09/perceptron.002.png)

すなわち，教師ベクトル=入力ベクトルである．
このパーセプトロンは，入力ベクトルを与えると出力として自分自身を復元することが期待される．

しかし，目的としているのは入力を復元することではなく，中間層の出力である．
中間層のノード数が入力層よりも少ない状態で入力を復元することができれば，より少ない情報量(=低次元)で入力データを表現できていることになる．
言い換えれば，入力データの冗長性を上手に排除することができる．

![オートエンコーダの中間層]({{ site.baseurl }}/images/09/perceptron.003.png)

このようなパーセプトロンはオートエンコーダ(自己復号化器，AutoEncoder)と呼ばれる．


## オートエンコーダと主成分分析の関係

学習データの次元を削減したいというのは，元々機械学習の分野ではよく行われてきたことである．
数学的には一般次元で問題を定式化して解析的に解くことはできるが，コンピュータサイエンスの視点から見るとたとえ一般次元で定式化されていても次元が大きすぎると有限時間で計算が終わらないので，次元削減したいというのは自然な考えである．
（「次元の呪い，The curse of dimensionality」という言葉がよく用いられる）
オートエンコーダ以外にも線形代数や統計でよく用いられてきた<u>主成分分析(principal component analysis, PCA)</u>という手法を簡単に紹介する．


