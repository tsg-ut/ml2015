---
layout: post
title:  "#10 事前学習"
date:   2015-06-15
categories: ml
---

## 事前学習

\#07，\#08のパーセプトロンの問題点の一つとして，初期値をランダムにとっていたことが挙げられる．
度々になるが，勾配法でランダムな初期値からスタートすると，局所解にトラップされてしまう可能性がある．

この問題の解決策として2006年にGeoffery Hintonによって提案されたのが，オートエンコーダである．
着想は，初期値をランダムに取る代わりに<u>最適な初期値を推定する</u>ことにある．
つまり，初期値を選択する操作自体を一つの推定問題として捉えるのである．

![事前学習]({{ site.baseurl }}/images/10/pretraining1.png)

まずはこの図のように最小の層をオートエンコーダとして見立てて，学習を行う．
このときのオートエンコーダの重みの初期値はランダムにとる．
学習はパーセプトロンと同様にSGD等を用いて行う．

そして，1層目のパラメータの初期値が推定された状態で，残りの層も含めて教師あり学習を行う．
残りの層の初期値はランダムにとる．
これまでのパーセプトロンとの違いは，1層目がオートエンコーダの自己学習を利用して自らパラメータの最適な初期値を学習しているという点である．
このようにオートエンコーダを用いてパーセプトロンの重みの初期値を予め推定しておくことを__事前学習(pre-training)__という．

![事前学習2]({{ site.baseurl }}/images/10/pretraining2.png)

Hintonが初めてオートエンコーダを発表した時は，2000→1000→500→30と3段のオートエンコーダを重ねて，元通りの画像を復元するように学習できた例を示している．
上の例でははじめの1層だけだったが，一般に各層の重みをそれぞれオートエンコーダとみなして事前学習を行うことができる．

【参考】[Reducing the Dimensionality of Data with Neural Networks](http://www.cs.toronto.edu/~hinton/science.pdf)

ちなみに，なぜオートエンコーダを用いるとよい初期値が得られるのか，という点についてはまだ理論的によくわかっていない．

## コードによる実例


