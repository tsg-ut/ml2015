<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TSG Machine Learning</title>
    <description>TSG Machine Learning Substudy Group in 2015
</description>
    <link>http://sig.tsg.ne.jp/ml2015/</link>
    <atom:link href="http://sig.tsg.ne.jp/ml2015/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 26 May 2015 19:06:43 +0900</pubDate>
    <lastBuildDate>Tue, 26 May 2015 19:06:43 +0900</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>#06 線形識別モデル</title>
        <description>&lt;p&gt;今回から本格的に機械学習らしい内容に入っていく．&lt;/p&gt;

&lt;p&gt;機械学習で扱う問題のほとんどは識別問題，すなわち与えられたデータの属性を推定する問題であることが多い．&lt;/p&gt;

&lt;p&gt;(例)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;スパムメール判定（受信したメールがスパムなのかそうでないのか）&lt;/li&gt;
&lt;li&gt;Googleの言語判定（テキストが何語で書かれているのか）&lt;/li&gt;
&lt;li&gt;YouTubeのおすすめ動画&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/spam.png&quot; alt=&quot;スパムフィルタ&quot;&gt;
↑GMailのスパムフィルタ&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/lang.png&quot; alt=&quot;言語判定&quot;&gt;
↑Googleの言語判定&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/youtube.png&quot; alt=&quot;おすすめ動画&quot;&gt;
↑YouTubeのおすすめ動画&lt;/p&gt;

&lt;h2&gt;それは機械学習が必要なのか&lt;/h2&gt;

&lt;p&gt;昨今の機械学習ブームの影響で，とりあえず機械学習を使ってみたいというケースが増えている（僕もそのクチだ）．
ただ，何でもかんでもとりあえず機械学習で識別させておけばいいわけではない．
例えば，自動販売機に投入される硬貨の種類を判定するのにわざわざ機械学習を用いる必要はない．
対象はせいぜい1円玉，5円玉，10円玉，50円玉，100円玉，500円玉の6種類しかないので，rule-basedに記述した方が速い．&lt;/p&gt;

&lt;p&gt;機械学習を用いた方が良いのは，処理すべきデータ規模が人力では扱えないほど大きかったり，ruleが曖昧な場合などである．
（「スパムメールの基準を書き出せ」と言われたら難しいだろう）&lt;/p&gt;

&lt;p&gt;また，機械学習を用いるにしても，徹頭徹尾全て機械学習に頼りきらなければいけない道理はどこにもない．
サイボウズ・ラボの中谷秀洋氏に話を伺う機会があったのだが，言語判定をやっていると異言語間で同じ綴りの単語が使われているケース等で，機械学習ではどうしても乗り越えられない壁に当たる時があるのだが，そういう場合は普通にif文一つかませてしまうと精度が上がるそうだ．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/shuyo/dsirnlp&quot;&gt;言語判定へのいざない - SlideShare&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;線形識別関数&lt;/h2&gt;

&lt;p&gt;それでは前置きから本題に入ろう．テキスト処理の例を据えながら説明する．
ここでは，ブログ記事のトピックが統計に関連するのかしないのかを判定するようなプログラムを作ることを考えてみよう．
まず，記事中に「統計」という言葉が登場していたら機械学習に関連する記事だと判定することにしよう．
これを式として表すなら，「統計」という単語の登場回数を\(x\)として，&lt;/p&gt;

&lt;div&gt;
\[
    f(x)=x-1
\]
&lt;/div&gt;

&lt;p&gt;という識別関数を考え，&lt;/p&gt;

&lt;div&gt;
\begin{align}
    f(x)\ge 0 &amp;\Rightarrow \mathrm{related}\\
    f(x)\lt 0 &amp;\Rightarrow \mathrm{not\,related}
\end{align}
&lt;/div&gt;

&lt;p&gt;というように書き表せる（自明に見えるかもしれないが丁寧に進める）．&lt;/p&gt;

&lt;p&gt;しかし，「統計」という単語が含まれているからといって，必ずしも統計に関する文章であるとは限らない．
（例えば&lt;a href=&quot;http://www.benricho.org/weather_ratio/&quot;&gt;このページ&lt;/a&gt;には「統計」という単語が登場するが，どちらかといえば天気に関する文章だ）
記事の長さは記事ごとによってまちまちなので，記事に出現する全単語数で割った値を用いた方がよいかもしれない（正規化という）．
また，「統計」という単語が登場しなくても「検定」や「有意」という単語が登場しているなら統計に関係あるかもしれない．&lt;/p&gt;

&lt;p&gt;これを踏まえて，次は「統計」「検定」「有意」という単語の登場回数を記事中の全単語数で割った値をそれぞれ\(x_1,x_2,x_3\)とし，&lt;/p&gt;

&lt;div&gt;
\[
    f(x_1,x_2,x_3)=x_1+3x_2+2x_3-r
\]
&lt;/div&gt;

&lt;p&gt;という識別関数を考える．relatedとnot relatedの判別基準は先ほどと同様とする．
3や2といった係数は僕が適当に与えたもので，この場合「検定」や「有意」が登場すると「統計」が登場したときよりもrelatedと判断しやすくなる．&lt;/p&gt;

&lt;p&gt;今ここで考えている識別関数は，記事から\(\{x_i\}_{i=1}^N\)というパラメータを抽出し，それを線形写像（1次関数）で処理している．&lt;/p&gt;

&lt;p&gt;一般化する．&lt;/p&gt;

&lt;p&gt;2クラス問題\(C_1,C_2\)を分類する線形識別関数は次のように書ける．&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+w_0
\]
&lt;/div&gt;

&lt;p&gt;識別の基準は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    C_1 &amp; \;\mathrm{if}\; f(\boldsymbol{x})\ge 0\\
    C_2 &amp; \;\mathrm{if}\; f(\boldsymbol{x})\lt 0
\end{align}
&lt;/div&gt;

&lt;p&gt;とする．&lt;/p&gt;

&lt;p&gt;\(\boldsymbol{x}\)は入力ベクトル，\(\boldsymbol{w}\)は係数ベクトル，\(w_0\)はバイアス項と呼ばれる．&lt;/p&gt;

&lt;p&gt;ここでバイアス項を\(\boldsymbol{w}\)の第(N+1)要素に追加し，\(\boldsymbol{x}\)の第(N+1)要素に1を追加すると&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}
\]
&lt;/div&gt;

&lt;p&gt;と書くことができる．
こちらの方が一般的には扱いやすいので，以降バイアス項は省略する．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/leastsq.png&quot; alt=&quot;二乗誤差&quot;&gt;&lt;/p&gt;

&lt;p&gt;ちなみに，\(\boldsymbol{w}^T\boldsymbol{x}=0\)は\(\boldsymbol{w}\)を法線ベクトルとする超平面の式になる．
つまり\(\boldsymbol{w}^T\boldsymbol{x}\)はベクトル\(\boldsymbol{x}\)の超平面からの距離になる．&lt;/p&gt;

&lt;h2&gt;学習と推定&lt;/h2&gt;

&lt;p&gt;上記の説明では，人間側が適当に係数をいじっていたが，実際に「検定」を重視したほうがいいのか，「有意」を重視したほうがいいのか，ということはデータを見なければわからない．
したがって，コンピュータに大量の「あらかじめ統計に関連するかどうかわかっている」データを処理させ，最適な係数を計算によって求めさせる．
これが機械学習のうち，学習のフェーズになる．&lt;/p&gt;

&lt;p&gt;学習さえしてしまえば推定は簡単で，「統計に関連するかどうか知りたい」記事のデータについて\(f(\boldsymbol{x})\)の値を計算して，上記の識別の基準にしたがって判断すればよい．&lt;/p&gt;

&lt;p&gt;ここで問題になるのは，&lt;u&gt;どのようにしてデータから最適なパラメータを計算するか&lt;/u&gt;ということになる．ここではシンプルな評価関数として二乗誤差を考える．&lt;/p&gt;

&lt;h2&gt;最小二乗誤差基準&lt;/h2&gt;

&lt;p&gt;上の例では識別関数が正のクラスが正の値のときは\(C_1\)，負の値のときは\(C_2\)に分類される．
ここで行いたいのは，学習データをコンピュータに与えて，最適なパラメータ\(\boldsymbol{w}\)を計算することである．
これを以下のように行う．&lt;/p&gt;

&lt;div style=&quot;border: solid 1px; padding: 20px; margin: 10px;&quot;&gt;
学習データに対して，\(C_1\)ならば+1，\(C_2\)ならば-1を返すように\(f(\boldsymbol{x})\)を設計する．
そのために二乗誤差\((t_i-f(\boldsymbol{x_i}))^2\)を計算し，その総和が最小になるような\(\boldsymbol{w}\)を計算する．
(x_iはi番目の学習データ．t_iはi番目の学習データのラベル値で，{1,-1})
&lt;/div&gt;

&lt;p&gt;すなわち，最小化すべきは&lt;/p&gt;

&lt;div&gt;
\[
    J(\boldsymbol{w})=\sum_{i=1}^K(t_i-\boldsymbol{w}^T\boldsymbol{x_i})^2
\]
&lt;/div&gt;

&lt;p&gt;この式は次のように書きなおすことができる．&lt;/p&gt;

&lt;div&gt;
\[
    J(\boldsymbol{w})=\|\boldsymbol{t}-X\boldsymbol{w}\|^2 \tag{6-1}
\]
&lt;/div&gt;

&lt;p&gt;ここで\(\boldsymbol{t}=(t_1 t_2 \cdots t_K)^T\)，&lt;/p&gt;

&lt;div&gt;
\[
    X = \left(\begin{array}{c}
        \boldsymbol{x_1}^T \\
        \boldsymbol{x_2}^T \\
        : \\
        \boldsymbol{x_K}^T
    \end{array}\right)
\]
&lt;/div&gt;

&lt;h3&gt;最適解&lt;/h3&gt;

&lt;p&gt;今一度解くべき問題を整理すると，評価関数(6-2)\(J(\boldsymbol{w})\)の最小化である．
ここでいきなり最急降下法にかける前に，式の上で計算してみる．&lt;/p&gt;

&lt;p&gt;まず\(J(\boldsymbol{w})\)を微分すると&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial J}{\partial\boldsymbol{w}}
    &amp;= \frac{\partial}{\partial\boldsymbol{w}}(\boldsymbol{w}^TX^TX\boldsymbol{w}-\boldsymbol{w}X^T\boldsymbol{t}-\boldsymbol{t}^TX\boldsymbol{w}+\|\boldsymbol{t}\|^2) \\
    &amp;= 2X^TX\boldsymbol{w}-2X^T\boldsymbol{t} \\
\end{align}
&lt;/div&gt;

&lt;p&gt;ここでは制約条件はついていないので，単純に導関数=0とおくと&lt;/p&gt;

&lt;div&gt;
\[
    X^TX\boldsymbol{w}=X^T\boldsymbol{t}
\]
&lt;/div&gt;

&lt;p&gt;これは&lt;u&gt;正規方程式&lt;/u&gt;と呼ばれている．
したがって&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{w}=(X^TX)^{-1}X^T\boldsymbol{t}
\]
&lt;/div&gt;

&lt;p&gt;という解が得られる．&lt;/p&gt;

&lt;h2&gt;簡単な例&lt;/h2&gt;

&lt;p&gt;初めにインストールしたモジュールの中に&lt;code&gt;scikit-learn&lt;/code&gt;というモジュールがある．
これはPython向けの機械学習モジュールで，様々なアルゴリズムが提供されていたり，サンプルデータが提供されている．
ここではiris(アヤメ)のサンプルデータを用いて，最小二乗学習を行ってみる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/load.png&quot; alt=&quot;irisのロード&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sklearn.datasets.load_iris&lt;/code&gt;でirisをロードできる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/data.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;データはこのような4次元データになっている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/feature_name.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;各次元はこのような特徴を表現している．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/target.png&quot; alt=&quot;target&quot;&gt;&lt;/p&gt;

&lt;p&gt;各データのラベル値はこのようになっている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/target_name.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ラベル値はそれぞれsetosa, versicolor, virginicaというアヤメの種類と対応している．&lt;/p&gt;

&lt;p&gt;現時点では2クラス分類しかできないので，「setosaかそうでないか」を推定するプログラムを書いてみる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 最小二乗法で学習&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 最小二乗法で推定&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# テストデータに対する正答率&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/iris.png&quot; alt=&quot;例&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;の行はirisを(学習データ):(テストデータ)=4:1に分割している(test_size=0.2)．&lt;code&gt;train_x, train_y&lt;/code&gt;を使って\(\boldsymbol{w}\)を推定し，できた識別器に&lt;code&gt;test_x&lt;/code&gt;をかけて&lt;code&gt;pred_y&lt;/code&gt;を&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;で得て，そのうち何%が&lt;code&gt;test_y&lt;/code&gt;と一致しているかを&lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt;を用いて&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;の行で計算している．&lt;/p&gt;

&lt;p&gt;この程度であれば100%の精度を達成することができる．&lt;/p&gt;
</description>
        <pubDate>Sun, 24 May 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/24/linear-recognition-model.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/24/linear-recognition-model.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#05 確率分布の基礎</title>
        <description>&lt;p&gt;確率は統計解析と密接に関係している．
例えば，株価のトレードデータから数日後の株価を予測する際，過去の統計データをもとに確率的に最もありえる株価でもって予測したりする．
また，患者の状態を観測してそこから医者が&amp;quot;最も取るべき行動&amp;quot;を推論し，医療に応用するといったこともなされる．&lt;/p&gt;

&lt;p&gt;本節では，確率解析のベースとなる確率分布の基礎を説明する．&lt;/p&gt;

&lt;h2&gt;二項分布&lt;/h2&gt;

&lt;p&gt;確率分布の最も単純な例として，コイン投げを考える．
確率\(p=\frac{1}{2}\)で表，\(q=1-p=\frac{1}{2}\)で裏が出るようなコインを考える．
確率変数\(x\)に対して「表が出る」事象を1，「裏が出る」事象を0とする．
コインを\(n\)回投げるとき，表が出る回数が\(k\)回になる確率は以下のように表される．&lt;/p&gt;

&lt;div&gt;
\[
    P(x=k) = {}_nC_k p^k(1-p)^{n-k} = \frac{n!}{k!(n-k)!} p^k(1-p)^{n-k}
\]
&lt;/div&gt;

&lt;p&gt;（高校で学ぶ反復事象の確率そのものである）&lt;/p&gt;

&lt;p&gt;このような確率変数\(x\)の分布はパラメータ\(n,p\)で決定されるため，二項分布\(B(n,p)\)に従うといい\(x\sim B(n,p)\)と書く．&lt;/p&gt;

&lt;p&gt;二項分布をとるのは，試行がベルヌーイ試行（試行の結果が二値）であり，互いに独立であるような場合である．
上の例ではコイントスが試行であり，コイントス自体は表と裏の二値で，一回前のトスの結果は次のトスには影響しない．&lt;/p&gt;

&lt;h3&gt;例1&lt;/h3&gt;

&lt;p&gt;以下のスクリプトは二項分布に従って確率変数をランダムに生成している．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;コイントスの例で言えば，「表裏が出る確率の等しい(p=0.5)コインを20回(=n)投げる」という試行を10回行っている．
実行例は以下のとおり．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;array([12, 11,  9,  9,  7, 10, 10, 10,  9, 11])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;このように，だいたい10回前後表が出ていることがわかる．&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;scipy.stats&lt;/code&gt;には他にも様々な確率分布に関するモジュールが提供されている．
また，各々のモジュールは統一的なインターフェースで提供されている（例えば，他の確率分布に従って確率変数をランダム生成する場合も&lt;code&gt;rvs&lt;/code&gt;を使う）．
一度&lt;a href=&quot;http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.binom.html&quot;&gt;scipy.stats.binom&lt;/a&gt;のドキュメントのMethodsの欄に目を通してみるといいかもしれない．&lt;/p&gt;

&lt;h3&gt;正規性&lt;/h3&gt;

&lt;p&gt;確率分布は「すべて足して1」にならなければならない．それを二項分布で確かめる．&lt;/p&gt;

&lt;div&gt;
\[
    \sum_{k=0}^n{}_nC_kp^k(1-p)^{n-k} = \{p+(1-p)\}^n = 1
\]
&lt;/div&gt;

&lt;p&gt;この計算には二項定理を用いている．&lt;/p&gt;

&lt;h3&gt;期待値と分散&lt;/h3&gt;

&lt;p&gt;二項分布の期待値は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \mathbb{E}(x)
        &amp;= \sum_{k=0}^n kP(k) \\
        &amp;= \sum_{k=0}^n k\cdot\frac{n\cdot(n-1)!}{k\cdot(k-1)!(n-k)!}p^k(1-p)^{n-k} \\
        &amp;= \sum_{k=0}^n k\cdot\frac{n}{k}\cdot{}_{n-1}C_{k-1}p^k(1-p)^{n-k} \\
        &amp;= np\sum_{k=0}^n {}_{n-1}C_{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)} \\
        &amp;= np\cdot\{p+(1-p)\}^n \\
        &amp;= np
\end{align}
&lt;/div&gt;

&lt;p&gt;となる．表裏の出る確率がイーブンなコインを10回投げるとだいたい5回(=10・(1/2))表になる直感にそっている．&lt;/p&gt;

&lt;p&gt;分散は&lt;/p&gt;

&lt;div&gt;
\[
    \mathrm{Var}(x)=np(1-p)
\]
&lt;/div&gt;

&lt;p&gt;である．&lt;/p&gt;

&lt;h3&gt;練習1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;二項分布の分散を計算せよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;期待値と似たような計算で確認できる．&lt;/p&gt;

&lt;h2&gt;正規分布と中心極限定理&lt;/h2&gt;

&lt;p&gt;二項分布の試行回数\(n\)を十分大きくしていくと，どうなるだろうか．&lt;/p&gt;

&lt;p&gt;コイントスの例で考える．トスの回数\(n\)を1回から100回まで増やしていく．
各\(n\)において，同じ試行を10000回行い，表の出た回数をヒストグラムにとってみる．
これは以下のようなスクリプトで実現できる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 二項分布の生成&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# キャンバスのクリア&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ヒストグラムの描画（binsはヒストグラムの分割数、あまり気にしなくて良い）&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;n = {}&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;fig{}.png&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;この結果得られたプロットをGIFアニメーションにしたのが下の画像である．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/05/binom.gif&quot; alt=&quot;二項分布の近似&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このように，二項分布の形状が次第に一定のカーブに近づいていく．
この分布は正規分布と呼ばれ，次のような式で表される．&lt;/p&gt;

&lt;div&gt;
\[
    P(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\]
&lt;/div&gt;

&lt;p&gt;\(\mu\)は平均（＝期待値），\(\sigma\)は分散である．&lt;/p&gt;

&lt;p&gt;実際に\(n=100\)のヒストグラムに正規分布を重ねてプロットすると以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/05/normal.png&quot; alt=&quot;正規分布&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このように&lt;strong&gt;二項分布のもとで試行を繰り返すと，その和の分布は次第に正規分布に近づく．&lt;/strong&gt;
この事実は中心極限定理と呼ばれる．中心極限定理の証明は難しいので割愛する．
中心極限定理の存在により，大標本の場合にはまず正規分布を仮定してデータ解析することも多く，正規分布は数ある確率分布でも特殊な位置を占めている．&lt;/p&gt;

&lt;h3&gt;練習2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;サイコロを振って1〜6の目が出る確率は（イカサマされていないという前提で）すべて等しく1/6である．
この分布は
$$ P(x)=\frac{1}{6} $$
と書け，一様分布という．
一様分布も二項分布と同様に，正規分布に収束することをスクリプトで確認せよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;収束はそれなりに速く，\(n=10\)程度でかなり正規分布に近い形になる．&lt;/p&gt;

&lt;p&gt;スクリプト例を&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/05/uniform.py&quot;&gt;uniform.py&lt;/a&gt;に示す．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/05/uniform.gif&quot;&gt;uniform.gif&lt;/a&gt;のような結果になれば概ねOKだろう．&lt;/p&gt;
</description>
        <pubDate>Tue, 12 May 2015 02:14:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/12/probability.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/12/probability.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#04 最適化手法</title>
        <description>&lt;p&gt;最適化とは簡単に言うと関数の最大値・最小値を求めることである．
コンピュータサイエンスの応用分野では最適化手法が幅広く用いられている．
機械学習の例で説明すると，例えばデータを学習して&amp;quot;最適な&amp;quot;パラメータを決定するというのは，結局データを引数として取る評価関数を最大化することに他ならない．&lt;/p&gt;

&lt;p&gt;しかし，コンピュータで最大値・最小値を計算するのは思ったほど簡単なことではない．
例えば人間なら導関数の零点を見つける際は単純に方程式を解くだけだが，コンピュータに方程式をただ投げても適切なアルゴリズムがなければ解は返ってこない．&lt;/p&gt;

&lt;p&gt;この節では，以下の2つのアルゴリズムを説明する．&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Lagrangeの未定乗数法&lt;/p&gt;

&lt;p&gt;制約条件付きの最適化問題を解く，数理解析的アルゴリズム．
コンピュータが直接的に扱うのは難しいが，様々な最適化アルゴリズムのベースになっている．&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最急降下法&lt;/p&gt;

&lt;p&gt;関数の最大値・最小値を，その導関数を用いて探索的に求めるアルゴリズム．
コンピュータにとって扱いやすく，アルゴリズム自体もシンプル（故に問題点も多い）．&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Lagrangeの未定乗数法&lt;/h2&gt;

&lt;p&gt;1変数関数の制約条件付き最大値・最小値を求める際は，導関数の零点から極値をとる値を求め，実際に極値同士の値を比較して最大値・最小値を求めていた．&lt;/p&gt;

&lt;p&gt;多変数関数の場合は，&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f(\boldsymbol{a})}{\partial x_1} = 0,\cdots,\frac{\partial f(\boldsymbol{a})}{\partial x_n} = 0
\]
&lt;/div&gt;

&lt;p&gt;だからといって\(f(\boldsymbol{a})\)が極値になっているとは限らない．&lt;/p&gt;

&lt;p&gt;例えば\(f(x,y)=x^2+y^3\)のグラフをプロットすると次のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/critical_point.png&quot; alt=&quot;臨界点&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;この関数の\((x,y)=(0,0)\)では，\(x,y\)ともに偏導関数は\(0\)になっているが，極値にはなっていない．
（極値であるかどうかに関わらず，偏導関数がすべて\(0\)になるような点を停留点，または臨界点という）&lt;/p&gt;

&lt;p&gt;1変数の場合でも極値にならない停留点は存在する（例えば\(f(x)=x^3\)の\(x=0\)）が，1変数関数の場合は増減表を書いて確かめることができる．
しかし多変数関数では増減表が書けない．ここに制約条件が付いてくると更に複雑になる．&lt;/p&gt;

&lt;p&gt;このような多変数関数の制約条件付き最大最小問題を解くにはLagrangeの未定乗数法が用いられる．&lt;/p&gt;

&lt;h3&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;制約条件\(g(\boldsymbol{x})=0\)の下で多変数関数\(f(x)\)の極値を与える\(\boldsymbol{x}\)は，次のような関数（Lagrange関数という）&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(\boldsymbol{x},\lambda) = f(\boldsymbol{x})+\lambda g(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;に対して，以下の連立方程式の解として与えられる．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial \tilde{f}}{\partial\boldsymbol{x}} &amp;= 0 \\
    \frac{\partial \tilde{f}}{\partial\lambda} &amp;= 0
\end{align}
&lt;/div&gt;

&lt;h3&gt;例1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;maximize \(f(x,y)=2x+3y\) s.t. \(x^2+y^2=1\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;次のようなLagrange関数を作る．&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(x,y,\lambda)=2x+3y+\lambda(x^2+y^2-1)
\]
&lt;/div&gt;

&lt;p&gt;続いて\(\tilde{f}\)を\(x,y,\lambda\)でそれぞれ偏微分して「=0」とする．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial\tilde{f}}{\partial x} &amp;= 2+2\lambda x = 0 \\
    \frac{\partial\tilde{f}}{\partial y} &amp;= 3+2\lambda y = 0 \\
    \frac{\partial\tilde{f}}{\partial\lambda} &amp;= x^2+y^2-1 = 0 \\
\end{align}
&lt;/div&gt;

&lt;p&gt;この方程式から\(\lambda\)を消去すると&lt;/p&gt;

&lt;div&gt;
\[
    (x,y) = (\pm\frac{2}{\sqrt{13}},\pm\frac{3}{\sqrt{13}})
\]
&lt;/div&gt;

&lt;p&gt;という解が得られる．これらの組は最大値を与える変数の候補になっている．
実際に\(f(x,y)\)に代入してみると，\((x,y)=(\frac{2}{\sqrt{13}},\frac{3}{\sqrt{13}})\)のときに最大値\(\sqrt{13}\)をとり，
\((x,y)=(-\frac{2}{\sqrt{13}},-\frac{3}{\sqrt{13}})\)のときに最小値\(-\sqrt{13}\)をとることがわかる．&lt;/p&gt;

&lt;p&gt;実際にグラフをプロットしてみると以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/lagrange.png&quot; alt=&quot;未定乗数法の例&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;練習1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;例1のプロットをPythonを使って作成してみよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;/ml2015/ml/2015/05/10/multivariate-analysis.html&quot;&gt;#03&lt;/a&gt;や&lt;a href=&quot;http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html&quot;&gt;matplotlibのチュートリアル&lt;/a&gt;を参考にするとよい．
3D空間で線を描画するには&lt;code&gt;Axes3D.plot&lt;/code&gt;，点(散布図)を描画するには&lt;code&gt;Axes3D.scatter&lt;/code&gt;を用いる．&lt;/p&gt;

&lt;p&gt;（スクリプト例: &lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex1.py&quot;&gt;ex1.py&lt;/a&gt;）&lt;/p&gt;

&lt;h3&gt;練習2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;半径1の球に内接する直方体の体積の最大値を求めよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;解答は&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex2.md&quot;&gt;ex2.md&lt;/a&gt;に示す．&lt;/p&gt;

&lt;h3&gt;Lagrangeの未定乗数法の原理&lt;/h3&gt;

&lt;p&gt;厳密な証明は難しく，この分科会の内容から少々逸脱してしまうので，ここでは割愛する．
イメージを掴みたい人は，&lt;a href=&quot;https://github.com/levelfour/machine-learning-2014/wiki/%E7%AC%AC4%E5%9B%9E---Lagrange%E3%81%AE%E6%9C%AA%E5%AE%9A%E4%B9%97%E6%95%B0%E6%B3%95#%E8%A8%BC%E6%98%8E&quot;&gt;昨年の分科会の資料&lt;/a&gt;を参考にするとよい．&lt;/p&gt;

&lt;h3&gt;複数制約条件の場合&lt;/h3&gt;

&lt;p&gt;ここでは制約条件が1つの場合のみについて述べたが，制約条件が\(M\)個（\(\{g_m(\boldsymbol{x})=0\}_{m=1}^M\)）ある場合は&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(\boldsymbol{x},\boldsymbol{\lambda}) = f(\boldsymbol{x})+\sum_{m=1}^M\lambda_mg_m(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;というLagrange関数をつくり，以下の連立方程式を解く．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial \tilde{f}}{\partial\boldsymbol{x}} &amp;= 0 \\
    \frac{\partial \tilde{f}}{\partial\boldsymbol{\lambda}} &amp;= 0
\end{align}
&lt;/div&gt;

&lt;h3&gt;不等式制約条件の場合&lt;/h3&gt;

&lt;p&gt;ここで述べたのは制約条件が等式の場合のみであった．制約条件が不等式になっているケースも実用上は多く登場する．
その場合は少しだけ複雑になる（Karush-Kuhn-Tucker条件の導入）ので，実際に用いる際に説明する．&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;h2&gt;最急降下法&lt;/h2&gt;

&lt;p&gt;最急降下法は，関数の導関数の値を用いて逐次的に関数値を最大にする解を更新するアルゴリズムである．&lt;/p&gt;

&lt;p&gt;関数\(f(\boldsymbol{x})\)に対して初期値\(x_0\)を与えて，&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}_{n+1}=\boldsymbol{x}_{n}+\eta\frac{\partial f(\boldsymbol{x}_{n})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;p&gt;で\(\boldsymbol{x}\)の値を逐次的に更新し，収束した点が最大になっている．
導関数の値が0になったら収束だが，実際にちょうど0になる時点まで探索し続けると収束が非常に遅くなるので，導関数値が十分に小さくなったら収束と見なす．
イメージとしては，関数値が増大する向き（＝導関数が正）に山を登ることになる．&lt;/p&gt;

&lt;p&gt;\(\eta(&amp;gt;)0\)は学習率で，収束の仕方を決めるパラメータである（大きいほど収束が速いわけではない）．&lt;/p&gt;

&lt;p&gt;最急降下法の更新の過程の様子は以下のアニメーションを参考にしてほしい．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/descent.gif&quot; alt=&quot;最急降下法&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最急降下法で最小値を求める際は，更新式を以下のように変更すればよい．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}_{n+1}=\boldsymbol{x}_n-\eta\frac{\partial f(\boldsymbol{x}_{n})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;h3&gt;最急降下法の問題点&lt;/h3&gt;

&lt;h4&gt;初期値依存性&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/overfit.png&quot; alt=&quot;初期値依存性&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上に示す図のように，最急降下法では初期値の取り方によって収束先が変わり得る．
それは，最急降下法は大域的な最大値を求めているのではなく，局所的最大値（すなわち極大値）を求めているからにすぎない．
そのため，実際には解析対象となる関数の特性を把握しつつ初期値を選択したり，複数の初期値に対して試行する必要がある．&lt;/p&gt;

&lt;h4&gt;収束速度&lt;/h4&gt;

&lt;p&gt;最急降下法は原理がシンプルな反面，収束速度が遅いことで知られる．
学習率\(\eta\)の選び方で収束速度は制御できるが，あまり大きな値にしすぎると見当違いな値に収束することもあり，あまり小さな値にしすぎると収束精度は向上するが収束速度は遅くなるといった問題がある．&lt;/p&gt;

&lt;h3&gt;例2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;#03の例1でプロットした下の関数の最大値を最急降下法で求める．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x}) = \exp\left(-\frac{(x-0.5)^2+(y-0.2)^2}{2}\right)
\]
&lt;/div&gt;

&lt;p&gt;この関数を微分すると以下のようになる．&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f}{\partial\boldsymbol{x}}
    = \left(\begin{array}{c}
        0.5-x \\
        0.2-y
    \end{array}\right) f(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;最大値を求めるPythonコードは以下のように書ける．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 学習率&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 反復回数&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 初期値ベクトル&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ini&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ini&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;x({}) = {}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ちなみに，このスクリプトのようにコメントにUnicode文字を使いたい場合は，ファイルの先頭に&lt;code&gt;# -*- coding: utf-8 -*-&lt;/code&gt;または&lt;code&gt;# coding: utf-8&lt;/code&gt;と書く必要がある．
この1行によって，Python処理系はUTF-8でスクリプトを解釈するようになる．&lt;/p&gt;

&lt;h3&gt;練習3&lt;/h3&gt;

&lt;p&gt;例2のスクリプトを実際に動かしてみよ．また，学習率，反復回数，初期値ベクトルを変化させると収束先の値がどのように変化するか，いろいろ試してみよ．&lt;/p&gt;

&lt;h3&gt;練習4&lt;/h3&gt;

&lt;p&gt;例2で，収束するまでの反復回数が最も少なくなるような学習率を探索せよ．0から2の間で試せば十分である．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex4.py&quot;&gt;ex4.py&lt;/a&gt;にスクリプト例を示す．&lt;/p&gt;
</description>
        <pubDate>Mon, 11 May 2015 21:11:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/11/lagrange-multiplier.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/11/lagrange-multiplier.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#03 多変数解析</title>
        <description>&lt;h2&gt;多変数関数&lt;/h2&gt;

&lt;p&gt;1変数関数は\(x\)に対して\(y\)を一意に対応付ける関係のことであった．この関係を\(y=f(x)\)等と書いた．&lt;/p&gt;

&lt;p&gt;多変数関数は，\(n\)個の変数\(x_1,x_2,\cdots,x_n\)を\(y\)に対応づける関係のことであり，
これを\(y=f(x_1,x_2,\cdots,x_n)\)，または\(y=f(\boldsymbol{x})\)と書いたりする．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x} = (x_1 x_2 \cdots x_n)^{\mathrm{T}} \longmapsto f(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;というようにベクトル\(\boldsymbol{x}\)と\(f(\boldsymbol{x})\)の対応付けとして見ることもできる．
こちらの見方の方が後々微分等で直感的に理解しやすいという利点がある．&lt;/p&gt;

&lt;h4&gt;例1&lt;/h4&gt;

&lt;p&gt;多変数関数&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x}) = \exp\left(-\frac{(x-0.5)^2+(y-0.2)^2}{2}\right)
\]
&lt;/div&gt;

&lt;p&gt;をプロットする．&lt;code&gt;matplotlib&lt;/code&gt;を用いると，以下のようなコードでプロットできる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mpl_toolkits.mplot3d&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_wireframe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/ml2015/images/03/gaussian.png&quot; alt=&quot;例1&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;偏微分&lt;/h2&gt;

&lt;p&gt;1変数関数と同様に多変数関数でも微分を行うが，多変数関数では引数が複数個あるので，どの変数で微分するかで導関数が変化する．&lt;/p&gt;

&lt;p&gt;関数\(f(x_1,\cdots,x_n)\)の引数のうち\(x_i\)以外を固定（定数と見なす）し，\(x_i\)で微分したときの導関数を&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f}{\partial x_i}
\]
&lt;/div&gt;

&lt;p&gt;と書き，偏導関数という．&lt;/p&gt;

&lt;h2&gt;ベクトルによる微分&lt;/h2&gt;

&lt;p&gt;多変量解析では&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial(\boldsymbol{a}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;p&gt;のようにベクトルで微分を行う操作が式変形の過程で登場することがある．
これは以下の略記である．&lt;/p&gt;

&lt;div&gt;
\begin{align}   
    \frac{\partial(\boldsymbol{a}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{x}}
    &amp;= \frac{\partial(a_1x_1+\cdots+a_nx_n)}{\partial \left(\begin{array}{c} x_1 \\ : \\ x_n \end{array}\right)} \\
    &amp;= \left(\begin{array}{c} \frac{\partial}{\partial x_1}(a_1x_1+\cdots+a_nx_n) \\ : \\ \frac{\partial}{\partial x_n}(a_1x_1+\cdots+a_nx_n) \end{array}\right) \\
    &amp;= \left(\begin{array}{c} a_1 \\ : \\ a_n \end{array}\right) \\
    &amp;= \boldsymbol{a}
\end{align}
&lt;/div&gt;

&lt;p&gt;このように，ベクトルで微分する場合も結果はスカラーの場合から連想される直感的な結果になる．&lt;/p&gt;

&lt;p&gt;2次形式&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}^{\mathrm{T}}\boldsymbol{Bx}=\sum_{i=1}^n\sum_{j=1}^n b_{ij}x_ix_j
\]
&lt;/div&gt;

&lt;p&gt;の微分も頻出なので，結果のみ示しておく．興味のある人は手を動かして計算してみるとよい．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial\boldsymbol{x}^{\mathrm{T}}\boldsymbol{Bx}}{\partial\boldsymbol{x}}
    &amp;= (\boldsymbol{B}+\boldsymbol{B}^{\mathrm{T}})\boldsymbol{x} \\
    &amp;= 2\boldsymbol{Bx}^{\mathrm{T}} \; (\boldsymbol{B}:\mathrm{symmetric\,matrix})
\end{align}
&lt;/div&gt;
</description>
        <pubDate>Sun, 10 May 2015 00:47:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/10/multivariate-analysis.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/10/multivariate-analysis.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#02 環境構築</title>
        <description>&lt;h2&gt;統計解析とプログラミング&lt;/h2&gt;

&lt;p&gt;統計処理とプログラミングは切っても切り離せない関係にある。
プログラミングと言っても、この分野ではGUIで華やかなアプリケーションを作る技術やアセンブリをバリバリ読むということは勿論無く、単純に計算したい数式を記述するツールとして用いられることがほとんどだ。
そのため、プログラミング初心者の人も臆する必要はない（むしろプログラミングの観点から見れば難易度は低いので、プログラミングの勉強としても良いはずだ）。&lt;/p&gt;

&lt;p&gt;統計解析によく用いられるプログラミング言語・ツールには以下のようなものがある（levelfourのバイアスがかかっている）。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Excel: 言わずもがな。&lt;s&gt;方眼紙として使わなければ最強なんだけどなあ&lt;/s&gt;&lt;/li&gt;
&lt;li&gt;MatLab: 数値計算の王道ソフトウェア。ライセンスも高いので研究室向け。&lt;/li&gt;
&lt;li&gt;R: GNU発のフリーの数値解析ソフトウェア。フリーなのに高機能。&lt;/li&gt;
&lt;li&gt;Python: 汎用プログラミング言語だが、数値計算ライブラリが充実。&lt;/li&gt;
&lt;li&gt;(Julia: 2012年に初公開された、LLVMベースの高速数値計算向け汎用プログラミング言語)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この分科会ではPythonを用いることにする。統計解析だけに絞るのであれば確かにR言語の方が（特に検定回りで）ライブラリが充実しているのだが、Pythonの方が汎用プログラミング言語ともあって、他のツールの助力を借りること無くファイルアクセスしたりWeb公開できるのが強みだ。
世界的に見てもこの流れがあるようで、最近はR言語を始めるならPythonをやろうとよく言われる。&lt;/p&gt;

&lt;h2&gt;Pythonの統計解析用環境&lt;/h2&gt;

&lt;p&gt;一般的に以下のようなライブラリ群が用いられる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;numpy: 行列計算&lt;/li&gt;
&lt;li&gt;scipy: 科学計算（フィッティング、連立方程式、特殊関数、微分積分など）&lt;/li&gt;
&lt;li&gt;matplotlib: グラフプロットツール&lt;/li&gt;
&lt;li&gt;pandas: データの読み書き、表の作成（Pythonで動くExcelのような感じ）&lt;/li&gt;
&lt;li&gt;scikit-learn: 機械学習&lt;/li&gt;
&lt;li&gt;IPython(Jupyter): 高機能インタラクティブPythonシェル&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この回では、Python自体の処理系のインストールと、上記ライブラリのインストールを目指す。&lt;/p&gt;

&lt;h2&gt;pyenvのインストール&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;pyenv&lt;/code&gt;を入れておくとよい。仮想環境を使うと&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意のバージョンのPythonを簡単に入れられる&lt;/li&gt;
&lt;li&gt;システムのPythonを破壊せずにとっておける&lt;/li&gt;
&lt;li&gt;Sandbox環境を簡単に作成&amp;amp;破棄できる&lt;/li&gt;
&lt;li&gt;後述のAnacondaもあっという間にインストール&lt;/li&gt;
&lt;li&gt;Ma OS Xだと&lt;code&gt;brew install pyenv&lt;/code&gt;で入れられるっぽい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;といった風にメリットづくしなので、オススメする。ちなみに同じバージョンのPythonを複数入れたいという需要があるかもしれないが、そんなときには&lt;code&gt;virtualenv&lt;/code&gt;を用いる。&lt;/p&gt;

&lt;p&gt;以下OSごとのインストール方法を説明する。levelfourはMac OS 10.9, 10.10, Ubuntu 14.04でのインストールを確認した。
また、2014年度の分科会で他のメンバーによりWindowsでのインストールも確認された。&lt;/p&gt;

&lt;h4&gt;Windows&lt;/h4&gt;

&lt;p&gt;&lt;s&gt;Cygwinで以下のページの通りに作業するとインストールできることを確認。&lt;/s&gt;&lt;/p&gt;

&lt;p&gt;&lt;s&gt;&lt;a href=&quot;http://qiita.com/la_luna_azul/items/3f64016feaad1722805c&quot;&gt;pyenvとvirtualenvのインストールと使い方 - Qiita&lt;/a&gt;&lt;/s&gt;&lt;/p&gt;

&lt;p&gt;Windowsだと&lt;strong&gt;pyenvからanacondaをうまくインストールできない&lt;/strong&gt;ようなので、後述のAnaconda Installerを使用するとよい。&lt;/p&gt;

&lt;h4&gt;Mac OS X&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ brew install pyenv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Linux&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/yyuu/pyenv-installer&quot;&gt;pyenv-installer&lt;/a&gt;を用いるとインストールできる。
（Ubuntu 14.04で確認）&lt;/p&gt;

&lt;h3&gt;pyenvの使い方（概略）&lt;/h3&gt;

&lt;p&gt;インストール終了後に標準出力に.bashrcに環境変数設定を追記するように促されていると思うので、その通りに従う。&lt;/p&gt;

&lt;h4&gt;インストール可能バージョンを見る&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv install -l
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;インストールする&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv install [what-you-want]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;インストール済バージョン一覧を参照&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv versions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;環境の切り替え&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv global [environment]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ちなみに&lt;code&gt;global&lt;/code&gt;でシステム全体で使用するPythonのバージョン、&lt;code&gt;local&lt;/code&gt;で&lt;strong&gt;そのディレクトリより下層&lt;/strong&gt;で使用するPythonのバージョンを設定することが出来る。&lt;/p&gt;

&lt;p&gt;【参考】&lt;a href=&quot;http://qiita.com/la_luna_azul/items/3f64016feaad1722805c&quot;&gt;pyenvとvirtualenvのインストールと使い方 - Qiita&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Anacondaのインストール&lt;/h2&gt;

&lt;p&gt;pyenvをインストールできたところで、次は上で挙げた数値計算ライブラリ群をインストールする。
しかし、これらライブラリのインストールには非常に手間がかかり、ビルドエラーが起こって解決が困難になることも多い。
そこで、Pythonの処理系と有用なライブラリ群をまとめたパッケージである&lt;a href=&quot;http://continuum.io/downloads#all&quot;&gt;Anaconda&lt;/a&gt;を利用する。
pyenvがインストールされていれば、Anacondaのインストールは非常に簡単である。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv install anaconda3-2.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;このインストールには20分〜30分程度の時間を要するので注意すること。
インストールの確認には&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv versions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;を実行した際にanaconda3-2.1.0が表示されるかを確認すればよい。
各種ライブラリ群が正常動作していることを確認するために、Anaconda環境に切り替える。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv global anaconda3-2.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;そしてPythonを立ち上げる。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以下の行を一行ずつ入力し、エラーが起こらなければ成功である。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Windows等pyenvが正常に動作しない環境の場合&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://continuum.io/downloads#all&quot;&gt;Anacondaのサポートページ&lt;/a&gt;からAnaconda installerをダウンロードしてインストールするのが簡単だと思われる。&lt;/p&gt;

&lt;h2&gt;Pythonの基礎知識&lt;/h2&gt;

&lt;p&gt;Pythonは汎用プログラミング言語の一つで、（多くの場合）インタプリタ上で動作する動的型付けスクリプト言語である。
Pythonの大きな特徴としては&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;インデントで関数等のブロックを表現する&lt;/li&gt;
&lt;li&gt;柔軟な動的型付け&lt;/li&gt;
&lt;li&gt;内包表記による簡潔で高速なコード&lt;/li&gt;
&lt;li&gt;関数型プログラミングも可能&lt;/li&gt;
&lt;li&gt;豊富な標準ライブラリ、活発な海外コミュニティ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;逆にデメリットとしては&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;インデントに縛られる&lt;/li&gt;
&lt;li&gt;lambda式が書きにくい&lt;/li&gt;
&lt;li&gt;正規表現が使いにくい&lt;/li&gt;
&lt;li&gt;オブジェクト指向にもかかわらずメソッドチェーンが書きにくい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;あたりだろうか（個人の感想）。&lt;/p&gt;

&lt;h3&gt;バージョン&lt;/h3&gt;

&lt;p&gt;Pythonは現在2.xから3.xへの移行期を向かえている。3.xでは2.xの後方互換性を切り捨てる形で、新たな機能を取り入れることを目指している。
（Pythonコミュニティとしては「今までよりもPythonicなコード」を目指しているようだ）&lt;/p&gt;

&lt;p&gt;そのため、3.xのコードは2.xの処理系では動かないし、その逆もまた然りだ。
昨年度の分科会では2.7の処理系を用いて説明を行ったが、3.xに対する環境も整備しつつあるこの時期が3.xへの移行タイミングと見極め、本分科会では3.xを対象として説明を行う。&lt;/p&gt;

&lt;p&gt;ちなみに、上でAnacondaをインストールしてもらった際にanaconda3-2.1.0というパッケージをインストールしてもらった。
これは2015年4月末時点の最新バージョンで、付属しているPython処理系のバージョンは3.4.1である。&lt;/p&gt;

&lt;h3&gt;サンプルスクリプト&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 定義域は(-3,3)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# sinカーブに正規分布ノイズをのせる&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# 3次関数フィッティング&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;polyfit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poly1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上記のスクリプトを保存してpythonで実行すると、以下のような結果が得られる。
sin関数のフィッティングである。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/02/sin.png&quot; alt=&quot;sin関数のフィッティング&quot;&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Apr 2015 21:07:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/environment.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/environment.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#01 機械学習の基礎</title>
        <description>&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;（釈迦に説法になる話も多分に含まれるが、限りなく前提知識を0に近づけたいのでご容赦願いたい）&lt;/p&gt;

&lt;p&gt;「機械学習」や「ビッグデータ」という言葉を昨今よく耳にするが、機械学習とは一体何だろうか。
&lt;a href=&quot;http://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&quot;&gt;Wikipedia&lt;/a&gt;から引用してみる。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;機械学習（きかいがくしゅう、英: machine learning）とは、人工知能における研究課題の一つで、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のことである。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ここで問題になるのは、どのような手法・アルゴリズムを用いれば計算機で学習を実現できるかということである。
元々人工知能に関する研究は1940年代の頃から始まっており、今日に至るまでの長い道のりの過程で工学的応用を得たのが機械学習と呼ばれる分野である。
まずは人工知能の歴史を軽く追って見ることにする。&lt;/p&gt;

&lt;h2&gt;人工知能の歴史&lt;/h2&gt;

&lt;h3&gt;Rule-based AI&lt;/h3&gt;

&lt;p&gt;人工知能というキーワードは1956年のダートマス大学における学会で初めて使われたという。
この頃から人工知能は盛んに研究されており、Alan Turingが有名な&lt;a href=&quot;http://ja.wikipedia.org/wiki/%E3%83%81%E3%83%A5%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%83%BB%E3%83%86%E3%82%B9%E3%83%88&quot;&gt;チューリングテスト&lt;/a&gt;を発表したのも1950年のことだった。
チューリングテストとは簡単に言うと、「自分がチャットしている相手が実はコンピュータだったら人工知能と言ってもいいよね」という人工知能のテストである。
実際、1964年頃にMITのJoseph Weizenbaum教授は有名な&lt;a href=&quot;http://ja.wikipedia.org/wiki/ELIZA&quot;&gt;ELIZA&lt;/a&gt;というカウンセラーを模した人工知能を実装し、この人工知能は多くの人間を騙すことに成功したという。&lt;/p&gt;

&lt;p&gt;ELIZAが実際に行った会話の記録は多く残っているし、今でもEmacsにはdoctorというELIZAのbuilt-in実装が入っている（emacsを起動してEsc - Xでdoctorを実行）。
確かにELIZAの会話は一見人間の精神科医のものかと思ってしまう。
しかし、実際にはパターンマッチングでそれらしい単語（例えば&amp;quot;anxiety&amp;quot;とか&amp;quot;headache&amp;quot;とか）にマッチした場合は用意された回答をし、マッチケースがなければオウム返しをするという、非常にシンプルな作りになっている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/eliza.png&quot; alt=&quot;ELIZAとの会話の様子&quot;&gt;&lt;/p&gt;

&lt;p&gt;この時期の人工知能は、ELIZAに限らず多くはパターンマッチング（if-then-elseをひたすら連ねたもの）を基本とした&lt;b&gt;rule-based&lt;/b&gt;なAIだった。
Rule-basedなAIでは、より高度な知能に見せるためにはマッチケースをひたすら増やすしかない。&lt;/p&gt;

&lt;p&gt;余談だが、「Lispが人工知能研究で用いられた」と言われることがあるが、これは当時まともにパターンマッチングができる言語がLispくらいしかなかったがために、Lispが用いられてきたという歴史的経緯があるようだ。
ELIZAも初期の段階でLisp実装がなされている。&lt;/p&gt;

&lt;h3&gt;Neural Network&lt;/h3&gt;

&lt;p&gt;同じ頃、1958年にアメリカの心理学者Frank RosenblattがPerceptronを発表した。
「人工知能を作るなら脳のニューロンの機構を模せばいいではないか」という発想のもと生まれたのが、Perceptronを始めとしたNeural Networkというモデルである。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/perceptron.png&quot; alt=&quot;パーセプトロンのモデル&quot;&gt;&lt;/p&gt;

&lt;p&gt;図中の\(x_1 \cdots x_n\)及び\(z\)がニューロンを形式化したもので、左側のニューロンから右側のニューロンへ信号が伝えられている様子をモデル化している。
各入力に\(w_1 \cdots w_n\)という重みをつけて、&lt;/p&gt;

&lt;p&gt;\[
  z = \sum_{i=1}^n x_i w_i
\]&lt;/p&gt;

&lt;p&gt;という入力を与える。&lt;/p&gt;

&lt;p&gt;しかし、最も単純なパーセプトロンではXOR(排他的論理和)を認識することができない。
この事実（正確には「単純パーセプトロンは線形分離不能なデータを識別できない」こと）は1969年に数学的に証明された。&lt;/p&gt;

&lt;p&gt;Neural Network自体は後に扱うので、この辺りで出た不明な単語は気にしなくて良い。&lt;/p&gt;

&lt;h3&gt;AIの冬&lt;/h3&gt;

&lt;p&gt;このように少し複雑な問題になると解けなくなってしまうことから人工知能研究への期待は薄れ、国や企業の投資も減っていった。
そのため多くの研究者は人間らしい知能を実現する人工知能(強いAI)の研究を諦め、それまでの研究を実用的なアプリケーション(弱いAI)に活かすことを考えた。
その際によく研究されたのが、光学文字認識や音声認識、機械翻訳などである。
実際、この時期に地道にアルゴリズムが改良され、これらの問題も解くことができるようになっていった。
しかし、実用化されるほどの認識精度にまでは至らなかった。
その大きな理由はデータ不足である。&lt;/p&gt;

&lt;h3&gt;ビッグデータ時代の幕開け&lt;/h3&gt;

&lt;p&gt;この状況を打開する出来事が1993年に起こった。
CERNによるWorld Wide Webの一般開放、すなわちインターネットの始まりである。
瞬く間にインターネット上にはテキスト、音声・画像をはじめ、ユーザの行動履歴、天気、株価情報といった広範囲にわたる膨大な量のデータで溢れかえることになった。
こういったデータをアルゴリズムで処理することにより、文字や音声の認識精度はますます向上した。&lt;/p&gt;

&lt;p&gt;さらに、&lt;a href=&quot;http://ja.wikipedia.org/wiki/%E3%83%A0%E3%83%BC%E3%82%A2%E3%81%AE%E6%B3%95%E5%89%87&quot;&gt;ムーアの法則&lt;/a&gt;に代表されるようにコンピュータの性能が指数関数的に向上し、大量のデータを高速に処理できるようになった。
あたかもコンピュータが大量のデータを読み込んで学習し、文字や音声を認識しているようにみえることから、機械学習と呼ばれるようになった。&lt;/p&gt;

&lt;p&gt;2006年、イギリスのコンピュータ科学者Geoffrey HintonがAuto EncoderとDeep Belief Networkを考案し、これが昨今注目を集めている&amp;quot;Deep Learning&amp;quot;の先駆けとなった。
さらに2011年、Stanford大学のAndrew NgらがGoogleと共同で研究を行った結果、コンピュータクラスタが自力でYouTubeの動画データから猫の概念を認識できるようになったと報告された。
この研究報告によって世界中に衝撃が走り、今日ではDeep Learningをはじめ機械学習の研究が世界中のコンピュータサイエンティストによって行われている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/cat.jpg&quot; alt=&quot;Googleの猫&quot; style=&quot;width:50%&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;統計と機械学習&lt;/h2&gt;

&lt;p&gt;先程パーセプトロンに触れた際に、&lt;b&gt;「あの数式のどこが脳の機構を模しているんだ」&lt;/b&gt;と感じた人も少なく無いと思う。
実際のところ、パーセプトロンを始めたNeural Networkは、ネットワーク構造を持ち、各素子がしきい値を持っていること以外はほとんど脳の機構と関係がない。
ではなぜあのようなモデルが用いられるかというと、結局コンピュータは数字を扱うことしかできないからである。
逆に言えば、適切に数字に直すことさえできればあらゆるデータを高速に処理することができるのがコンピュータの強みである。
そのため、今日の機会学習理論の多くは数理統計をベースとしている。&lt;/p&gt;

&lt;p&gt;機械学習には様々な種類があるが、その大部分は分類問題である。
先の例だと、文字認識も「与えられた画像をそこに書かれている文字に分類する」問題だし、音声認識も「入力された振幅データがどの音素に相当するのか分類する」ことになる。
その際には&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;予め答えのわかっているデータを&lt;b&gt;学習&lt;/b&gt;する&lt;/li&gt;
&lt;li&gt;未知のデータがどのクラスに分類されるのか&lt;b&gt;推定&lt;/b&gt;する&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;という2段階にわかれている。&lt;/p&gt;

&lt;h3&gt;学習&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/feature_extraction.png&quot; alt=&quot;特徴抽出の例&quot;&gt;&lt;/p&gt;

&lt;p&gt;例えば上のような元画像が与えられたとき、この画像が「れ」というひらがなであると認識するにはどうすればよいか。
先程も述べたように、結局コンピュータには数字しか扱うことができないから、なんとかしてコンピュータの扱えるような数値データとしてデータの特徴を捉え直す必要がある。
このようにデータからその特徴を数値として取り出すことを&lt;b&gt;特徴抽出&lt;/b&gt;という。&lt;/p&gt;

&lt;p&gt;一例だが、例えば文字画像を20×20で格子状に区切って各格子で平均をとり、合計400個の濃淡データとして扱う方法がある。
こうすると、例えば「れ」なら「左から30%のあたりに濃度が大きいデータが縦に並んでいる」といった特徴になるわけである。
このデータを400次元空間にmappingすれば、「&amp;quot;れ&amp;quot;は400次元空間のこのあたりに来やすい」みたいな計算をコンピュータで行うことができる。&lt;/p&gt;

&lt;h3&gt;推定&lt;/h3&gt;

&lt;p&gt;未知のデータが与えられたときも同じように特徴抽出を行い、既に学習したデータと照らし合わせればクラスに分類することができる。
このときにコンピュータが行っているのも、単純に「このデータは今までのどのデータにより近いのか」という類似度（距離）計算にすぎない（勿論類似度以外で分類するアルゴリズムもある）。&lt;/p&gt;

&lt;h2&gt;機械学習の学習&lt;/h2&gt;

&lt;h3&gt;当分科会の進め方&lt;/h3&gt;

&lt;p&gt;ご察しの通り、機械学習ではそれなりに高度な数学（主に解析学、線形代数、確率統計）が要求される。
そのため、この分科会ではおおまかに次のような流れで進めていく。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;基礎的な確率統計&lt;/li&gt;
&lt;li&gt;数理統計モデル&lt;/li&gt;
&lt;li&gt;機械学習の基本的なアルゴリズム&lt;/li&gt;
&lt;li&gt;最新のトピック（主にDeep Learning）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1年生にとっては難しいこともあると思うが、可能な限り高校レベルの数学で理解可能な丁寧な解説を心がけるし、わからない点があれば遠慮なく質問してほしい
（答えられる範囲で回答するよう努力するので）。&lt;/p&gt;

&lt;p&gt;理論だけをやると理解しづらいし実感がわきにくいと思うので、できるだけ実例・応用例を見せながら進めたい（希望）。また、回毎に簡単な練習をつけたい（希望）。&lt;/p&gt;

&lt;p&gt;この分科会の最終目標として、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;KaggleのcompetitionにTSGの有志で参加したい&lt;/li&gt;
&lt;li&gt;機械学習を使って何らかしらの認識エンジンを作りたい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;という2つを据えておく（希望）。&lt;/p&gt;

&lt;p&gt;ちなみに、&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt;というのは統計解析のコンペティションで、様々な企業が「このデータを解析して欲しい！」といって賞金をつけ、世界中の企業や団体や個人が解析してスコアを競いあうサイトである。&lt;/p&gt;

&lt;h3&gt;本&lt;/h3&gt;

&lt;p&gt;参考書というか、僕が読んだ本・読みかけの本で良かったと思う本を挙げておく。
順番に意味はない。&lt;/p&gt;

&lt;h4&gt;自然科学の統計学&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4130420674&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
&lt;s&gt;最初から積読化されている本を挙げるのもどうかと思ったが&lt;/s&gt;多方面から良書であるとのレビューを聞いている。
教養の「基礎統計」からのつなぎに最適（基礎統計で使われる教科書の続編でもあるし）。&lt;/p&gt;

&lt;h4&gt;はじめてのパターン認識&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4627849710&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
一覧性に優れた本で、某書と違ってカバンに入れて持ち運ぶのにはとても適している良書。
説明もわかりやすい。&lt;/p&gt;

&lt;h4&gt;パターン認識と機械学習&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4621061224&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4621061240&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
鉄板。だが難しい。避けては通れない一冊。
levelfourは数学が苦手なので、4章くらいまでしかまだ読んでないです。&lt;/p&gt;

&lt;h4&gt;AIの衝撃&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4062883074&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;今度は理論書ではなくて普通の新書。機械学習のバックグラウンドやこの先の展望が読みやすく書かれていて面白い。
機械学習を勉強する前に読めばよかったなあと思ってたり。ちなみに2015年3月刊行です。&lt;/p&gt;

&lt;h3&gt;研究室&lt;/h3&gt;

&lt;p&gt;折角東大にいるのだから、最先端の研究室に行って教授に話を聞いてみるとよい。というか本当にオススメ。&lt;/p&gt;

&lt;h4&gt;山西先生&lt;/h4&gt;

&lt;p&gt;計数工学科の先生。アルゴリズム寄りの研究者で、最近の興味は「潜在情報の抽出」だとか。
つまり、機械学習で得られたパラメータはコンピュータの計算したただの数字でしかないけど、その数字には具体的にどういう意味付けができるか、ということ。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://ibis.t.u-tokyo.ac.jp/yamanishiken/&quot;&gt;http://ibis.t.u-tokyo.ac.jp/yamanishiken/&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;杉山先生&lt;/h4&gt;

&lt;p&gt;情報科学科の先生。去年までは東工大にいらっしゃった。
研究自体はアルゴリズム寄り。「機械学習のアルゴリズムは様々なアプローチからおおよそ研究し尽くされたから、一段上のレイヤから抽象的にアルゴリズムを俯瞰する（要約、曲解あり）」ことを考えているそう。
あとは色々な企業でコンサルもなさってるそうです。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://www.ms.k.u-tokyo.ac.jp/index-jp.html&quot;&gt;http://www.ms.k.u-tokyo.ac.jp/index-jp.html&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;中山先生&lt;/h4&gt;

&lt;p&gt;情報理工学研究科の創造情報学の先生。画像認識・物体認識が専門で、世界的な画像認識のアカデミックコンペに毎年参加されているそうです。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://www.nlab.ci.i.u-tokyo.ac.jp/&quot;&gt;http://www.nlab.ci.i.u-tokyo.ac.jp/&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;中川先生&lt;/h4&gt;

&lt;p&gt;情報理工学研究科の数理情報学の先生。最近はプライバシー保護データマイニング(PPDM)がメイン。
「ビッグデータ使って統計するのはいいけど、データに紐付いた個人情報を上手く匿名化しないとマズイよね」的な話。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://www.r.dl.itc.u-tokyo.ac.jp/&quot;&gt;http://www.r.dl.itc.u-tokyo.ac.jp/&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;松尾先生&lt;/h4&gt;

&lt;p&gt;ウェブマイニング、人工知能の専門家。
最近メディアにもよく顔を出して本もたくさん執筆なさっています。書籍部にも平積みにしてあるはずなので読むと良いかも。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://weblab.t.u-tokyo.ac.jp/&quot;&gt;http://weblab.t.u-tokyo.ac.jp/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Apr 2015 21:06:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/machine-learning-basics.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/machine-learning-basics.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
  </channel>
</rss>
