<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TSG Machine Learning</title>
    <description>TSG Machine Learning Substudy Group in 2015
</description>
    <link>http://sig.tsg.ne.jp/ml2015/</link>
    <atom:link href="http://sig.tsg.ne.jp/ml2015/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 29 Jun 2015 23:40:13 +0900</pubDate>
    <lastBuildDate>Mon, 29 Jun 2015 23:40:13 +0900</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>#12 畳込みニューラルネット</title>
        <description>&lt;p&gt;Deep Neural Networkになると学習を収束させるのが計算量の面から困難になってくる．
その1つの解決策として提示したのが，#10で扱ったオートエンコーダである．
コンセプトとしては，&lt;u&gt;最適な初期値を与えれば計算がうまくいく&lt;/u&gt;ということにある．
そのためにオートエンコーダを用いて事前学習を行うことによってパーセプトロンに初期値を与えた．
#10のサンプルコードでは2層のパーセプトロンをオートエンコーダで事前学習させたが，これを更に多層にすればDeep Neural Networkになる．&lt;/p&gt;

&lt;p&gt;この節では，Deep Neural Networkの学習を収束させる別のアプローチとして，&lt;u&gt;ノード間結合数(パラメータの次元数)を減らして計算量を減らす&lt;/u&gt;ことを考える．
このようなニューラルネットワークを&lt;strong&gt;畳込みニューラルネット(Convolution Neural Network, CNN)&lt;/strong&gt;という．&lt;/p&gt;

&lt;h2&gt;畳込みニューラルネット&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/12/cnn1.png&quot; alt=&quot;全結合&quot;&gt;&lt;/p&gt;

&lt;p&gt;これまで扱ってきたニューラルネットは，すべてノード間が全結合したニューラルネットであった．
このように全結合していると，当然すべての結合に相当するパラメータの推定を行わなければならない．
しかし，学習精度を上げるためには必ずしも全パラメータが必須というわけではない（現実のデータは疎(sparse)な構造を持つことが多い）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/12/cnn2.png&quot; alt=&quot;畳込み&quot;&gt;&lt;/p&gt;

&lt;p&gt;そこで，このようにノード間結合を減らすことを考える．
このようにすれば，推定すべきパラメータ数が減少し，ある程度計算量爆発を抑えることができる．&lt;/p&gt;

&lt;p&gt;CNNは脳の受容野の仕組みを模したものである．
一層目の入力画像に対して，中間層は位置敏感性を持ち（つまり、各々の中間層ノードは入力層の特定の部位に反応する），出力層は中間層に比べて位置に鈍感である．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/12/cnn3.png&quot; alt=&quot;畳込みとプーリング&quot;&gt;&lt;/p&gt;

&lt;p&gt;前者を&lt;strong&gt;畳込み&lt;/strong&gt;，後者を&lt;strong&gt;プーリング&lt;/strong&gt;という．&lt;/p&gt;

&lt;p&gt;CNNは畳込みとプーリングという2種類の演算に対応する層を交互に重ねたニューラルネットワークである．&lt;/p&gt;
</description>
        <pubDate>Tue, 30 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/30/convolution-neural-network.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/30/convolution-neural-network.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#11 学習のテクニック</title>
        <description>&lt;p&gt;パーセプトロンの層数を増やせば多彩な表現力が得られ，Deep Learningの基本的な考え方は多層のパーセプトロンをどのように上手に収束させるかということにある．
#10ではオートエンコーダを用いた事前学習によって，良いパラメータに収束しやすい初期値を得ることを考えた．
この節では，ニューラルネットワークに依らない，学習における一般的なテクニックをいくつか紹介する．&lt;/p&gt;

&lt;h2&gt;機械学習における問題点&lt;/h2&gt;

&lt;h3&gt;過学習・過適合&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/11/overfit.png&quot; alt=&quot;過学習&quot;&gt;&lt;/p&gt;

&lt;p&gt;上図は\(y=\sin(x)\)の形状に分布するデータ列を表現するのに，それぞれ1, 3, 5, 10, 20, 50次元の多項式でフィッティングした結果である．&lt;/p&gt;

&lt;p&gt;1次関数は線形なので，正弦曲線状にフィッティングするには表現力が不足している．
3次関数くらいになるとフィッティングに十分な表現力を持つことがわかる．
しかし，あまりに次元が大きくなりすぎるとデータ列を忠実に通るような曲線になるようにフィッティングされてしまうので，正弦曲線から離れてギザギザになってしまう．20次元や50次元の結果はそれが顕著で，これは望むべき結果ではない．
このようにあまりに忠実に元データを再現している状態を&lt;strong&gt;過学習(過適合, overfitting)&lt;/strong&gt;という．&lt;/p&gt;

&lt;p&gt;一般に，&lt;u&gt;統計モデルの表現力(=パラメータ数)とその尤もらしさは常にtrade-offの関係&lt;/u&gt;にある．&lt;/p&gt;

&lt;h3&gt;計算の難しさ&lt;/h3&gt;

&lt;p&gt;機械学習は，学習データを使ってモデルのパラメータを計算決定することである．
一般的にパラメータを増やせば表現力が上がり，学習データ量を増やせば学習精度は向上するが，計算量が飛躍的に向上する（前者は特に「次元の呪い」と呼ばれる問題である）．&lt;/p&gt;

&lt;p&gt;また，何度も触れている通り，勾配法(#04参照)を用いてパラメータ更新をする際には初期値の選び方が問題になる．
初期値によっては局所解にトラップしてしまうし，最悪の場合計算が収束しない場合もある．&lt;/p&gt;

&lt;p&gt;勾配法以外でも，オンライン学習でパラメータを更新する回数が多くなればなるほどパラメータがオーバーフロー・アンダーフローする問題は依然存在する．&lt;/p&gt;

&lt;h2&gt;様々な解決策&lt;/h2&gt;

&lt;h3&gt;正規化(Normalization)&lt;/h3&gt;

&lt;p&gt;例えば顔画像認識をする際に，撮影した顔画像写真の光源の当たり方が写真によって違っていた場合，学習に悪影響をおよぼす可能性がある．
文字認識をする際に，人によって文字の形が正方形に近かったり縦長の長方形に近かったりすることがあるが，これもアスペクト比を統一するのが望ましい．
このように学習に関係ないデータ間の差異を，学習に影響を及ぼさないように取り除くことを&lt;strong&gt;正規化&lt;/strong&gt;という．&lt;/p&gt;

&lt;p&gt;Web上の情報では正規化を後述の正則化と取り違えているものもあるので注意を要する．&lt;/p&gt;

&lt;p&gt;【参考】&lt;a href=&quot;http://kivantium.hateblo.jp/entry/2014/11/25/230658&quot;&gt;ご注文は機械学習ですか？ - kivantiumの活動日記&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;早期終了(early stopping)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/11/error.png&quot; alt=&quot;誤差の推移&quot;&gt;&lt;/p&gt;

&lt;p&gt;SGDを用いて学習をするとき，ある一定量以上のデータを学習すると学習がそれ以上あまり進まなくなってくる．
上図はオンライン学習で1データずつ学習した時の，横軸に既に学習したデータ数，縦軸にその時点での推定誤差をプロットしている．
この場合，500データ以上になると誤差があまり変わらなくなるので，このあたりで学習を打ち切ることで高速に学習を終えることができる．&lt;/p&gt;

&lt;p&gt;なお，これは手書き数字(digits)認識のエラー推移である．&lt;/p&gt;

&lt;h3&gt;グリッドサーチ&lt;/h3&gt;

&lt;p&gt;機械学習のモデルには様々なパラメータがある．
例えば，本分科会で扱っている多層パーセプトロンでも&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;各層のノード数&lt;/li&gt;
&lt;li&gt;活性化関数(シグモイド関数, tanh等)の傾斜パラメータ&lt;/li&gt;
&lt;li&gt;学習率&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;といったパラメータがある．このパラメータによって学習の収束がうまくいくかどうか，そして学習自体の精度が決まってくるため，&lt;u&gt;パラメータチューニング&lt;/u&gt;は重要な作業になる．&lt;/p&gt;

&lt;p&gt;パラメータチューニングで一般的に用いられるのが&lt;strong&gt;グリッドサーチ&lt;/strong&gt;という手法である．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/11/grid_search.png&quot; alt=&quot;グリッドサーチ&quot;&gt;&lt;/p&gt;

&lt;p&gt;上図は手書き数字認識を2層のパーセプトロンで行った時のグリッドサーチの結果であり，横軸は中間層のノード数，縦軸は活性化関数としてとったシグモイド関数の傾斜パラメータ\(\beta\)の，推定精度のコンターマップである．
グリッドサーチという名前は付いているが，やっていることは単純で，チューニングしたいパラメータをある範囲の中で変化させながら学習器を作り，精度を測定するだけである．
単純ではあるが，上図からわかるようにきちんとパラメータチューニングをするかしないかで，同じアルゴリズムでも精度が大分変わってしまう．&lt;/p&gt;

&lt;p&gt;このソースコードは&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/11/grid_search.py&quot;&gt;grid_search.py&lt;/a&gt;に示した．&lt;/p&gt;

&lt;h3&gt;次元削減(次元圧縮)&lt;/h3&gt;

&lt;p&gt;元の学習データの次元数が大きすぎる場合は，特徴抽出したり，次元削減をして計算量を減らすことを考えるとよい．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/feature_extraction.png&quot; alt=&quot;特徴抽出&quot;&gt;&lt;/p&gt;

&lt;p&gt;特徴抽出は，元のデータの特徴を捉えて必要な情報だけを抽出する手法である．
上の画像は#01で提示したもので，元画像は400x400=16000画素あるが，勿論16000画素あっても不必要な情報が多く学習に時間がかかりすぎてしまう．
20x20のセルに落としこむことによって，学習精度を保ちつつ(必要な特徴を残しつつ)次元数を元の2.5%まで落とすことができる．&lt;/p&gt;

&lt;p&gt;次元削減には様々なアルゴリズムがある．この分科会では既に#09で&lt;u&gt;オートエンコーダ&lt;/u&gt;と&lt;u&gt;主成分分析&lt;/u&gt;を取り上げている．
他にもSVMで用いられるカーネル主成分分析等があるが，詳細についてはこれ以上は詳しく述べない．&lt;/p&gt;

&lt;h3&gt;正則化(Regularization)&lt;/h3&gt;

&lt;p&gt;過学習の原因はモデルの自由度(パラメータ数)が大きいことに起因する．だからといって，自由度を下げるとモデルの表現力が低下してしまい，必要な情報をうまく表すことができない．
そこで，パラメータになんらかの制約を加えることを考える．
これまで考えていた誤差関数は&lt;/p&gt;

&lt;div&gt;
\[
    E(\boldsymbol{w})=\frac{1}{2}||\boldsymbol{y}-\boldsymbol{t}||^2
\]
&lt;/div&gt;

&lt;p&gt;の形をしたものであった．この右辺に&lt;strong&gt;正則化項&lt;/strong&gt;を加える．&lt;/p&gt;

&lt;div&gt;
\[
    E(\boldsymbol{w})=\frac{1}{2}||\boldsymbol{y}-\boldsymbol{t}||^2+\lambda||\boldsymbol{w}||^p
\]
&lt;/div&gt;

&lt;p&gt;ただし\(\lambda &amp;gt; 0\)である．このように正則化項を導入すると，誤差関数を最小化することで同時に\(||\boldsymbol{w}||\)が小さい値に制限される．
このような手法を&lt;strong&gt;正則化&lt;/strong&gt;という．特に&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;p=1 → Lasso正則化(L1正則化)&lt;/li&gt;
&lt;li&gt;p=2 → Ridge正則化(L2正則化)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;と呼ばれる．L1正則化はパラメータの一部が0に近づくので特徴選択の役割も同時に果たすのだが，解析的に書き下せないためL2正則化が用いられることも多い．
\(\lambda\)は正則化の度合を決めるパラメータである．&lt;/p&gt;

&lt;p&gt;ちなみに，LassoとRidgeを混合したElasticNetという正則化手法もある．&lt;/p&gt;

&lt;div&gt;
\[
    E(\boldsymbol{w})=\frac{1}{2}||\boldsymbol{y}-\boldsymbol{t}||^2+\lambda\{\alpha||\boldsymbol{w}||+(1-\alpha)||\boldsymbol{w}||^2\}
\]
&lt;/div&gt;

&lt;h2&gt;参考&lt;/h2&gt;

&lt;p&gt;多層パーセプトロンのパラメータの選択については公式のドキュメントが参考になる．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://deeplearning.net/tutorial/mlp.html#tips-and-tricks-for-training-mlps&quot;&gt;Multilayer Perceptron - DeepLearning 0.1 Documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;この辺りの機械学習の周辺テクニックについては下記のスライドが参考になる．&lt;/p&gt;

&lt;p&gt;&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/bK9wnTG51FHn5x&quot; width=&quot;425&quot; height=&quot;355&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/canard0328/ss-44288984&quot; title=&quot;機械学習によるデータ分析まわりのお話&quot; target=&quot;_blank&quot;&gt;機械学習によるデータ分析まわりのお話&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;//www.slideshare.net/canard0328&quot; target=&quot;_blank&quot;&gt;canard0328 &lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/29/techniques-of-learning.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/29/techniques-of-learning.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#10 事前学習</title>
        <description>&lt;h2&gt;事前学習&lt;/h2&gt;

&lt;p&gt;#07，#08のパーセプトロンの問題点の一つとして，初期値をランダムにとっていたことが挙げられる．
度々になるが，勾配法でランダムな初期値からスタートすると，局所解にトラップされてしまう可能性がある．&lt;/p&gt;

&lt;p&gt;この問題の解決策として2006年にGeoffery Hintonによって提案されたのが，オートエンコーダである．
着想は，初期値をランダムに取る代わりに&lt;u&gt;最適な初期値を推定する&lt;/u&gt;ことにある．
つまり，初期値を選択する操作自体を一つの推定問題として捉えるのである．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/10/pretraining1.png&quot; alt=&quot;事前学習&quot;&gt;&lt;/p&gt;

&lt;p&gt;まずはこの図のように最小の層をオートエンコーダとして見立てて，学習を行う．
このときのオートエンコーダの重みの初期値はランダムにとる．
学習はパーセプトロンと同様にSGD等を用いて行う．&lt;/p&gt;

&lt;p&gt;そして，1層目のパラメータの初期値が推定された状態で，残りの層も含めて教師あり学習を行う．
残りの層の初期値はランダムにとる．
これまでのパーセプトロンとの違いは，1層目がオートエンコーダの自己学習を利用して自らパラメータの最適な初期値を学習しているという点である．
このようにオートエンコーダを用いてパーセプトロンの重みの初期値を予め推定しておくことを&lt;strong&gt;事前学習(pre-training)&lt;/strong&gt;という．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/10/pretraining2.png&quot; alt=&quot;事前学習2&quot;&gt;&lt;/p&gt;

&lt;p&gt;Hintonが初めてオートエンコーダを発表した時は，2000→1000→500→30と3段のオートエンコーダを重ねて，元通りの画像を復元するように学習できた例を示している．
上の例でははじめの1層だけだったが，一般に各層の重みをそれぞれオートエンコーダとみなして事前学習を行うことができる．&lt;/p&gt;

&lt;p&gt;【参考】&lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/science.pdf&quot;&gt;Reducing the Dimensionality of Data with Neural Networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ちなみに，なぜオートエンコーダを用いるとよい初期値が得られるのか，という点についてはまだ理論的によくわかっていない．&lt;/p&gt;

&lt;h2&gt;コードによる実例&lt;/h2&gt;

&lt;h3&gt;オートエンコーダ&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/10/autoencoder.py&quot;&gt;autoencoder.py&lt;/a&gt;にコードを示す．&lt;/p&gt;

&lt;p&gt;基本的なニューラルネットの実装は#08で示したdigit.pyのPerceptronクラスを流用している．
AutoEncoderは学習の仕方もほとんどPerceptronと同じなので，Perceptronクラスを継承している（継承しやすいように多少Perceptronクラスは修正した）．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AutoEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_mnodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 入力信号を教師信号として学習&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 中間層出力を得る&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;これがAutoEncoderクラスの実装である．
ほとんどPerceptronクラスと同じなのだが，学習を行うメソッド&lt;code&gt;AutoEncoder.fit&lt;/code&gt;だけ，教師信号として入力自身を与えるように変更している．&lt;/p&gt;

&lt;p&gt;このautoencoder.pyを実行すると，次のようなウインドウが現れる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/10/encode.png&quot; alt=&quot;エンコード結果&quot;&gt;&lt;/p&gt;

&lt;p&gt;左から順に，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1列目: 入力ベクトル&lt;/li&gt;
&lt;li&gt;2列目: オートエンコーダの中間層の出力&lt;/li&gt;
&lt;li&gt;3列目: 出力ベクトル&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;となっている．元の入力画像は8×8の64次元で，中間層では一旦16次元まで落としてそれを4×4にconvertして表示している．
それを出力層にかけると入力画像に近い画像が得られる．&lt;/p&gt;

&lt;p&gt;ちなみに，上から順にそれぞれ9，4，3である．&lt;/p&gt;

&lt;h3&gt;事前学習&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/10/digit.py&quot;&gt;digit.py&lt;/a&gt;にコードを示す．&lt;/p&gt;

&lt;p&gt;このコードは，上のautoencoder.pyに更に修正を加えたものになっている．
重要な変更箇所は以下のとおり．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pretrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 事前学習&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ae&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mid_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ae&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ae&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;このメソッドが事前学習を行っている．事前学習自体はautoencoder.pyで定義したAutoEncoderクラスに投げている．
事前学習でできた重みベクトル\(\boldsymbol{w}\)を&lt;code&gt;AutoEncoder.weight()&lt;/code&gt;で取得し，Perceptronクラスの重みの初期値に設定している．&lt;/p&gt;

&lt;p&gt;この新しいPerceptronクラスで数字認識を行うと，90%程度の精度を出すことができる．&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/15/pretraining.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/15/pretraining.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#09 オートエンコーダ</title>
        <description>&lt;h2&gt;オートエンコーダ&lt;/h2&gt;

&lt;p&gt;#07，#08で扱った多層パーセプトロンは，このような形状をしていた．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/perceptron.001.png&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;入力ベクトルとして\(\boldsymbol{x}\)を与え，教師ベクトルとして\(\boldsymbol{z}\)を与えて，誤差関数を最小化することで学習を行った．
（#07での説明では教師信号は\(z\)というスカラーで説明したが，これをベクトルに拡張するのは難しくない）&lt;/p&gt;

&lt;p&gt;ここで，教師ベクトルを次のようにとってみる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/perceptron.002.png&quot; alt=&quot;オートエンコーダ&quot;&gt;&lt;/p&gt;

&lt;p&gt;すなわち，教師ベクトル=入力ベクトルである．
このパーセプトロンは，入力ベクトルを与えると出力として自分自身を復元することが期待される．&lt;/p&gt;

&lt;p&gt;しかし，目的としているのは入力を復元することではなく，中間層の出力である．
中間層のノード数が入力層よりも少ない状態で入力を復元することができれば，より少ない情報量(=低次元)で入力データを表現できていることになる．
言い換えれば，入力データの冗長性を上手に排除することができる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/perceptron.003.png&quot; alt=&quot;オートエンコーダの中間層&quot;&gt;&lt;/p&gt;

&lt;p&gt;このようなパーセプトロンはオートエンコーダ(自己復号化器，AutoEncoder)と呼ばれる．&lt;/p&gt;

&lt;h2&gt;オートエンコーダと主成分分析の関係&lt;/h2&gt;

&lt;p&gt;学習データの次元を削減したいというのは，元々機械学習の分野ではよく行われてきたことである．
数学的には一般次元で問題を定式化して解析的に解くことはできるが，コンピュータサイエンスの視点から見るとたとえ一般次元で定式化されていても次元が大きすぎると有限時間で計算が終わらないので，次元削減したいというのは自然な考えである．
（「次元の呪い，The curse of dimensionality」という言葉がよく用いられる）
オートエンコーダ以外にも線形代数や統計でよく用いられてきた&lt;u&gt;主成分分析(principal component analysis, PCA)&lt;/u&gt;という手法を簡単に紹介する．&lt;/p&gt;

&lt;h3&gt;主成分分析&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/pca1.png&quot; alt=&quot;3D空間中のデータ&quot;&gt;&lt;/p&gt;

&lt;p&gt;3D空間中にこのようなデータ列があったとする（本来は一般次元で考えるべきだが，視覚化のため3次元とする）．
今，このプロットは通常通り(x,y,z)の直交座標系をとっている．
ちなみに，データ列は&lt;/p&gt;

&lt;div&gt;
\[
    x=y=\frac{z-1}{0.8}
\]
&lt;/div&gt;

&lt;p&gt;という直線上にガウシアンノイズを乗せたものである．&lt;/p&gt;

&lt;p&gt;ここで，「このデータ列を最もよく表現できる軸」をとることを考える．
データ列は直線上にのっているのだから，この直線の方向ベクトル&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{n}=(\begin{array}
        00.5 &amp; 0.3 &amp; 1
    \end{array})
\]
&lt;/div&gt;

&lt;p&gt;を軸としてとるとデータをよく表現できる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/pca2.png&quot; alt=&quot;軸の取り方&quot;&gt;&lt;/p&gt;

&lt;p&gt;(縮尺がうまくいかなかった)&lt;/p&gt;

&lt;p&gt;実際には勿論データ列の式は天下りには与えられないので，解析的に推定する必要がある．これを数学的に考察する．&lt;/p&gt;

&lt;p&gt;今とった軸は，データ列をよく表現できる軸，すなわち&lt;u&gt;データ列の分散が最大になるような軸&lt;/u&gt;である．
言い換えると，データ\(\boldsymbol{x_i}\)をベクトル\(\boldsymbol{n}\)に射影した値\(\boldsymbol{n}^{\mathrm{T}}\boldsymbol{x_i}\)が最もばらつくような\(\boldsymbol{n}\)を求める．
すなわち，制約\(|\boldsymbol{n}|=1\)の下で評価関数&lt;/p&gt;

&lt;div&gt;
\[
    J=(\boldsymbol{n},\lambda)=\sum_{k=1}^K(\boldsymbol{n}^{\mathrm{T}}\boldsymbol{x_k})^2+\lambda(1-|\boldsymbol{n}|^2)
\]
&lt;/div&gt;

&lt;p&gt;を最大化する（Lagrangeの未定乗数法）．&lt;/p&gt;

&lt;p&gt;計算は割愛して，評価関数Jの極値条件は&lt;/p&gt;

&lt;div&gt;
\[
    R\boldsymbol{n}=\lambda\boldsymbol{n} \tag{9-1}
\]
&lt;/div&gt;

&lt;p&gt;である．ただし&lt;/p&gt;

&lt;div&gt;
\[
    R=\sum_{k=1}^K\boldsymbol{x_kx_k}^{\mathrm{T}}
\]
&lt;/div&gt;

&lt;p&gt;で，分散・共分散行列という（単なる分散は1次元で，共分散行列は一般次元への拡張）．&lt;/p&gt;

&lt;p&gt;式(9-1)からわかることは，&lt;u&gt;データ列の分散・共分散行列に対する固有値問題の解が，データ列を最もよく表現する&lt;/u&gt;ということである．&lt;/p&gt;

&lt;p&gt;ここでとったベクトル\(\boldsymbol{n}\)を法線ベクトルとする平面は，元の3次元空間の中でデータ列の分散が最も小さくなる平面である．
この平面は\(\boldsymbol{n}\)に対する直交補空間と呼ばれる．
直交補空間内で同様の操作によってさらに軸をとり，その直交補空間内で同様の操作を行い，…を繰り返して軸を次々ととっていく．
この解析手法を&lt;strong&gt;主成分分析(principal component analysis, PCA)&lt;/strong&gt;と呼ぶ．&lt;/p&gt;

&lt;p&gt;主成分分析の本質は次元削減にある．
元のデータの次元が大きすぎるとき，上述の通り何らかの手段によって次元削減したい．
そのようなときにデータ列を最もよく表現する順に軸を元の次元より少なくとれば，元のデータの情報をうまく保持しつつ次元を落とすことができる．
しかしここで注意しなければならないのは，主成分分析の過程では次元とともに情報量も落ちるので，一般には認識精度は低下する．&lt;/p&gt;

&lt;h3&gt;オートエンコーダとの関係&lt;/h3&gt;

&lt;p&gt;主成分分析でとった\(M\)個の固有値ベクトルを行ベクトルとして格納した行列\(\Gamma\)は，次の最小化問題の解になっている．&lt;/p&gt;

&lt;div&gt;
\[
    \min_\Gamma\sum_{k=1}^K|\boldsymbol{x_k}-\Gamma^{\mathrm{T}}\Gamma\boldsymbol{x_i}|
\]
&lt;/div&gt;

&lt;p&gt;一方，&lt;u&gt;中間層の活性化関数が恒等関数であるような&lt;/u&gt;パーセプトロンは，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;中間層出力: \(W\boldsymbol{x_k}\)&lt;/li&gt;
&lt;li&gt;出力層出力: \(W^{\mathrm{T}}W\boldsymbol{x_k}\)&lt;/li&gt;
&lt;li&gt;教師ベクトル: \(\boldsymbol{x_k}\)&lt;/li&gt;
&lt;li&gt;誤差: \(\boldsymbol{x_k}-W^{\mathrm{T}}W\boldsymbol{x_k}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;であるから，\(\Gamma\leftrightarrow W\)と対応付けると主成分分析と同じ問題に落とし込める．&lt;/p&gt;

&lt;p&gt;#10で実際に作成するオートエンコーダは中間層の活性化関数がロジスティック関数なので完全に主成分分析と同じわけではないが，次元削減の基本的な概念は主成分分析をイメージすると良い．&lt;/p&gt;

&lt;p&gt;【参考】&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0893608089900142&quot;&gt;Neural networks and principal component analysis&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/14/autoencoder.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/14/autoencoder.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#08 確率的勾配降下法</title>
        <description>&lt;p&gt;#07ではパーセプトロンを紹介した．
ここではパーセプトロンを用いた学習を紹介する．&lt;/p&gt;

&lt;p&gt;そもそも教師あり学習における学習とは，#06で触れたようにデータセット&lt;/p&gt;

&lt;div&gt;
\[
    \mathcal{D}=\{(\boldsymbol{x_1},t_1),\cdots,(\boldsymbol{x_N},t_N)\}
\]
&lt;/div&gt;

&lt;p&gt;が与えられて，各学習データ\((\boldsymbol{x_i},t_i)\)に対して\(z_i\)という出力が得られるとき，&lt;u&gt;誤差関数を最小にするようにパラメータを推定すること&lt;/u&gt;であった．
また，一般的に誤差関数として二乗誤差関数&lt;/p&gt;

&lt;div&gt;
\[
    E(\boldsymbol{w})=\frac{1}{2}\sum_{i=1}^N|t_i-z_i|^2
\]
&lt;/div&gt;

&lt;p&gt;が用いられることが多い．(先頭の1/2は微分したときに係数が消えるようにつけてあるだけ)&lt;/p&gt;

&lt;p&gt;#06では単純な線形識別関数を用いて識別していたので，解析的に二乗誤差関数の最小化を行うことができた（正規方程式に落とし込める）．
しかし，パーセプトロンが少し複雑になると解析的に解くのは難しくなるので，#04で紹介した最適化手法の勾配法を用いて最小化する．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{w}^{(n+1)}=\boldsymbol{w}^{(n)}-\eta\frac{\partial E}{\partial\boldsymbol{w}}
\]
&lt;/div&gt;

&lt;p&gt;という更新式でパラメータ\(\boldsymbol{w}\)を更新するアルゴリズムであった．&lt;/p&gt;

&lt;p&gt;気をつけたいのは，このとき必要な学習データは事前にすべて読み込ませなければいけないという点である．
なぜなら，二乗誤差関数Eを計算するときにはすべての学習データが必要だからである．
このように学習データをまとめて学習する方法は&lt;strong&gt;バッチ学習&lt;/strong&gt;と呼ばれている．&lt;/p&gt;

&lt;p&gt;ここから紹介するのは，学習データを少しずつ（極端には1つずつ）学習する方法であり，&lt;strong&gt;オンライン学習&lt;/strong&gt;と呼ばれる．&lt;/p&gt;

&lt;h2&gt;確率的勾配降下法(SGD)&lt;/h2&gt;

&lt;p&gt;確率的勾配降下法(Stochastic Gradient Descent = SGD)とは，&lt;u&gt;ランダムに学習データを1つ選んで誤差関数を計算し，その勾配方向にパラメータを修正する操作を反復する&lt;/u&gt;手法である．&lt;/p&gt;

&lt;p&gt;今，\(k+1\)回目のパラメータ更新において\(n_{k+1}\)番目の学習データを選んだとき，更新式は以下のようになる．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{w}^{(k+1)}=\boldsymbol{w}^{(k)}-\eta\frac{\partial}{\partial\boldsymbol{w}}\left(\frac{1}{2}|t^{(n_{k+1})}-z^{(n_{k+1})}|^2\right) \tag{8-1}
\]
&lt;/div&gt;

&lt;p&gt;すべての学習データに対する誤差関数ではなく，\(n_{k+1}\)番目の学習データに対する誤差関数&lt;/p&gt;

&lt;div&gt;
\[
    E^{(n_{k+1})}(\boldsymbol{w})=\frac{1}{2}|t^{(n_{k+1})}-z^{(n_{k+1})}|^2 \tag{8-2}
\]
&lt;/div&gt;

&lt;p&gt;の勾配方向にパラメータを更新している．&lt;/p&gt;

&lt;h3&gt;SGDの利点&lt;/h3&gt;

&lt;p&gt;SGDにはナイーブな勾配法に比べて以下のような利点があげられる．&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;局所最適解にトラップしにくい（勾配法の初期値依存問題への解決）&lt;/li&gt;
&lt;li&gt;冗長な学習データがある場合，勾配法よりも学習が高速&lt;/li&gt;
&lt;li&gt;学習データを収集しながら逐次的に学習できる&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;誤差逆伝播法(Back Propagation)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/08/perceptron.003.jpg&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;原理的には(8-1)の更新式を用いれば，SGDにより最適パラメータが推定できる．
今回は1段目はロジスティック関数を活性化関数に用い，2段目では恒等関数\(f(x)=x\)を活性化関数に用いるものとする．&lt;/p&gt;

&lt;p&gt;図のパーセプトロンに対して(8-2)の微分を計算しようとすると，2段目（青い部分）は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial E^{(n)}}{\partial\boldsymbol{v}}
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial z^{(n)}}{\partial\boldsymbol{v}} \\
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial}{\partial\boldsymbol{v}}(\boldsymbol{v}^{\mathrm{T}}\boldsymbol{y}) \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{y}
\end{align}
&lt;/div&gt;

&lt;p&gt;1段目は（\([x_1, \cdots,x_4]\mapsto y_1\)，すなわち赤い部分にのみ着目すると）&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial E^{(n)}}{\partial\boldsymbol{w_1}}
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial z^{(n)}}{\partial\boldsymbol{w_1}} \\
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial}{\partial\boldsymbol{w_1}}(\boldsymbol{v}^{\mathrm{T}}\boldsymbol{y}) \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\frac{\partial\boldsymbol{y}}{\partial\boldsymbol{w_1}} \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\frac{\partial}{\partial\boldsymbol{w_1}}(\sigma(\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x})) \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\left(\frac{\partial\sigma(u)}{\partial u}\right)_{u=\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x}}\frac{\partial(\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{w_1}^{\mathrm{T}}} \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\left(\frac{\partial\sigma(u)}{\partial u}\right)_{u=\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x}}\;\;\boldsymbol{x}^{\mathrm{T}} \\
    &amp;= (z^{(n)}-t^{(n)})\sigma&#39;\boldsymbol{v}\boldsymbol{x}^{\mathrm{T}}
\end{align}
&lt;/div&gt;

&lt;p&gt;ただし\(\sigma\)はロジスティック関数とする．&lt;/p&gt;

&lt;div&gt;
\[
    \sigma(x)=\frac{1}{1+\exp(-x)}
\]
&lt;/div&gt;

&lt;p&gt;ここで重要なのは，どちらも&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial(\mathit{Error Function})}{\partial(\overrightarrow{\mathit{Param}})}\propto(\mathit{Error})\times(\overrightarrow{\mathit{Input}})
\]
&lt;/div&gt;

&lt;p&gt;という形になっているという点である（\(z_n-t_n\)が誤差，\(\boldsymbol{x,y}\)が入力ベクトル）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/08/perceptron.gif&quot; alt=&quot;パーセプトロンの更新&quot;&gt;&lt;/p&gt;

&lt;p&gt;イメージとしてはこのGIFのように，入力ベクトルの方向にパラメータを修正することを繰り返すことになる
（このGIFでは固定係数\(\eta\)による更新だが，ここでは固定係数ではなく誤差がかかった変動係数である）．&lt;/p&gt;

&lt;h2&gt;多層パーセプトロンのテスト&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/08/digit.py&quot;&gt;digit.py&lt;/a&gt;はscikit-learnに含まれるMNIST datasets(手書きの数字)を多層パーセプトロンで分類している．
自前実装なのでパラメータの初期値の取り方や中間層数の取り方を適当にやってしまっている．&lt;/p&gt;

&lt;p&gt;初期値ベクトルの取り方については&lt;a href=&quot;http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&quot;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;を参照するとよさそう．&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/stochastic-gradient-descent.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/stochastic-gradient-descent.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#07 ニューラルネットワーク</title>
        <description>&lt;p&gt;ニューラルネットワークは線形識別モデルの一つで，生体の神経回路（シナプスは他のシナプスから電気信号を入力として受け付け，ある一定量以上の刺激を受けたら発火して次のシナプスへ出力する）を模したモデルである．
最初期のものは60年以上前に提案されたもので，それ以降現在に至るまでパーセプトロン，ボルツマンマシン，オートエンコーダ等多数のアルゴリズムが考案され，昨今の&lt;u&gt;Deep Learning&lt;/u&gt;のブームにつながっている．&lt;/p&gt;

&lt;p&gt;神経回路を模した，といっても実際には特徴量を数学的に重み付けして評価関数を最適化する，今までのような統計的解析手法にすぎない．
しかし，多層パーセプトロンや深層学習のように多層にすればするほど，モデルの表現能力が飛躍的に向上することがわかったため，画像処理や音声処理をはじめとした多くの分野で応用されるようになっている．&lt;/p&gt;

&lt;p&gt;今後の分科会はニューラルネットワークを中心に，およそ3回でDeep Learningに到達する予定である．&lt;/p&gt;

&lt;h2&gt;単層パーセプトロン&lt;/h2&gt;

&lt;p&gt;一般的にグラフ理論におけるネットワークとは，双方向かつ閉路等も含む，ノードとエッジとフローから構成されるものである．
パーセプトロンとは，ニューラルネットワークの中で一方向なものである．
順伝播型ネットワークとも呼ばれる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/perceptron.png&quot; alt=&quot;パーセプトロンの模式図&quot;&gt;&lt;/p&gt;

&lt;p&gt;この図で，最も右のノードは&lt;/p&gt;

&lt;div&gt;
\[
    u=w_0+w_1x_1+\cdots+w_nx_n=\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}
\]
&lt;/div&gt;

&lt;p&gt;という入力を受け付ける．この部分は「他のシナプスから電気信号が伝わってくる」部分に対応する．&lt;/p&gt;

&lt;p&gt;「ある一定量以上の刺激を受けたら発火して出力する」ことを表現するために入力\(u\)を次の関数にかける．&lt;/p&gt;

&lt;div&gt;
\[
    f(x)=
    \begin{cases}
        1\;\;(x\ge 0) \\
        0\;\;(x\lt 0)
    \end{cases}
\]
&lt;/div&gt;

&lt;p&gt;これはhinge関数と呼ばれる階段上の関数である．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/hinge.png&quot; alt=&quot;hinge関数&quot;&gt;&lt;/p&gt;

&lt;p&gt;この関数によって出力を\(z=f(u)\)と得ると，純粋に「ある一定値を超えると発火する」ことを実現できるが，実際には連続値をとりhinge関数に形状が似たLogistic関数が利用されることが多い．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/logistic.png&quot; alt=&quot;Logistic関数&quot;&gt;&lt;/p&gt;

&lt;p&gt;\(\beta\)はパラメータ（正数）であり，大きければ大きいほどロジスティック関数の勾配は急になる．&lt;/p&gt;

&lt;p&gt;このようにパーセプトロンの出力にかける関数のことを活性化関数という．&lt;/p&gt;

&lt;h2&gt;多層パーセプトロン&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/perceptron.001.jpg&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;多層パーセプトロンは，単層パーセプトロンを重ねたものである．
入出力と活性化は基本的に単層パーセプトロンと同じである．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/perceptron.002.jpg&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;このように部分的に抽出すると，単層パーセプトロンとして見做すことができる．&lt;/p&gt;

&lt;p&gt;これからよく登場するのは2層の多層パーセプトロンになる．
多層になればなるほど学習時のパラメータの推定が計算量の面から困難になるからである．
2層のパーセプトロンにおいて，&lt;u&gt;中間層の活性化関数にロジスティック関数，出力層の活性化関数に恒等関数\(f(x)=x\)を用いる．&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;パーセプトロンを多層にすることで一般的に表現能力が上がり，多彩な分類・認識を行うことができる．
しかし，同時にパラメータ推定を行うのに飛躍的にリソースが増加する．
（その問題を上手く解決したのがDeep Learningであるといえる）&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/neural-network.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/neural-network.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#06 線形識別モデル</title>
        <description>&lt;p&gt;今回から本格的に機械学習らしい内容に入っていく．&lt;/p&gt;

&lt;p&gt;機械学習で扱う問題のほとんどは識別問題，すなわち与えられたデータの属性を推定する問題であることが多い．&lt;/p&gt;

&lt;p&gt;(例)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;スパムメール判定（受信したメールがスパムなのかそうでないのか）&lt;/li&gt;
&lt;li&gt;Googleの言語判定（テキストが何語で書かれているのか）&lt;/li&gt;
&lt;li&gt;YouTubeのおすすめ動画&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/spam.png&quot; alt=&quot;スパムフィルタ&quot;&gt;
↑GMailのスパムフィルタ&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/lang.png&quot; alt=&quot;言語判定&quot;&gt;
↑Googleの言語判定&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/youtube.png&quot; alt=&quot;おすすめ動画&quot;&gt;
↑YouTubeのおすすめ動画&lt;/p&gt;

&lt;h2&gt;それは機械学習が必要なのか&lt;/h2&gt;

&lt;p&gt;昨今の機械学習ブームの影響で，とりあえず機械学習を使ってみたいというケースが増えている（僕もそのクチだ）．
ただ，何でもかんでもとりあえず機械学習で識別させておけばいいわけではない．
例えば，自動販売機に投入される硬貨の種類を判定するのにわざわざ機械学習を用いる必要はない．
対象はせいぜい1円玉，5円玉，10円玉，50円玉，100円玉，500円玉の6種類しかないので，rule-basedに記述した方が速い．&lt;/p&gt;

&lt;p&gt;機械学習を用いた方が良いのは，処理すべきデータ規模が人力では扱えないほど大きかったり，ruleが曖昧な場合などである．
（「スパムメールの基準を書き出せ」と言われたら難しいだろう）&lt;/p&gt;

&lt;p&gt;また，機械学習を用いるにしても，徹頭徹尾全て機械学習に頼りきらなければいけない道理はどこにもない．
サイボウズ・ラボの中谷秀洋氏に話を伺う機会があったのだが，言語判定をやっていると異言語間で同じ綴りの単語が使われているケース等で，機械学習ではどうしても乗り越えられない壁に当たる時があるのだが，そういう場合は普通にif文一つかませてしまうと精度が上がるそうだ．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/shuyo/dsirnlp&quot;&gt;言語判定へのいざない - SlideShare&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;線形識別関数&lt;/h2&gt;

&lt;p&gt;それでは前置きから本題に入ろう．テキスト処理の例を据えながら説明する．
ここでは，ブログ記事のトピックが統計に関連するのかしないのかを判定するようなプログラムを作ることを考えてみよう．
まず，記事中に「統計」という言葉が登場していたら機械学習に関連する記事だと判定することにしよう．
これを式として表すなら，「統計」という単語の登場回数を\(x\)として，&lt;/p&gt;

&lt;div&gt;
\[
    f(x)=x-1
\]
&lt;/div&gt;

&lt;p&gt;という識別関数を考え，&lt;/p&gt;

&lt;div&gt;
\begin{align}
    f(x)\ge 0 &amp;\Rightarrow \mathrm{related}\\
    f(x)\lt 0 &amp;\Rightarrow \mathrm{not\,related}
\end{align}
&lt;/div&gt;

&lt;p&gt;というように書き表せる（自明に見えるかもしれないが丁寧に進める）．&lt;/p&gt;

&lt;p&gt;しかし，「統計」という単語が含まれているからといって，必ずしも統計に関する文章であるとは限らない．
（例えば&lt;a href=&quot;http://www.benricho.org/weather_ratio/&quot;&gt;このページ&lt;/a&gt;には「統計」という単語が登場するが，どちらかといえば天気に関する文章だ）
記事の長さは記事ごとによってまちまちなので，記事に出現する全単語数で割った値を用いた方がよいかもしれない（正規化という）．
また，「統計」という単語が登場しなくても「検定」や「有意」という単語が登場しているなら統計に関係あるかもしれない．&lt;/p&gt;

&lt;p&gt;これを踏まえて，次は「統計」「検定」「有意」という単語の登場回数を記事中の全単語数で割った値をそれぞれ\(x_1,x_2,x_3\)とし，&lt;/p&gt;

&lt;div&gt;
\[
    f(x_1,x_2,x_3)=x_1+3x_2+2x_3-r
\]
&lt;/div&gt;

&lt;p&gt;という識別関数を考える．relatedとnot relatedの判別基準は先ほどと同様とする．
3や2といった係数は僕が適当に与えたもので，この場合「検定」や「有意」が登場すると「統計」が登場したときよりもrelatedと判断しやすくなる．&lt;/p&gt;

&lt;p&gt;今ここで考えている識別関数は，記事から\(\{x_i\}_{i=1}^N\)というパラメータを抽出し，それを線形写像（1次関数）で処理している．&lt;/p&gt;

&lt;p&gt;一般化する．&lt;/p&gt;

&lt;p&gt;2クラス問題\(C_1,C_2\)を分類する線形識別関数は次のように書ける．&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+w_0
\]
&lt;/div&gt;

&lt;p&gt;識別の基準は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    C_1 &amp; \;\mathrm{if}\; f(\boldsymbol{x})\ge 0\\
    C_2 &amp; \;\mathrm{if}\; f(\boldsymbol{x})\lt 0
\end{align}
&lt;/div&gt;

&lt;p&gt;とする．&lt;/p&gt;

&lt;p&gt;\(\boldsymbol{x}\)は入力ベクトル，\(\boldsymbol{w}\)は係数ベクトル，\(w_0\)はバイアス項と呼ばれる．&lt;/p&gt;

&lt;p&gt;ここでバイアス項を\(\boldsymbol{w}\)の第(N+1)要素に追加し，\(\boldsymbol{x}\)の第(N+1)要素に1を追加すると&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}
\]
&lt;/div&gt;

&lt;p&gt;と書くことができる．
こちらの方が一般的には扱いやすいので，以降バイアス項は省略する．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/leastsq.png&quot; alt=&quot;二乗誤差&quot;&gt;&lt;/p&gt;

&lt;p&gt;ちなみに，\(\boldsymbol{w}^T\boldsymbol{x}=0\)は\(\boldsymbol{w}\)を法線ベクトルとする超平面の式になる．
つまり\(\boldsymbol{w}^T\boldsymbol{x}\)はベクトル\(\boldsymbol{x}\)の超平面からの距離になる．&lt;/p&gt;

&lt;h2&gt;学習と推定&lt;/h2&gt;

&lt;p&gt;上記の説明では，人間側が適当に係数をいじっていたが，実際に「検定」を重視したほうがいいのか，「有意」を重視したほうがいいのか，ということはデータを見なければわからない．
したがって，コンピュータに大量の「あらかじめ統計に関連するかどうかわかっている」データを処理させ，最適な係数を計算によって求めさせる．
これが機械学習のうち，学習のフェーズになる．&lt;/p&gt;

&lt;p&gt;学習さえしてしまえば推定は簡単で，「統計に関連するかどうか知りたい」記事のデータについて\(f(\boldsymbol{x})\)の値を計算して，上記の識別の基準にしたがって判断すればよい．&lt;/p&gt;

&lt;p&gt;ここで問題になるのは，&lt;u&gt;どのようにしてデータから最適なパラメータを計算するか&lt;/u&gt;ということになる．ここではシンプルな評価関数として二乗誤差を考える．&lt;/p&gt;

&lt;h2&gt;最小二乗誤差基準&lt;/h2&gt;

&lt;p&gt;上の例では識別関数が正のクラスが正の値のときは\(C_1\)，負の値のときは\(C_2\)に分類される．
ここで行いたいのは，学習データをコンピュータに与えて，最適なパラメータ\(\boldsymbol{w}\)を計算することである．
これを以下のように行う．&lt;/p&gt;

&lt;div style=&quot;border: solid 1px; padding: 20px; margin: 10px;&quot;&gt;
学習データに対して，\(C_1\)ならば+1，\(C_2\)ならば-1を返すように\(f(\boldsymbol{x})\)を設計する．
そのために二乗誤差\((t_i-f(\boldsymbol{x_i}))^2\)を計算し，その総和が最小になるような\(\boldsymbol{w}\)を計算する．
(x_iはi番目の学習データ．t_iはi番目の学習データのラベル値で，{1,-1})
&lt;/div&gt;

&lt;p&gt;すなわち，最小化すべきは&lt;/p&gt;

&lt;div&gt;
\[
    J(\boldsymbol{w})=\sum_{i=1}^K(t_i-\boldsymbol{w}^T\boldsymbol{x_i})^2
\]
&lt;/div&gt;

&lt;p&gt;この式は次のように書きなおすことができる．&lt;/p&gt;

&lt;div&gt;
\[
    J(\boldsymbol{w})=\|\boldsymbol{t}-X\boldsymbol{w}\|^2 \tag{6-1}
\]
&lt;/div&gt;

&lt;p&gt;ここで\(\boldsymbol{t}=(t_1 t_2 \cdots t_K)^T\)，&lt;/p&gt;

&lt;div&gt;
\[
    X = \left(\begin{array}{c}
        \boldsymbol{x_1}^T \\
        \boldsymbol{x_2}^T \\
        : \\
        \boldsymbol{x_K}^T
    \end{array}\right)
\]
&lt;/div&gt;

&lt;h3&gt;最適解&lt;/h3&gt;

&lt;p&gt;今一度解くべき問題を整理すると，評価関数(6-2)\(J(\boldsymbol{w})\)の最小化である．
ここでいきなり最急降下法にかける前に，式の上で計算してみる．&lt;/p&gt;

&lt;p&gt;まず\(J(\boldsymbol{w})\)を微分すると&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial J}{\partial\boldsymbol{w}}
    &amp;= \frac{\partial}{\partial\boldsymbol{w}}(\boldsymbol{w}^TX^TX\boldsymbol{w}-\boldsymbol{w}X^T\boldsymbol{t}-\boldsymbol{t}^TX\boldsymbol{w}+\|\boldsymbol{t}\|^2) \\
    &amp;= 2X^TX\boldsymbol{w}-2X^T\boldsymbol{t} \\
\end{align}
&lt;/div&gt;

&lt;p&gt;ここでは制約条件はついていないので，単純に導関数=0とおくと&lt;/p&gt;

&lt;div&gt;
\[
    X^TX\boldsymbol{w}=X^T\boldsymbol{t}
\]
&lt;/div&gt;

&lt;p&gt;これは&lt;u&gt;正規方程式&lt;/u&gt;と呼ばれている．
したがって&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{w}=(X^TX)^{-1}X^T\boldsymbol{t}
\]
&lt;/div&gt;

&lt;p&gt;という解が得られる．&lt;/p&gt;

&lt;h2&gt;簡単な例&lt;/h2&gt;

&lt;p&gt;初めにインストールしたモジュールの中に&lt;code&gt;scikit-learn&lt;/code&gt;というモジュールがある．
これはPython向けの機械学習モジュールで，様々なアルゴリズムが提供されていたり，サンプルデータが提供されている．
ここではiris(アヤメ)のサンプルデータを用いて，最小二乗学習を行ってみる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/load.png&quot; alt=&quot;irisのロード&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sklearn.datasets.load_iris&lt;/code&gt;でirisをロードできる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/data.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;データはこのような4次元データになっている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/feature_name.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;各次元はこのような特徴を表現している．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/target.png&quot; alt=&quot;target&quot;&gt;&lt;/p&gt;

&lt;p&gt;各データのラベル値はこのようになっている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/target_name.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ラベル値はそれぞれsetosa, versicolor, virginicaというアヤメの種類と対応している．&lt;/p&gt;

&lt;p&gt;現時点では2クラス分類しかできないので，「setosaかそうでないか」を推定するプログラムを書いてみる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 最小二乗法で学習&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 最小二乗法で推定&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# テストデータに対する正答率&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/iris.png&quot; alt=&quot;例&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;の行はirisを(学習データ):(テストデータ)=4:1に分割している(test_size=0.2)．&lt;code&gt;train_x, train_y&lt;/code&gt;を使って\(\boldsymbol{w}\)を推定し，できた識別器に&lt;code&gt;test_x&lt;/code&gt;をかけて&lt;code&gt;pred_y&lt;/code&gt;を&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;で得て，そのうち何%が&lt;code&gt;test_y&lt;/code&gt;と一致しているかを&lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt;を用いて&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;の行で計算している．&lt;/p&gt;

&lt;p&gt;この程度であれば100%の精度を達成することができる．&lt;/p&gt;
</description>
        <pubDate>Sun, 24 May 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/24/linear-recognition-model.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/24/linear-recognition-model.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#05 確率分布の基礎</title>
        <description>&lt;p&gt;確率は統計解析と密接に関係している．
例えば，株価のトレードデータから数日後の株価を予測する際，過去の統計データをもとに確率的に最もありえる株価でもって予測したりする．
また，患者の状態を観測してそこから医者が&amp;quot;最も取るべき行動&amp;quot;を推論し，医療に応用するといったこともなされる．&lt;/p&gt;

&lt;p&gt;本節では，確率解析のベースとなる確率分布の基礎を説明する．&lt;/p&gt;

&lt;h2&gt;二項分布&lt;/h2&gt;

&lt;p&gt;確率分布の最も単純な例として，コイン投げを考える．
確率\(p=\frac{1}{2}\)で表，\(q=1-p=\frac{1}{2}\)で裏が出るようなコインを考える．
確率変数\(x\)に対して「表が出る」事象を1，「裏が出る」事象を0とする．
コインを\(n\)回投げるとき，表が出る回数が\(k\)回になる確率は以下のように表される．&lt;/p&gt;

&lt;div&gt;
\[
    P(x=k) = {}_nC_k p^k(1-p)^{n-k} = \frac{n!}{k!(n-k)!} p^k(1-p)^{n-k}
\]
&lt;/div&gt;

&lt;p&gt;（高校で学ぶ反復事象の確率そのものである）&lt;/p&gt;

&lt;p&gt;このような確率変数\(x\)の分布はパラメータ\(n,p\)で決定されるため，二項分布\(B(n,p)\)に従うといい\(x\sim B(n,p)\)と書く．&lt;/p&gt;

&lt;p&gt;二項分布をとるのは，試行がベルヌーイ試行（試行の結果が二値）であり，互いに独立であるような場合である．
上の例ではコイントスが試行であり，コイントス自体は表と裏の二値で，一回前のトスの結果は次のトスには影響しない．&lt;/p&gt;

&lt;h3&gt;例1&lt;/h3&gt;

&lt;p&gt;以下のスクリプトは二項分布に従って確率変数をランダムに生成している．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;コイントスの例で言えば，「表裏が出る確率の等しい(p=0.5)コインを20回(=n)投げる」という試行を10回行っている．
実行例は以下のとおり．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;array([12, 11,  9,  9,  7, 10, 10, 10,  9, 11])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;このように，だいたい10回前後表が出ていることがわかる．&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;scipy.stats&lt;/code&gt;には他にも様々な確率分布に関するモジュールが提供されている．
また，各々のモジュールは統一的なインターフェースで提供されている（例えば，他の確率分布に従って確率変数をランダム生成する場合も&lt;code&gt;rvs&lt;/code&gt;を使う）．
一度&lt;a href=&quot;http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.binom.html&quot;&gt;scipy.stats.binom&lt;/a&gt;のドキュメントのMethodsの欄に目を通してみるといいかもしれない．&lt;/p&gt;

&lt;h3&gt;正規性&lt;/h3&gt;

&lt;p&gt;確率分布は「すべて足して1」にならなければならない．それを二項分布で確かめる．&lt;/p&gt;

&lt;div&gt;
\[
    \sum_{k=0}^n{}_nC_kp^k(1-p)^{n-k} = \{p+(1-p)\}^n = 1
\]
&lt;/div&gt;

&lt;p&gt;この計算には二項定理を用いている．&lt;/p&gt;

&lt;h3&gt;期待値と分散&lt;/h3&gt;

&lt;p&gt;二項分布の期待値は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \mathbb{E}(x)
        &amp;= \sum_{k=0}^n kP(k) \\
        &amp;= \sum_{k=0}^n k\cdot\frac{n\cdot(n-1)!}{k\cdot(k-1)!(n-k)!}p^k(1-p)^{n-k} \\
        &amp;= \sum_{k=0}^n k\cdot\frac{n}{k}\cdot{}_{n-1}C_{k-1}p^k(1-p)^{n-k} \\
        &amp;= np\sum_{k=0}^n {}_{n-1}C_{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)} \\
        &amp;= np\cdot\{p+(1-p)\}^n \\
        &amp;= np
\end{align}
&lt;/div&gt;

&lt;p&gt;となる．表裏の出る確率がイーブンなコインを10回投げるとだいたい5回(=10・(1/2))表になる直感にそっている．&lt;/p&gt;

&lt;p&gt;分散は&lt;/p&gt;

&lt;div&gt;
\[
    \mathrm{Var}(x)=np(1-p)
\]
&lt;/div&gt;

&lt;p&gt;である．&lt;/p&gt;

&lt;h3&gt;練習1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;二項分布の分散を計算せよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;期待値と似たような計算で確認できる．&lt;/p&gt;

&lt;h2&gt;正規分布と中心極限定理&lt;/h2&gt;

&lt;p&gt;二項分布の試行回数\(n\)を十分大きくしていくと，どうなるだろうか．&lt;/p&gt;

&lt;p&gt;コイントスの例で考える．トスの回数\(n\)を1回から100回まで増やしていく．
各\(n\)において，同じ試行を10000回行い，表の出た回数をヒストグラムにとってみる．
これは以下のようなスクリプトで実現できる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 二項分布の生成&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# キャンバスのクリア&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ヒストグラムの描画（binsはヒストグラムの分割数、あまり気にしなくて良い）&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;n = {}&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;fig{}.png&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;この結果得られたプロットをGIFアニメーションにしたのが下の画像である．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/05/binom.gif&quot; alt=&quot;二項分布の近似&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このように，二項分布の形状が次第に一定のカーブに近づいていく．
この分布は正規分布と呼ばれ，次のような式で表される．&lt;/p&gt;

&lt;div&gt;
\[
    P(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\]
&lt;/div&gt;

&lt;p&gt;\(\mu\)は平均（＝期待値），\(\sigma\)は分散である．&lt;/p&gt;

&lt;p&gt;実際に\(n=100\)のヒストグラムに正規分布を重ねてプロットすると以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/05/normal.png&quot; alt=&quot;正規分布&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このように&lt;strong&gt;二項分布のもとで試行を繰り返すと，その和の分布は次第に正規分布に近づく．&lt;/strong&gt;
この事実は中心極限定理と呼ばれる．中心極限定理の証明は難しいので割愛する．
中心極限定理の存在により，大標本の場合にはまず正規分布を仮定してデータ解析することも多く，正規分布は数ある確率分布でも特殊な位置を占めている．&lt;/p&gt;

&lt;h3&gt;練習2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;サイコロを振って1〜6の目が出る確率は（イカサマされていないという前提で）すべて等しく1/6である．
この分布は
$$ P(x)=\frac{1}{6} $$
と書け，一様分布という．
一様分布も二項分布と同様に，正規分布に収束することをスクリプトで確認せよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;収束はそれなりに速く，\(n=10\)程度でかなり正規分布に近い形になる．&lt;/p&gt;

&lt;p&gt;スクリプト例を&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/05/uniform.py&quot;&gt;uniform.py&lt;/a&gt;に示す．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/05/uniform.gif&quot;&gt;uniform.gif&lt;/a&gt;のような結果になれば概ねOKだろう．&lt;/p&gt;
</description>
        <pubDate>Tue, 12 May 2015 02:14:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/12/probability.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/12/probability.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#04 最適化手法</title>
        <description>&lt;p&gt;最適化とは簡単に言うと関数の最大値・最小値を求めることである．
コンピュータサイエンスの応用分野では最適化手法が幅広く用いられている．
機械学習の例で説明すると，例えばデータを学習して&amp;quot;最適な&amp;quot;パラメータを決定するというのは，結局データを引数として取る評価関数を最大化することに他ならない．&lt;/p&gt;

&lt;p&gt;しかし，コンピュータで最大値・最小値を計算するのは思ったほど簡単なことではない．
例えば人間なら導関数の零点を見つける際は単純に方程式を解くだけだが，コンピュータに方程式をただ投げても適切なアルゴリズムがなければ解は返ってこない．&lt;/p&gt;

&lt;p&gt;この節では，以下の2つのアルゴリズムを説明する．&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Lagrangeの未定乗数法&lt;/p&gt;

&lt;p&gt;制約条件付きの最適化問題を解く，数理解析的アルゴリズム．
コンピュータが直接的に扱うのは難しいが，様々な最適化アルゴリズムのベースになっている．&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最急降下法&lt;/p&gt;

&lt;p&gt;関数の最大値・最小値を，その導関数を用いて探索的に求めるアルゴリズム．
コンピュータにとって扱いやすく，アルゴリズム自体もシンプル（故に問題点も多い）．&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Lagrangeの未定乗数法&lt;/h2&gt;

&lt;p&gt;1変数関数の制約条件付き最大値・最小値を求める際は，導関数の零点から極値をとる値を求め，実際に極値同士の値を比較して最大値・最小値を求めていた．&lt;/p&gt;

&lt;p&gt;多変数関数の場合は，&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f(\boldsymbol{a})}{\partial x_1} = 0,\cdots,\frac{\partial f(\boldsymbol{a})}{\partial x_n} = 0
\]
&lt;/div&gt;

&lt;p&gt;だからといって\(f(\boldsymbol{a})\)が極値になっているとは限らない．&lt;/p&gt;

&lt;p&gt;例えば\(f(x,y)=x^2+y^3\)のグラフをプロットすると次のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/critical_point.png&quot; alt=&quot;臨界点&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;この関数の\((x,y)=(0,0)\)では，\(x,y\)ともに偏導関数は\(0\)になっているが，極値にはなっていない．
（極値であるかどうかに関わらず，偏導関数がすべて\(0\)になるような点を停留点，または臨界点という）&lt;/p&gt;

&lt;p&gt;1変数の場合でも極値にならない停留点は存在する（例えば\(f(x)=x^3\)の\(x=0\)）が，1変数関数の場合は増減表を書いて確かめることができる．
しかし多変数関数では増減表が書けない．ここに制約条件が付いてくると更に複雑になる．&lt;/p&gt;

&lt;p&gt;このような多変数関数の制約条件付き最大最小問題を解くにはLagrangeの未定乗数法が用いられる．&lt;/p&gt;

&lt;h3&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;制約条件\(g(\boldsymbol{x})=0\)の下で多変数関数\(f(x)\)の極値を与える\(\boldsymbol{x}\)は，次のような関数（Lagrange関数という）&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(\boldsymbol{x},\lambda) = f(\boldsymbol{x})+\lambda g(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;に対して，以下の連立方程式の解として与えられる．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial \tilde{f}}{\partial\boldsymbol{x}} &amp;= 0 \\
    \frac{\partial \tilde{f}}{\partial\lambda} &amp;= 0
\end{align}
&lt;/div&gt;

&lt;h3&gt;例1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;maximize \(f(x,y)=2x+3y\) s.t. \(x^2+y^2=1\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;次のようなLagrange関数を作る．&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(x,y,\lambda)=2x+3y+\lambda(x^2+y^2-1)
\]
&lt;/div&gt;

&lt;p&gt;続いて\(\tilde{f}\)を\(x,y,\lambda\)でそれぞれ偏微分して「=0」とする．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial\tilde{f}}{\partial x} &amp;= 2+2\lambda x = 0 \\
    \frac{\partial\tilde{f}}{\partial y} &amp;= 3+2\lambda y = 0 \\
    \frac{\partial\tilde{f}}{\partial\lambda} &amp;= x^2+y^2-1 = 0 \\
\end{align}
&lt;/div&gt;

&lt;p&gt;この方程式から\(\lambda\)を消去すると&lt;/p&gt;

&lt;div&gt;
\[
    (x,y) = (\pm\frac{2}{\sqrt{13}},\pm\frac{3}{\sqrt{13}})
\]
&lt;/div&gt;

&lt;p&gt;という解が得られる．これらの組は最大値を与える変数の候補になっている．
実際に\(f(x,y)\)に代入してみると，\((x,y)=(\frac{2}{\sqrt{13}},\frac{3}{\sqrt{13}})\)のときに最大値\(\sqrt{13}\)をとり，
\((x,y)=(-\frac{2}{\sqrt{13}},-\frac{3}{\sqrt{13}})\)のときに最小値\(-\sqrt{13}\)をとることがわかる．&lt;/p&gt;

&lt;p&gt;実際にグラフをプロットしてみると以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/lagrange.png&quot; alt=&quot;未定乗数法の例&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;練習1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;例1のプロットをPythonを使って作成してみよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;/ml2015/ml/2015/05/10/multivariate-analysis.html&quot;&gt;#03&lt;/a&gt;や&lt;a href=&quot;http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html&quot;&gt;matplotlibのチュートリアル&lt;/a&gt;を参考にするとよい．
3D空間で線を描画するには&lt;code&gt;Axes3D.plot&lt;/code&gt;，点(散布図)を描画するには&lt;code&gt;Axes3D.scatter&lt;/code&gt;を用いる．&lt;/p&gt;

&lt;p&gt;（スクリプト例: &lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex1.py&quot;&gt;ex1.py&lt;/a&gt;）&lt;/p&gt;

&lt;h3&gt;練習2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;半径1の球に内接する直方体の体積の最大値を求めよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;解答は&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex2.md&quot;&gt;ex2.md&lt;/a&gt;に示す．&lt;/p&gt;

&lt;h3&gt;Lagrangeの未定乗数法の原理&lt;/h3&gt;

&lt;p&gt;厳密な証明は難しく，この分科会の内容から少々逸脱してしまうので，ここでは割愛する．
イメージを掴みたい人は，&lt;a href=&quot;https://github.com/levelfour/machine-learning-2014/wiki/%E7%AC%AC4%E5%9B%9E---Lagrange%E3%81%AE%E6%9C%AA%E5%AE%9A%E4%B9%97%E6%95%B0%E6%B3%95#%E8%A8%BC%E6%98%8E&quot;&gt;昨年の分科会の資料&lt;/a&gt;を参考にするとよい．&lt;/p&gt;

&lt;h3&gt;複数制約条件の場合&lt;/h3&gt;

&lt;p&gt;ここでは制約条件が1つの場合のみについて述べたが，制約条件が\(M\)個（\(\{g_m(\boldsymbol{x})=0\}_{m=1}^M\)）ある場合は&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(\boldsymbol{x},\boldsymbol{\lambda}) = f(\boldsymbol{x})+\sum_{m=1}^M\lambda_mg_m(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;というLagrange関数をつくり，以下の連立方程式を解く．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial \tilde{f}}{\partial\boldsymbol{x}} &amp;= 0 \\
    \frac{\partial \tilde{f}}{\partial\boldsymbol{\lambda}} &amp;= 0
\end{align}
&lt;/div&gt;

&lt;h3&gt;不等式制約条件の場合&lt;/h3&gt;

&lt;p&gt;ここで述べたのは制約条件が等式の場合のみであった．制約条件が不等式になっているケースも実用上は多く登場する．
その場合は少しだけ複雑になる（Karush-Kuhn-Tucker条件の導入）ので，実際に用いる際に説明する．&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;h2&gt;最急降下法&lt;/h2&gt;

&lt;p&gt;最急降下法は，関数の導関数の値を用いて逐次的に関数値を最大にする解を更新するアルゴリズムである．&lt;/p&gt;

&lt;p&gt;関数\(f(\boldsymbol{x})\)に対して初期値\(x_0\)を与えて，&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}_{n+1}=\boldsymbol{x}_{n}+\eta\frac{\partial f(\boldsymbol{x}_{n})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;p&gt;で\(\boldsymbol{x}\)の値を逐次的に更新し，収束した点が最大になっている．
導関数の値が0になったら収束だが，実際にちょうど0になる時点まで探索し続けると収束が非常に遅くなるので，導関数値が十分に小さくなったら収束と見なす．
イメージとしては，関数値が増大する向き（＝導関数が正）に山を登ることになる．&lt;/p&gt;

&lt;p&gt;\(\eta(&amp;gt;)0\)は学習率で，収束の仕方を決めるパラメータである（大きいほど収束が速いわけではない）．&lt;/p&gt;

&lt;p&gt;最急降下法の更新の過程の様子は以下のアニメーションを参考にしてほしい．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/descent.gif&quot; alt=&quot;最急降下法&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最急降下法で最小値を求める際は，更新式を以下のように変更すればよい．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}_{n+1}=\boldsymbol{x}_n-\eta\frac{\partial f(\boldsymbol{x}_{n})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;h3&gt;最急降下法の問題点&lt;/h3&gt;

&lt;h4&gt;初期値依存性&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/overfit.png&quot; alt=&quot;初期値依存性&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上に示す図のように，最急降下法では初期値の取り方によって収束先が変わり得る．
それは，最急降下法は大域的な最大値を求めているのではなく，局所的最大値（すなわち極大値）を求めているからにすぎない．
そのため，実際には解析対象となる関数の特性を把握しつつ初期値を選択したり，複数の初期値に対して試行する必要がある．&lt;/p&gt;

&lt;h4&gt;収束速度&lt;/h4&gt;

&lt;p&gt;最急降下法は原理がシンプルな反面，収束速度が遅いことで知られる．
学習率\(\eta\)の選び方で収束速度は制御できるが，あまり大きな値にしすぎると見当違いな値に収束することもあり，あまり小さな値にしすぎると収束精度は向上するが収束速度は遅くなるといった問題がある．&lt;/p&gt;

&lt;h3&gt;例2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;#03の例1でプロットした下の関数の最大値を最急降下法で求める．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x}) = \exp\left(-\frac{(x-0.5)^2+(y-0.2)^2}{2}\right)
\]
&lt;/div&gt;

&lt;p&gt;この関数を微分すると以下のようになる．&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f}{\partial\boldsymbol{x}}
    = \left(\begin{array}{c}
        0.5-x \\
        0.2-y
    \end{array}\right) f(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;最大値を求めるPythonコードは以下のように書ける．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 学習率&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 反復回数&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 初期値ベクトル&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ini&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ini&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;x({}) = {}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ちなみに，このスクリプトのようにコメントにUnicode文字を使いたい場合は，ファイルの先頭に&lt;code&gt;# -*- coding: utf-8 -*-&lt;/code&gt;または&lt;code&gt;# coding: utf-8&lt;/code&gt;と書く必要がある．
この1行によって，Python処理系はUTF-8でスクリプトを解釈するようになる．&lt;/p&gt;

&lt;h3&gt;練習3&lt;/h3&gt;

&lt;p&gt;例2のスクリプトを実際に動かしてみよ．また，学習率，反復回数，初期値ベクトルを変化させると収束先の値がどのように変化するか，いろいろ試してみよ．&lt;/p&gt;

&lt;h3&gt;練習4&lt;/h3&gt;

&lt;p&gt;例2で，収束するまでの反復回数が最も少なくなるような学習率を探索せよ．0から2の間で試せば十分である．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex4.py&quot;&gt;ex4.py&lt;/a&gt;にスクリプト例を示す．&lt;/p&gt;
</description>
        <pubDate>Mon, 11 May 2015 21:11:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/11/lagrange-multiplier.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/11/lagrange-multiplier.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#03 多変数解析</title>
        <description>&lt;h2&gt;多変数関数&lt;/h2&gt;

&lt;p&gt;1変数関数は\(x\)に対して\(y\)を一意に対応付ける関係のことであった．この関係を\(y=f(x)\)等と書いた．&lt;/p&gt;

&lt;p&gt;多変数関数は，\(n\)個の変数\(x_1,x_2,\cdots,x_n\)を\(y\)に対応づける関係のことであり，
これを\(y=f(x_1,x_2,\cdots,x_n)\)，または\(y=f(\boldsymbol{x})\)と書いたりする．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x} = (x_1 x_2 \cdots x_n)^{\mathrm{T}} \longmapsto f(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;というようにベクトル\(\boldsymbol{x}\)と\(f(\boldsymbol{x})\)の対応付けとして見ることもできる．
こちらの見方の方が後々微分等で直感的に理解しやすいという利点がある．&lt;/p&gt;

&lt;h4&gt;例1&lt;/h4&gt;

&lt;p&gt;多変数関数&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x}) = \exp\left(-\frac{(x-0.5)^2+(y-0.2)^2}{2}\right)
\]
&lt;/div&gt;

&lt;p&gt;をプロットする．&lt;code&gt;matplotlib&lt;/code&gt;を用いると，以下のようなコードでプロットできる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mpl_toolkits.mplot3d&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_wireframe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/ml2015/images/03/gaussian.png&quot; alt=&quot;例1&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;偏微分&lt;/h2&gt;

&lt;p&gt;1変数関数と同様に多変数関数でも微分を行うが，多変数関数では引数が複数個あるので，どの変数で微分するかで導関数が変化する．&lt;/p&gt;

&lt;p&gt;関数\(f(x_1,\cdots,x_n)\)の引数のうち\(x_i\)以外を固定（定数と見なす）し，\(x_i\)で微分したときの導関数を&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f}{\partial x_i}
\]
&lt;/div&gt;

&lt;p&gt;と書き，偏導関数という．&lt;/p&gt;

&lt;h2&gt;ベクトルによる微分&lt;/h2&gt;

&lt;p&gt;多変量解析では&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial(\boldsymbol{a}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;p&gt;のようにベクトルで微分を行う操作が式変形の過程で登場することがある．
これは以下の略記である．&lt;/p&gt;

&lt;div&gt;
\begin{align}   
    \frac{\partial(\boldsymbol{a}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{x}}
    &amp;= \frac{\partial(a_1x_1+\cdots+a_nx_n)}{\partial \left(\begin{array}{c} x_1 \\ : \\ x_n \end{array}\right)} \\
    &amp;= \left(\begin{array}{c} \frac{\partial}{\partial x_1}(a_1x_1+\cdots+a_nx_n) \\ : \\ \frac{\partial}{\partial x_n}(a_1x_1+\cdots+a_nx_n) \end{array}\right) \\
    &amp;= \left(\begin{array}{c} a_1 \\ : \\ a_n \end{array}\right) \\
    &amp;= \boldsymbol{a}
\end{align}
&lt;/div&gt;

&lt;p&gt;このように，ベクトルで微分する場合も結果はスカラーの場合から連想される直感的な結果になる．&lt;/p&gt;

&lt;p&gt;2次形式&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}^{\mathrm{T}}\boldsymbol{Bx}=\sum_{i=1}^n\sum_{j=1}^n b_{ij}x_ix_j
\]
&lt;/div&gt;

&lt;p&gt;の微分も頻出なので，結果のみ示しておく．興味のある人は手を動かして計算してみるとよい．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial\boldsymbol{x}^{\mathrm{T}}\boldsymbol{Bx}}{\partial\boldsymbol{x}}
    &amp;= (\boldsymbol{B}+\boldsymbol{B}^{\mathrm{T}})\boldsymbol{x} \\
    &amp;= 2\boldsymbol{Bx}^{\mathrm{T}} \; (\boldsymbol{B}:\mathrm{symmetric\,matrix})
\end{align}
&lt;/div&gt;
</description>
        <pubDate>Sun, 10 May 2015 00:47:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/10/multivariate-analysis.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/10/multivariate-analysis.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
  </channel>
</rss>
