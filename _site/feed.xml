<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TSG Machine Learning</title>
    <description>TSG Machine Learning Substudy Group in 2015
</description>
    <link>http://sig.tsg.ne.jp/ml2015/</link>
    <atom:link href="http://sig.tsg.ne.jp/ml2015/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 15 Jun 2015 23:26:02 +0900</pubDate>
    <lastBuildDate>Mon, 15 Jun 2015 23:26:02 +0900</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>#10 事前学習</title>
        <description>&lt;h2&gt;事前学習&lt;/h2&gt;

&lt;p&gt;#07，#08のパーセプトロンの問題点の一つとして，初期値をランダムにとっていたことが挙げられる．
度々になるが，勾配法でランダムな初期値からスタートすると，局所解にトラップされてしまう可能性がある．&lt;/p&gt;

&lt;p&gt;この問題の解決策として2006年にGeoffery Hintonによって提案されたのが，オートエンコーダである．
着想は，初期値をランダムに取る代わりに&lt;u&gt;最適な初期値を推定する&lt;/u&gt;ことにある．
つまり，初期値を選択する操作自体を一つの推定問題として捉えるのである．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/10/pretraining1.png&quot; alt=&quot;事前学習&quot;&gt;&lt;/p&gt;

&lt;p&gt;まずはこの図のように最小の層をオートエンコーダとして見立てて，学習を行う．
このときのオートエンコーダの重みの初期値はランダムにとる．
学習はパーセプトロンと同様にSGD等を用いて行う．&lt;/p&gt;

&lt;p&gt;そして，1層目のパラメータの初期値が推定された状態で，残りの層も含めて教師あり学習を行う．
残りの層の初期値はランダムにとる．
これまでのパーセプトロンとの違いは，1層目がオートエンコーダの自己学習を利用して自らパラメータの最適な初期値を学習しているという点である．
このようにオートエンコーダを用いてパーセプトロンの重みの初期値を予め推定しておくことを&lt;strong&gt;事前学習(pre-training)&lt;/strong&gt;という．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/10/pretraining2.png&quot; alt=&quot;事前学習2&quot;&gt;&lt;/p&gt;

&lt;p&gt;Hintonが初めてオートエンコーダを発表した時は，2000→1000→500→30と3段のオートエンコーダを重ねて，元通りの画像を復元するように学習できた例を示している．
上の例でははじめの1層だけだったが，一般に各層の重みをそれぞれオートエンコーダとみなして事前学習を行うことができる．&lt;/p&gt;

&lt;p&gt;【参考】&lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/science.pdf&quot;&gt;Reducing the Dimensionality of Data with Neural Networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ちなみに，なぜオートエンコーダを用いるとよい初期値が得られるのか，という点についてはまだ理論的によくわかっていない．&lt;/p&gt;

&lt;h2&gt;コードによる実例&lt;/h2&gt;

&lt;h3&gt;オートエンコーダ&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/10/autoencoder.py&quot;&gt;autoencoder.py&lt;/a&gt;にコードを示す．&lt;/p&gt;

&lt;p&gt;基本的なニューラルネットの実装は#08で示したdigit.pyのPerceptronクラスを流用している．
AutoEncoderは学習の仕方もほとんどPerceptronと同じなので，Perceptronクラスを継承している（継承しやすいように多少Perceptronクラスは修正した）．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AutoEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_mnodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 入力信号を教師信号として学習&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 中間層出力を得る&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;これがAutoEncoderクラスの実装である．
ほとんどPerceptronクラスと同じなのだが，学習を行うメソッド&lt;code&gt;AutoEncoder.fit&lt;/code&gt;だけ，教師信号として入力自身を与えるように変更している．&lt;/p&gt;

&lt;p&gt;このautoencoder.pyを実行すると，次のようなウインドウが現れる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/10/encode.png&quot; alt=&quot;エンコード結果&quot;&gt;&lt;/p&gt;

&lt;p&gt;左から順に，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1列目: 入力ベクトル&lt;/li&gt;
&lt;li&gt;2列目: オートエンコーダの中間層の出力&lt;/li&gt;
&lt;li&gt;3列目: 出力ベクトル&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;となっている．元の入力画像は8×8の64次元で，中間層では一旦16次元まで落としてそれを4×4にconvertして表示している．
それを出力層にかけると入力画像に近い画像が得られる．&lt;/p&gt;

&lt;p&gt;ちなみに，上から順にそれぞれ9，4，3である．&lt;/p&gt;

&lt;h3&gt;事前学習&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/10/digit.py&quot;&gt;digit.py&lt;/a&gt;にコードを示す．&lt;/p&gt;

&lt;p&gt;このコードは，上のautoencoder.pyに更に修正を加えたものになっている．
重要な変更箇所は以下のとおり．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pretrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 事前学習&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ae&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mid_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ae&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ae&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;このメソッドが事前学習を行っている．事前学習自体はautoencoder.pyで定義したAutoEncoderクラスに投げている．
事前学習でできた重みベクトル\(\boldsymbol{w}\)を&lt;code&gt;AutoEncoder.weight()&lt;/code&gt;で取得し，Perceptronクラスの重みの初期値に設定している．&lt;/p&gt;

&lt;p&gt;この新しいPerceptronクラスで数字認識を行うと，90%程度の精度を出すことができる．&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/15/pretraining.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/15/pretraining.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#09 オートエンコーダ</title>
        <description>&lt;h2&gt;オートエンコーダ&lt;/h2&gt;

&lt;p&gt;#07，#08で扱った多層パーセプトロンは，このような形状をしていた．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/perceptron.001.png&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;入力ベクトルとして\(\boldsymbol{x}\)を与え，教師ベクトルとして\(\boldsymbol{z}\)を与えて，誤差関数を最小化することで学習を行った．
（#07での説明では教師信号は\(z\)というスカラーで説明したが，これをベクトルに拡張するのは難しくない）&lt;/p&gt;

&lt;p&gt;ここで，教師ベクトルを次のようにとってみる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/perceptron.002.png&quot; alt=&quot;オートエンコーダ&quot;&gt;&lt;/p&gt;

&lt;p&gt;すなわち，教師ベクトル=入力ベクトルである．
このパーセプトロンは，入力ベクトルを与えると出力として自分自身を復元することが期待される．&lt;/p&gt;

&lt;p&gt;しかし，目的としているのは入力を復元することではなく，中間層の出力である．
中間層のノード数が入力層よりも少ない状態で入力を復元することができれば，より少ない情報量(=低次元)で入力データを表現できていることになる．
言い換えれば，入力データの冗長性を上手に排除することができる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/perceptron.003.png&quot; alt=&quot;オートエンコーダの中間層&quot;&gt;&lt;/p&gt;

&lt;p&gt;このようなパーセプトロンはオートエンコーダ(自己復号化器，AutoEncoder)と呼ばれる．&lt;/p&gt;

&lt;h2&gt;オートエンコーダと主成分分析の関係&lt;/h2&gt;

&lt;p&gt;学習データの次元を削減したいというのは，元々機械学習の分野ではよく行われてきたことである．
数学的には一般次元で問題を定式化して解析的に解くことはできるが，コンピュータサイエンスの視点から見るとたとえ一般次元で定式化されていても次元が大きすぎると有限時間で計算が終わらないので，次元削減したいというのは自然な考えである．
（「次元の呪い，The curse of dimensionality」という言葉がよく用いられる）
オートエンコーダ以外にも線形代数や統計でよく用いられてきた&lt;u&gt;主成分分析(principal component analysis, PCA)&lt;/u&gt;という手法を簡単に紹介する．&lt;/p&gt;

&lt;h3&gt;主成分分析&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/pca1.png&quot; alt=&quot;3D空間中のデータ&quot;&gt;&lt;/p&gt;

&lt;p&gt;3D空間中にこのようなデータ列があったとする（本来は一般次元で考えるべきだが，視覚化のため3次元とする）．
今，このプロットは通常通り(x,y,z)の直交座標系をとっている．
ちなみに，データ列は&lt;/p&gt;

&lt;div&gt;
\[
    x=y=\frac{z-1}{0.8}
\]
&lt;/div&gt;

&lt;p&gt;という直線上にガウシアンノイズを乗せたものである．&lt;/p&gt;

&lt;p&gt;ここで，「このデータ列を最もよく表現できる軸」をとることを考える．
データ列は直線上にのっているのだから，この直線の方向ベクトル&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{n}=(\begin{array}
        00.5 &amp; 0.3 &amp; 1
    \end{array})
\]
&lt;/div&gt;

&lt;p&gt;を軸としてとるとデータをよく表現できる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/09/pca2.png&quot; alt=&quot;軸の取り方&quot;&gt;&lt;/p&gt;

&lt;p&gt;(縮尺がうまくいかなかった)&lt;/p&gt;

&lt;p&gt;実際には勿論データ列の式は天下りには与えられないので，解析的に推定する必要がある．これを数学的に考察する．&lt;/p&gt;

&lt;p&gt;今とった軸は，データ列をよく表現できる軸，すなわち&lt;u&gt;データ列の分散が最大になるような軸&lt;/u&gt;である．
言い換えると，データ\(\boldsymbol{x_i}\)をベクトル\(\boldsymbol{n}\)に射影した値\(\boldsymbol{n}^{\mathrm{T}}\boldsymbol{x_i}\)が最もばらつくような\(\boldsymbol{n}\)を求める．
すなわち，制約\(|\boldsymbol{n}|=1\)の下で評価関数&lt;/p&gt;

&lt;div&gt;
\[
    J=(\boldsymbol{n},\lambda)=\sum_{k=1}^K(\boldsymbol{n}^{\mathrm{T}}\boldsymbol{x_k})^2+\lambda(1-|\boldsymbol{n}|^2)
\]
&lt;/div&gt;

&lt;p&gt;を最大化する（Lagrangeの未定乗数法）．&lt;/p&gt;

&lt;p&gt;計算は割愛して，評価関数Jの極値条件は&lt;/p&gt;

&lt;div&gt;
\[
    R\boldsymbol{n}=\lambda\boldsymbol{n} \tag{9-1}
\]
&lt;/div&gt;

&lt;p&gt;である．ただし&lt;/p&gt;

&lt;div&gt;
\[
    R=\sum_{k=1}^K\boldsymbol{x_kx_k}^{\mathrm{T}}
\]
&lt;/div&gt;

&lt;p&gt;で，分散・共分散行列という（単なる分散は1次元で，共分散行列は一般次元への拡張）．&lt;/p&gt;

&lt;p&gt;式(9-1)からわかることは，&lt;u&gt;データ列の分散・共分散行列に対する固有値問題の解が，データ列を最もよく表現する&lt;/u&gt;ということである．&lt;/p&gt;

&lt;p&gt;ここでとったベクトル\(\boldsymbol{n}\)を法線ベクトルとする平面は，元の3次元空間の中でデータ列の分散が最も小さくなる平面である．
この平面は\(\boldsymbol{n}\)に対する直交補空間と呼ばれる．
直交補空間内で同様の操作によってさらに軸をとり，その直交補空間内で同様の操作を行い，…を繰り返して軸を次々ととっていく．
この解析手法を&lt;strong&gt;主成分分析(principal component analysis, PCA)&lt;/strong&gt;と呼ぶ．&lt;/p&gt;

&lt;p&gt;主成分分析の本質は次元削減にある．
元のデータの次元が大きすぎるとき，上述の通り何らかの手段によって次元削減したい．
そのようなときにデータ列を最もよく表現する順に軸を元の次元より少なくとれば，元のデータの情報をうまく保持しつつ次元を落とすことができる．
しかしここで注意しなければならないのは，主成分分析の過程では次元とともに情報量も落ちるので，一般には認識精度は低下する．&lt;/p&gt;

&lt;h3&gt;オートエンコーダとの関係&lt;/h3&gt;

&lt;p&gt;主成分分析でとった\(M\)個の固有値ベクトルを行ベクトルとして格納した行列\(\Gamma\)は，次の最小化問題の解になっている．&lt;/p&gt;

&lt;div&gt;
\[
    \min_\Gamma\sum_{k=1}^K|\boldsymbol{x_k}-\Gamma^{\mathrm{T}}\Gamma\boldsymbol{x_i}|
\]
&lt;/div&gt;

&lt;p&gt;一方，&lt;u&gt;中間層の活性化関数が恒等関数であるような&lt;/u&gt;パーセプトロンは，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;中間層出力: \(W\boldsymbol{x_k}\)&lt;/li&gt;
&lt;li&gt;出力層出力: \(W^{\mathrm{T}}W\boldsymbol{x_k}\)&lt;/li&gt;
&lt;li&gt;教師ベクトル: \(\boldsymbol{x_k}\)&lt;/li&gt;
&lt;li&gt;誤差: \(\boldsymbol{x_k}-W^{\mathrm{T}}W\boldsymbol{x_k}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;であるから，\(\Gamma\leftrightarrow W\)と対応付けると主成分分析と同じ問題に落とし込める．&lt;/p&gt;

&lt;p&gt;#10で実際に作成するオートエンコーダは中間層の活性化関数がロジスティック関数なので完全に主成分分析と同じわけではないが，次元削減の基本的な概念は主成分分析をイメージすると良い．&lt;/p&gt;

&lt;p&gt;【参考】&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0893608089900142&quot;&gt;Neural networks and principal component analysis&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/14/autoencoder.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/14/autoencoder.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#08 確率的勾配降下法</title>
        <description>&lt;p&gt;#07ではパーセプトロンを紹介した．
ここではパーセプトロンを用いた学習を紹介する．&lt;/p&gt;

&lt;p&gt;そもそも教師あり学習における学習とは，#06で触れたようにデータセット&lt;/p&gt;

&lt;div&gt;
\[
    \mathcal{D}=\{(\boldsymbol{x_1},t_1),\cdots,(\boldsymbol{x_N},t_N)\}
\]
&lt;/div&gt;

&lt;p&gt;が与えられて，各学習データ\((\boldsymbol{x_i},t_i)\)に対して\(z_i\)という出力が得られるとき，&lt;u&gt;誤差関数を最小にするようにパラメータを推定すること&lt;/u&gt;であった．
また，一般的に誤差関数として二乗誤差関数&lt;/p&gt;

&lt;div&gt;
\[
    E(\boldsymbol{w})=\frac{1}{2}\sum_{i=1}^N|t_i-z_i|^2
\]
&lt;/div&gt;

&lt;p&gt;が用いられることが多い．(先頭の1/2は微分したときに係数が消えるようにつけてあるだけ)&lt;/p&gt;

&lt;p&gt;#06では単純な線形識別関数を用いて識別していたので，解析的に二乗誤差関数の最小化を行うことができた（正規方程式に落とし込める）．
しかし，パーセプトロンが少し複雑になると解析的に解くのは難しくなるので，#04で紹介した最適化手法の勾配法を用いて最小化する．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{w}^{(n+1)}=\boldsymbol{w}^{(n)}-\eta\frac{\partial E}{\partial\boldsymbol{w}}
\]
&lt;/div&gt;

&lt;p&gt;という更新式でパラメータ\(\boldsymbol{w}\)を更新するアルゴリズムであった．&lt;/p&gt;

&lt;p&gt;気をつけたいのは，このとき必要な学習データは事前にすべて読み込ませなければいけないという点である．
なぜなら，二乗誤差関数Eを計算するときにはすべての学習データが必要だからである．
このように学習データをまとめて学習する方法は&lt;strong&gt;バッチ学習&lt;/strong&gt;と呼ばれている．&lt;/p&gt;

&lt;p&gt;ここから紹介するのは，学習データを少しずつ（極端には1つずつ）学習する方法であり，&lt;strong&gt;オンライン学習&lt;/strong&gt;と呼ばれる．&lt;/p&gt;

&lt;h2&gt;確率的勾配降下法(SGD)&lt;/h2&gt;

&lt;p&gt;確率的勾配降下法(Stochastic Gradient Descent = SGD)とは，&lt;u&gt;ランダムに学習データを1つ選んで誤差関数を計算し，その勾配方向にパラメータを修正する操作を反復する&lt;/u&gt;手法である．&lt;/p&gt;

&lt;p&gt;今，\(k+1\)回目のパラメータ更新において\(n_{k+1}\)番目の学習データを選んだとき，更新式は以下のようになる．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{w}^{(k+1)}=\boldsymbol{w}^{(k)}-\eta\frac{\partial}{\partial\boldsymbol{w}}\left(\frac{1}{2}|t^{(n_{k+1})}-z^{(n_{k+1})}|^2\right) \tag{8-1}
\]
&lt;/div&gt;

&lt;p&gt;すべての学習データに対する誤差関数ではなく，\(n_{k+1}\)番目の学習データに対する誤差関数&lt;/p&gt;

&lt;div&gt;
\[
    E^{(n_{k+1})}(\boldsymbol{w})=\frac{1}{2}|t^{(n_{k+1})}-z^{(n_{k+1})}|^2 \tag{8-2}
\]
&lt;/div&gt;

&lt;p&gt;の勾配方向にパラメータを更新している．&lt;/p&gt;

&lt;h3&gt;SGDの利点&lt;/h3&gt;

&lt;p&gt;SGDにはナイーブな勾配法に比べて以下のような利点があげられる．&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;局所最適解にトラップしにくい（勾配法の初期値依存問題への解決）&lt;/li&gt;
&lt;li&gt;冗長な学習データがある場合，勾配法よりも学習が高速&lt;/li&gt;
&lt;li&gt;学習データを収集しながら逐次的に学習できる&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;誤差逆伝播法(Back Propagation)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/08/perceptron.003.jpg&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;原理的には(8-1)の更新式を用いれば，SGDにより最適パラメータが推定できる．
今回は1段目はロジスティック関数を活性化関数に用い，2段目では恒等関数\(f(x)=x\)を活性化関数に用いるものとする．&lt;/p&gt;

&lt;p&gt;図のパーセプトロンに対して(8-2)の微分を計算しようとすると，2段目（青い部分）は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial E^{(n)}}{\partial\boldsymbol{v}}
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial z^{(n)}}{\partial\boldsymbol{v}} \\
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial}{\partial\boldsymbol{v}}(\boldsymbol{v}^{\mathrm{T}}\boldsymbol{y}) \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{y}
\end{align}
&lt;/div&gt;

&lt;p&gt;1段目は（\([x_1, \cdots,x_4]\mapsto y_1\)，すなわち赤い部分にのみ着目すると）&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial E^{(n)}}{\partial\boldsymbol{w_1}}
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial z^{(n)}}{\partial\boldsymbol{w_1}} \\
    &amp;= (z^{(n)}-t^{(n)})\frac{\partial}{\partial\boldsymbol{w_1}}(\boldsymbol{v}^{\mathrm{T}}\boldsymbol{y}) \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\frac{\partial\boldsymbol{y}}{\partial\boldsymbol{w_1}} \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\frac{\partial}{\partial\boldsymbol{w_1}}(\sigma(\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x})) \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\left(\frac{\partial\sigma(u)}{\partial u}\right)_{u=\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x}}\frac{\partial(\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{w_1}^{\mathrm{T}}} \\
    &amp;= (z^{(n)}-t^{(n)})\boldsymbol{v}\left(\frac{\partial\sigma(u)}{\partial u}\right)_{u=\boldsymbol{w_1}^{\mathrm{T}}\boldsymbol{x}}\;\;\boldsymbol{x}^{\mathrm{T}} \\
    &amp;= (z^{(n)}-t^{(n)})\sigma&#39;\boldsymbol{v}\boldsymbol{x}^{\mathrm{T}}
\end{align}
&lt;/div&gt;

&lt;p&gt;ただし\(\sigma\)はロジスティック関数とする．&lt;/p&gt;

&lt;div&gt;
\[
    \sigma(x)=\frac{1}{1+\exp(-x)}
\]
&lt;/div&gt;

&lt;p&gt;ここで重要なのは，どちらも&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial(\mathit{Error Function})}{\partial(\overrightarrow{\mathit{Param}})}\propto(\mathit{Error})\times(\overrightarrow{\mathit{Input}})
\]
&lt;/div&gt;

&lt;p&gt;という形になっているという点である（\(z_n-t_n\)が誤差，\(\boldsymbol{x,y}\)が入力ベクトル）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/08/perceptron.gif&quot; alt=&quot;パーセプトロンの更新&quot;&gt;&lt;/p&gt;

&lt;p&gt;イメージとしてはこのGIFのように，入力ベクトルの方向にパラメータを修正することを繰り返すことになる
（このGIFでは固定係数\(\eta\)による更新だが，ここでは固定係数ではなく誤差がかかった変動係数である）．&lt;/p&gt;

&lt;h2&gt;多層パーセプトロンのテスト&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/08/digit.py&quot;&gt;digit.py&lt;/a&gt;はscikit-learnに含まれるMNIST datasets(手書きの数字)を多層パーセプトロンで分類している．
自前実装なのでパラメータの初期値の取り方や中間層数の取り方を適当にやってしまっている．&lt;/p&gt;

&lt;p&gt;初期値ベクトルの取り方については&lt;a href=&quot;http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&quot;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;を参照するとよさそう．&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/stochastic-gradient-descent.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/stochastic-gradient-descent.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#07 ニューラルネットワーク</title>
        <description>&lt;p&gt;ニューラルネットワークは線形識別モデルの一つで，生体の神経回路（シナプスは他のシナプスから電気信号を入力として受け付け，ある一定量以上の刺激を受けたら発火して次のシナプスへ出力する）を模したモデルである．
最初期のものは60年以上前に提案されたもので，それ以降現在に至るまでパーセプトロン，ボルツマンマシン，オートエンコーダ等多数のアルゴリズムが考案され，昨今の&lt;u&gt;Deep Learning&lt;/u&gt;のブームにつながっている．&lt;/p&gt;

&lt;p&gt;神経回路を模した，といっても実際には特徴量を数学的に重み付けして評価関数を最適化する，今までのような統計的解析手法にすぎない．
しかし，多層パーセプトロンや深層学習のように多層にすればするほど，モデルの表現能力が飛躍的に向上することがわかったため，画像処理や音声処理をはじめとした多くの分野で応用されるようになっている．&lt;/p&gt;

&lt;p&gt;今後の分科会はニューラルネットワークを中心に，およそ3回でDeep Learningに到達する予定である．&lt;/p&gt;

&lt;h2&gt;単層パーセプトロン&lt;/h2&gt;

&lt;p&gt;一般的にグラフ理論におけるネットワークとは，双方向かつ閉路等も含む，ノードとエッジとフローから構成されるものである．
パーセプトロンとは，ニューラルネットワークの中で一方向なものである．
順伝播型ネットワークとも呼ばれる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/perceptron.png&quot; alt=&quot;パーセプトロンの模式図&quot;&gt;&lt;/p&gt;

&lt;p&gt;この図で，最も右のノードは&lt;/p&gt;

&lt;div&gt;
\[
    u=w_0+w_1x_1+\cdots+w_nx_n=\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}
\]
&lt;/div&gt;

&lt;p&gt;という入力を受け付ける．この部分は「他のシナプスから電気信号が伝わってくる」部分に対応する．&lt;/p&gt;

&lt;p&gt;「ある一定量以上の刺激を受けたら発火して出力する」ことを表現するために入力\(u\)を次の関数にかける．&lt;/p&gt;

&lt;div&gt;
\[
    f(x)=
    \begin{cases}
        1\;\;(x\ge 0) \\
        0\;\;(x\lt 0)
    \end{cases}
\]
&lt;/div&gt;

&lt;p&gt;これはhinge関数と呼ばれる階段上の関数である．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/hinge.png&quot; alt=&quot;hinge関数&quot;&gt;&lt;/p&gt;

&lt;p&gt;この関数によって出力を\(z=f(u)\)と得ると，純粋に「ある一定値を超えると発火する」ことを実現できるが，実際には連続値をとりhinge関数に形状が似たLogistic関数が利用されることが多い．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/logistic.png&quot; alt=&quot;Logistic関数&quot;&gt;&lt;/p&gt;

&lt;p&gt;\(\beta\)はパラメータ（正数）であり，大きければ大きいほどロジスティック関数の勾配は急になる．&lt;/p&gt;

&lt;p&gt;このようにパーセプトロンの出力にかける関数のことを活性化関数という．&lt;/p&gt;

&lt;h2&gt;多層パーセプトロン&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/perceptron.001.jpg&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;多層パーセプトロンは，単層パーセプトロンを重ねたものである．
入出力と活性化は基本的に単層パーセプトロンと同じである．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/07/perceptron.002.jpg&quot; alt=&quot;多層パーセプトロン&quot;&gt;&lt;/p&gt;

&lt;p&gt;このように部分的に抽出すると，単層パーセプトロンとして見做すことができる．&lt;/p&gt;

&lt;p&gt;これからよく登場するのは2層の多層パーセプトロンになる．
多層になればなるほど学習時のパラメータの推定が計算量の面から困難になるからである．
2層のパーセプトロンにおいて，&lt;u&gt;中間層の活性化関数にロジスティック関数，出力層の活性化関数に恒等関数\(f(x)=x\)を用いる．&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;パーセプトロンを多層にすることで一般的に表現能力が上がり，多彩な分類・認識を行うことができる．
しかし，同時にパラメータ推定を行うのに飛躍的にリソースが増加する．
（その問題を上手く解決したのがDeep Learningであるといえる）&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jun 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/neural-network.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/06/08/neural-network.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#06 線形識別モデル</title>
        <description>&lt;p&gt;今回から本格的に機械学習らしい内容に入っていく．&lt;/p&gt;

&lt;p&gt;機械学習で扱う問題のほとんどは識別問題，すなわち与えられたデータの属性を推定する問題であることが多い．&lt;/p&gt;

&lt;p&gt;(例)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;スパムメール判定（受信したメールがスパムなのかそうでないのか）&lt;/li&gt;
&lt;li&gt;Googleの言語判定（テキストが何語で書かれているのか）&lt;/li&gt;
&lt;li&gt;YouTubeのおすすめ動画&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/spam.png&quot; alt=&quot;スパムフィルタ&quot;&gt;
↑GMailのスパムフィルタ&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/lang.png&quot; alt=&quot;言語判定&quot;&gt;
↑Googleの言語判定&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/youtube.png&quot; alt=&quot;おすすめ動画&quot;&gt;
↑YouTubeのおすすめ動画&lt;/p&gt;

&lt;h2&gt;それは機械学習が必要なのか&lt;/h2&gt;

&lt;p&gt;昨今の機械学習ブームの影響で，とりあえず機械学習を使ってみたいというケースが増えている（僕もそのクチだ）．
ただ，何でもかんでもとりあえず機械学習で識別させておけばいいわけではない．
例えば，自動販売機に投入される硬貨の種類を判定するのにわざわざ機械学習を用いる必要はない．
対象はせいぜい1円玉，5円玉，10円玉，50円玉，100円玉，500円玉の6種類しかないので，rule-basedに記述した方が速い．&lt;/p&gt;

&lt;p&gt;機械学習を用いた方が良いのは，処理すべきデータ規模が人力では扱えないほど大きかったり，ruleが曖昧な場合などである．
（「スパムメールの基準を書き出せ」と言われたら難しいだろう）&lt;/p&gt;

&lt;p&gt;また，機械学習を用いるにしても，徹頭徹尾全て機械学習に頼りきらなければいけない道理はどこにもない．
サイボウズ・ラボの中谷秀洋氏に話を伺う機会があったのだが，言語判定をやっていると異言語間で同じ綴りの単語が使われているケース等で，機械学習ではどうしても乗り越えられない壁に当たる時があるのだが，そういう場合は普通にif文一つかませてしまうと精度が上がるそうだ．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/shuyo/dsirnlp&quot;&gt;言語判定へのいざない - SlideShare&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;線形識別関数&lt;/h2&gt;

&lt;p&gt;それでは前置きから本題に入ろう．テキスト処理の例を据えながら説明する．
ここでは，ブログ記事のトピックが統計に関連するのかしないのかを判定するようなプログラムを作ることを考えてみよう．
まず，記事中に「統計」という言葉が登場していたら機械学習に関連する記事だと判定することにしよう．
これを式として表すなら，「統計」という単語の登場回数を\(x\)として，&lt;/p&gt;

&lt;div&gt;
\[
    f(x)=x-1
\]
&lt;/div&gt;

&lt;p&gt;という識別関数を考え，&lt;/p&gt;

&lt;div&gt;
\begin{align}
    f(x)\ge 0 &amp;\Rightarrow \mathrm{related}\\
    f(x)\lt 0 &amp;\Rightarrow \mathrm{not\,related}
\end{align}
&lt;/div&gt;

&lt;p&gt;というように書き表せる（自明に見えるかもしれないが丁寧に進める）．&lt;/p&gt;

&lt;p&gt;しかし，「統計」という単語が含まれているからといって，必ずしも統計に関する文章であるとは限らない．
（例えば&lt;a href=&quot;http://www.benricho.org/weather_ratio/&quot;&gt;このページ&lt;/a&gt;には「統計」という単語が登場するが，どちらかといえば天気に関する文章だ）
記事の長さは記事ごとによってまちまちなので，記事に出現する全単語数で割った値を用いた方がよいかもしれない（正規化という）．
また，「統計」という単語が登場しなくても「検定」や「有意」という単語が登場しているなら統計に関係あるかもしれない．&lt;/p&gt;

&lt;p&gt;これを踏まえて，次は「統計」「検定」「有意」という単語の登場回数を記事中の全単語数で割った値をそれぞれ\(x_1,x_2,x_3\)とし，&lt;/p&gt;

&lt;div&gt;
\[
    f(x_1,x_2,x_3)=x_1+3x_2+2x_3-r
\]
&lt;/div&gt;

&lt;p&gt;という識別関数を考える．relatedとnot relatedの判別基準は先ほどと同様とする．
3や2といった係数は僕が適当に与えたもので，この場合「検定」や「有意」が登場すると「統計」が登場したときよりもrelatedと判断しやすくなる．&lt;/p&gt;

&lt;p&gt;今ここで考えている識別関数は，記事から\(\{x_i\}_{i=1}^N\)というパラメータを抽出し，それを線形写像（1次関数）で処理している．&lt;/p&gt;

&lt;p&gt;一般化する．&lt;/p&gt;

&lt;p&gt;2クラス問題\(C_1,C_2\)を分類する線形識別関数は次のように書ける．&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+w_0
\]
&lt;/div&gt;

&lt;p&gt;識別の基準は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    C_1 &amp; \;\mathrm{if}\; f(\boldsymbol{x})\ge 0\\
    C_2 &amp; \;\mathrm{if}\; f(\boldsymbol{x})\lt 0
\end{align}
&lt;/div&gt;

&lt;p&gt;とする．&lt;/p&gt;

&lt;p&gt;\(\boldsymbol{x}\)は入力ベクトル，\(\boldsymbol{w}\)は係数ベクトル，\(w_0\)はバイアス項と呼ばれる．&lt;/p&gt;

&lt;p&gt;ここでバイアス項を\(\boldsymbol{w}\)の第(N+1)要素に追加し，\(\boldsymbol{x}\)の第(N+1)要素に1を追加すると&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}
\]
&lt;/div&gt;

&lt;p&gt;と書くことができる．
こちらの方が一般的には扱いやすいので，以降バイアス項は省略する．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/leastsq.png&quot; alt=&quot;二乗誤差&quot;&gt;&lt;/p&gt;

&lt;p&gt;ちなみに，\(\boldsymbol{w}^T\boldsymbol{x}=0\)は\(\boldsymbol{w}\)を法線ベクトルとする超平面の式になる．
つまり\(\boldsymbol{w}^T\boldsymbol{x}\)はベクトル\(\boldsymbol{x}\)の超平面からの距離になる．&lt;/p&gt;

&lt;h2&gt;学習と推定&lt;/h2&gt;

&lt;p&gt;上記の説明では，人間側が適当に係数をいじっていたが，実際に「検定」を重視したほうがいいのか，「有意」を重視したほうがいいのか，ということはデータを見なければわからない．
したがって，コンピュータに大量の「あらかじめ統計に関連するかどうかわかっている」データを処理させ，最適な係数を計算によって求めさせる．
これが機械学習のうち，学習のフェーズになる．&lt;/p&gt;

&lt;p&gt;学習さえしてしまえば推定は簡単で，「統計に関連するかどうか知りたい」記事のデータについて\(f(\boldsymbol{x})\)の値を計算して，上記の識別の基準にしたがって判断すればよい．&lt;/p&gt;

&lt;p&gt;ここで問題になるのは，&lt;u&gt;どのようにしてデータから最適なパラメータを計算するか&lt;/u&gt;ということになる．ここではシンプルな評価関数として二乗誤差を考える．&lt;/p&gt;

&lt;h2&gt;最小二乗誤差基準&lt;/h2&gt;

&lt;p&gt;上の例では識別関数が正のクラスが正の値のときは\(C_1\)，負の値のときは\(C_2\)に分類される．
ここで行いたいのは，学習データをコンピュータに与えて，最適なパラメータ\(\boldsymbol{w}\)を計算することである．
これを以下のように行う．&lt;/p&gt;

&lt;div style=&quot;border: solid 1px; padding: 20px; margin: 10px;&quot;&gt;
学習データに対して，\(C_1\)ならば+1，\(C_2\)ならば-1を返すように\(f(\boldsymbol{x})\)を設計する．
そのために二乗誤差\((t_i-f(\boldsymbol{x_i}))^2\)を計算し，その総和が最小になるような\(\boldsymbol{w}\)を計算する．
(x_iはi番目の学習データ．t_iはi番目の学習データのラベル値で，{1,-1})
&lt;/div&gt;

&lt;p&gt;すなわち，最小化すべきは&lt;/p&gt;

&lt;div&gt;
\[
    J(\boldsymbol{w})=\sum_{i=1}^K(t_i-\boldsymbol{w}^T\boldsymbol{x_i})^2
\]
&lt;/div&gt;

&lt;p&gt;この式は次のように書きなおすことができる．&lt;/p&gt;

&lt;div&gt;
\[
    J(\boldsymbol{w})=\|\boldsymbol{t}-X\boldsymbol{w}\|^2 \tag{6-1}
\]
&lt;/div&gt;

&lt;p&gt;ここで\(\boldsymbol{t}=(t_1 t_2 \cdots t_K)^T\)，&lt;/p&gt;

&lt;div&gt;
\[
    X = \left(\begin{array}{c}
        \boldsymbol{x_1}^T \\
        \boldsymbol{x_2}^T \\
        : \\
        \boldsymbol{x_K}^T
    \end{array}\right)
\]
&lt;/div&gt;

&lt;h3&gt;最適解&lt;/h3&gt;

&lt;p&gt;今一度解くべき問題を整理すると，評価関数(6-2)\(J(\boldsymbol{w})\)の最小化である．
ここでいきなり最急降下法にかける前に，式の上で計算してみる．&lt;/p&gt;

&lt;p&gt;まず\(J(\boldsymbol{w})\)を微分すると&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial J}{\partial\boldsymbol{w}}
    &amp;= \frac{\partial}{\partial\boldsymbol{w}}(\boldsymbol{w}^TX^TX\boldsymbol{w}-\boldsymbol{w}X^T\boldsymbol{t}-\boldsymbol{t}^TX\boldsymbol{w}+\|\boldsymbol{t}\|^2) \\
    &amp;= 2X^TX\boldsymbol{w}-2X^T\boldsymbol{t} \\
\end{align}
&lt;/div&gt;

&lt;p&gt;ここでは制約条件はついていないので，単純に導関数=0とおくと&lt;/p&gt;

&lt;div&gt;
\[
    X^TX\boldsymbol{w}=X^T\boldsymbol{t}
\]
&lt;/div&gt;

&lt;p&gt;これは&lt;u&gt;正規方程式&lt;/u&gt;と呼ばれている．
したがって&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{w}=(X^TX)^{-1}X^T\boldsymbol{t}
\]
&lt;/div&gt;

&lt;p&gt;という解が得られる．&lt;/p&gt;

&lt;h2&gt;簡単な例&lt;/h2&gt;

&lt;p&gt;初めにインストールしたモジュールの中に&lt;code&gt;scikit-learn&lt;/code&gt;というモジュールがある．
これはPython向けの機械学習モジュールで，様々なアルゴリズムが提供されていたり，サンプルデータが提供されている．
ここではiris(アヤメ)のサンプルデータを用いて，最小二乗学習を行ってみる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/load.png&quot; alt=&quot;irisのロード&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sklearn.datasets.load_iris&lt;/code&gt;でirisをロードできる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/data.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;データはこのような4次元データになっている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/feature_name.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;各次元はこのような特徴を表現している．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/target.png&quot; alt=&quot;target&quot;&gt;&lt;/p&gt;

&lt;p&gt;各データのラベル値はこのようになっている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/target_name.png&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ラベル値はそれぞれsetosa, versicolor, virginicaというアヤメの種類と対応している．&lt;/p&gt;

&lt;p&gt;現時点では2クラス分類しかできないので，「setosaかそうでないか」を推定するプログラムを書いてみる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 最小二乗法で学習&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 最小二乗法で推定&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# テストデータに対する正答率&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/ml2015/images/06/iris.png&quot; alt=&quot;例&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_validation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;の行はirisを(学習データ):(テストデータ)=4:1に分割している(test_size=0.2)．&lt;code&gt;train_x, train_y&lt;/code&gt;を使って\(\boldsymbol{w}\)を推定し，できた識別器に&lt;code&gt;test_x&lt;/code&gt;をかけて&lt;code&gt;pred_y&lt;/code&gt;を&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;で得て，そのうち何%が&lt;code&gt;test_y&lt;/code&gt;と一致しているかを&lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt;を用いて&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;の行で計算している．&lt;/p&gt;

&lt;p&gt;この程度であれば100%の精度を達成することができる．&lt;/p&gt;
</description>
        <pubDate>Sun, 24 May 2015 00:00:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/24/linear-recognition-model.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/24/linear-recognition-model.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#05 確率分布の基礎</title>
        <description>&lt;p&gt;確率は統計解析と密接に関係している．
例えば，株価のトレードデータから数日後の株価を予測する際，過去の統計データをもとに確率的に最もありえる株価でもって予測したりする．
また，患者の状態を観測してそこから医者が&amp;quot;最も取るべき行動&amp;quot;を推論し，医療に応用するといったこともなされる．&lt;/p&gt;

&lt;p&gt;本節では，確率解析のベースとなる確率分布の基礎を説明する．&lt;/p&gt;

&lt;h2&gt;二項分布&lt;/h2&gt;

&lt;p&gt;確率分布の最も単純な例として，コイン投げを考える．
確率\(p=\frac{1}{2}\)で表，\(q=1-p=\frac{1}{2}\)で裏が出るようなコインを考える．
確率変数\(x\)に対して「表が出る」事象を1，「裏が出る」事象を0とする．
コインを\(n\)回投げるとき，表が出る回数が\(k\)回になる確率は以下のように表される．&lt;/p&gt;

&lt;div&gt;
\[
    P(x=k) = {}_nC_k p^k(1-p)^{n-k} = \frac{n!}{k!(n-k)!} p^k(1-p)^{n-k}
\]
&lt;/div&gt;

&lt;p&gt;（高校で学ぶ反復事象の確率そのものである）&lt;/p&gt;

&lt;p&gt;このような確率変数\(x\)の分布はパラメータ\(n,p\)で決定されるため，二項分布\(B(n,p)\)に従うといい\(x\sim B(n,p)\)と書く．&lt;/p&gt;

&lt;p&gt;二項分布をとるのは，試行がベルヌーイ試行（試行の結果が二値）であり，互いに独立であるような場合である．
上の例ではコイントスが試行であり，コイントス自体は表と裏の二値で，一回前のトスの結果は次のトスには影響しない．&lt;/p&gt;

&lt;h3&gt;例1&lt;/h3&gt;

&lt;p&gt;以下のスクリプトは二項分布に従って確率変数をランダムに生成している．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;コイントスの例で言えば，「表裏が出る確率の等しい(p=0.5)コインを20回(=n)投げる」という試行を10回行っている．
実行例は以下のとおり．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;array([12, 11,  9,  9,  7, 10, 10, 10,  9, 11])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;このように，だいたい10回前後表が出ていることがわかる．&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;scipy.stats&lt;/code&gt;には他にも様々な確率分布に関するモジュールが提供されている．
また，各々のモジュールは統一的なインターフェースで提供されている（例えば，他の確率分布に従って確率変数をランダム生成する場合も&lt;code&gt;rvs&lt;/code&gt;を使う）．
一度&lt;a href=&quot;http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.binom.html&quot;&gt;scipy.stats.binom&lt;/a&gt;のドキュメントのMethodsの欄に目を通してみるといいかもしれない．&lt;/p&gt;

&lt;h3&gt;正規性&lt;/h3&gt;

&lt;p&gt;確率分布は「すべて足して1」にならなければならない．それを二項分布で確かめる．&lt;/p&gt;

&lt;div&gt;
\[
    \sum_{k=0}^n{}_nC_kp^k(1-p)^{n-k} = \{p+(1-p)\}^n = 1
\]
&lt;/div&gt;

&lt;p&gt;この計算には二項定理を用いている．&lt;/p&gt;

&lt;h3&gt;期待値と分散&lt;/h3&gt;

&lt;p&gt;二項分布の期待値は&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \mathbb{E}(x)
        &amp;= \sum_{k=0}^n kP(k) \\
        &amp;= \sum_{k=0}^n k\cdot\frac{n\cdot(n-1)!}{k\cdot(k-1)!(n-k)!}p^k(1-p)^{n-k} \\
        &amp;= \sum_{k=0}^n k\cdot\frac{n}{k}\cdot{}_{n-1}C_{k-1}p^k(1-p)^{n-k} \\
        &amp;= np\sum_{k=0}^n {}_{n-1}C_{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)} \\
        &amp;= np\cdot\{p+(1-p)\}^n \\
        &amp;= np
\end{align}
&lt;/div&gt;

&lt;p&gt;となる．表裏の出る確率がイーブンなコインを10回投げるとだいたい5回(=10・(1/2))表になる直感にそっている．&lt;/p&gt;

&lt;p&gt;分散は&lt;/p&gt;

&lt;div&gt;
\[
    \mathrm{Var}(x)=np(1-p)
\]
&lt;/div&gt;

&lt;p&gt;である．&lt;/p&gt;

&lt;h3&gt;練習1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;二項分布の分散を計算せよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;期待値と似たような計算で確認できる．&lt;/p&gt;

&lt;h2&gt;正規分布と中心極限定理&lt;/h2&gt;

&lt;p&gt;二項分布の試行回数\(n\)を十分大きくしていくと，どうなるだろうか．&lt;/p&gt;

&lt;p&gt;コイントスの例で考える．トスの回数\(n\)を1回から100回まで増やしていく．
各\(n\)において，同じ試行を10000回行い，表の出た回数をヒストグラムにとってみる．
これは以下のようなスクリプトで実現できる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 二項分布の生成&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# キャンバスのクリア&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ヒストグラムの描画（binsはヒストグラムの分割数、あまり気にしなくて良い）&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;n = {}&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;fig{}.png&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;この結果得られたプロットをGIFアニメーションにしたのが下の画像である．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/05/binom.gif&quot; alt=&quot;二項分布の近似&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このように，二項分布の形状が次第に一定のカーブに近づいていく．
この分布は正規分布と呼ばれ，次のような式で表される．&lt;/p&gt;

&lt;div&gt;
\[
    P(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\]
&lt;/div&gt;

&lt;p&gt;\(\mu\)は平均（＝期待値），\(\sigma\)は分散である．&lt;/p&gt;

&lt;p&gt;実際に\(n=100\)のヒストグラムに正規分布を重ねてプロットすると以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/05/normal.png&quot; alt=&quot;正規分布&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このように&lt;strong&gt;二項分布のもとで試行を繰り返すと，その和の分布は次第に正規分布に近づく．&lt;/strong&gt;
この事実は中心極限定理と呼ばれる．中心極限定理の証明は難しいので割愛する．
中心極限定理の存在により，大標本の場合にはまず正規分布を仮定してデータ解析することも多く，正規分布は数ある確率分布でも特殊な位置を占めている．&lt;/p&gt;

&lt;h3&gt;練習2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;サイコロを振って1〜6の目が出る確率は（イカサマされていないという前提で）すべて等しく1/6である．
この分布は
$$ P(x)=\frac{1}{6} $$
と書け，一様分布という．
一様分布も二項分布と同様に，正規分布に収束することをスクリプトで確認せよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;収束はそれなりに速く，\(n=10\)程度でかなり正規分布に近い形になる．&lt;/p&gt;

&lt;p&gt;スクリプト例を&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/05/uniform.py&quot;&gt;uniform.py&lt;/a&gt;に示す．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/05/uniform.gif&quot;&gt;uniform.gif&lt;/a&gt;のような結果になれば概ねOKだろう．&lt;/p&gt;
</description>
        <pubDate>Tue, 12 May 2015 02:14:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/12/probability.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/12/probability.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#04 最適化手法</title>
        <description>&lt;p&gt;最適化とは簡単に言うと関数の最大値・最小値を求めることである．
コンピュータサイエンスの応用分野では最適化手法が幅広く用いられている．
機械学習の例で説明すると，例えばデータを学習して&amp;quot;最適な&amp;quot;パラメータを決定するというのは，結局データを引数として取る評価関数を最大化することに他ならない．&lt;/p&gt;

&lt;p&gt;しかし，コンピュータで最大値・最小値を計算するのは思ったほど簡単なことではない．
例えば人間なら導関数の零点を見つける際は単純に方程式を解くだけだが，コンピュータに方程式をただ投げても適切なアルゴリズムがなければ解は返ってこない．&lt;/p&gt;

&lt;p&gt;この節では，以下の2つのアルゴリズムを説明する．&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Lagrangeの未定乗数法&lt;/p&gt;

&lt;p&gt;制約条件付きの最適化問題を解く，数理解析的アルゴリズム．
コンピュータが直接的に扱うのは難しいが，様々な最適化アルゴリズムのベースになっている．&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最急降下法&lt;/p&gt;

&lt;p&gt;関数の最大値・最小値を，その導関数を用いて探索的に求めるアルゴリズム．
コンピュータにとって扱いやすく，アルゴリズム自体もシンプル（故に問題点も多い）．&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Lagrangeの未定乗数法&lt;/h2&gt;

&lt;p&gt;1変数関数の制約条件付き最大値・最小値を求める際は，導関数の零点から極値をとる値を求め，実際に極値同士の値を比較して最大値・最小値を求めていた．&lt;/p&gt;

&lt;p&gt;多変数関数の場合は，&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f(\boldsymbol{a})}{\partial x_1} = 0,\cdots,\frac{\partial f(\boldsymbol{a})}{\partial x_n} = 0
\]
&lt;/div&gt;

&lt;p&gt;だからといって\(f(\boldsymbol{a})\)が極値になっているとは限らない．&lt;/p&gt;

&lt;p&gt;例えば\(f(x,y)=x^2+y^3\)のグラフをプロットすると次のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/critical_point.png&quot; alt=&quot;臨界点&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;この関数の\((x,y)=(0,0)\)では，\(x,y\)ともに偏導関数は\(0\)になっているが，極値にはなっていない．
（極値であるかどうかに関わらず，偏導関数がすべて\(0\)になるような点を停留点，または臨界点という）&lt;/p&gt;

&lt;p&gt;1変数の場合でも極値にならない停留点は存在する（例えば\(f(x)=x^3\)の\(x=0\)）が，1変数関数の場合は増減表を書いて確かめることができる．
しかし多変数関数では増減表が書けない．ここに制約条件が付いてくると更に複雑になる．&lt;/p&gt;

&lt;p&gt;このような多変数関数の制約条件付き最大最小問題を解くにはLagrangeの未定乗数法が用いられる．&lt;/p&gt;

&lt;h3&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;制約条件\(g(\boldsymbol{x})=0\)の下で多変数関数\(f(x)\)の極値を与える\(\boldsymbol{x}\)は，次のような関数（Lagrange関数という）&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(\boldsymbol{x},\lambda) = f(\boldsymbol{x})+\lambda g(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;に対して，以下の連立方程式の解として与えられる．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial \tilde{f}}{\partial\boldsymbol{x}} &amp;= 0 \\
    \frac{\partial \tilde{f}}{\partial\lambda} &amp;= 0
\end{align}
&lt;/div&gt;

&lt;h3&gt;例1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;maximize \(f(x,y)=2x+3y\) s.t. \(x^2+y^2=1\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;次のようなLagrange関数を作る．&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(x,y,\lambda)=2x+3y+\lambda(x^2+y^2-1)
\]
&lt;/div&gt;

&lt;p&gt;続いて\(\tilde{f}\)を\(x,y,\lambda\)でそれぞれ偏微分して「=0」とする．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial\tilde{f}}{\partial x} &amp;= 2+2\lambda x = 0 \\
    \frac{\partial\tilde{f}}{\partial y} &amp;= 3+2\lambda y = 0 \\
    \frac{\partial\tilde{f}}{\partial\lambda} &amp;= x^2+y^2-1 = 0 \\
\end{align}
&lt;/div&gt;

&lt;p&gt;この方程式から\(\lambda\)を消去すると&lt;/p&gt;

&lt;div&gt;
\[
    (x,y) = (\pm\frac{2}{\sqrt{13}},\pm\frac{3}{\sqrt{13}})
\]
&lt;/div&gt;

&lt;p&gt;という解が得られる．これらの組は最大値を与える変数の候補になっている．
実際に\(f(x,y)\)に代入してみると，\((x,y)=(\frac{2}{\sqrt{13}},\frac{3}{\sqrt{13}})\)のときに最大値\(\sqrt{13}\)をとり，
\((x,y)=(-\frac{2}{\sqrt{13}},-\frac{3}{\sqrt{13}})\)のときに最小値\(-\sqrt{13}\)をとることがわかる．&lt;/p&gt;

&lt;p&gt;実際にグラフをプロットしてみると以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/lagrange.png&quot; alt=&quot;未定乗数法の例&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;練習1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;例1のプロットをPythonを使って作成してみよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;/ml2015/ml/2015/05/10/multivariate-analysis.html&quot;&gt;#03&lt;/a&gt;や&lt;a href=&quot;http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html&quot;&gt;matplotlibのチュートリアル&lt;/a&gt;を参考にするとよい．
3D空間で線を描画するには&lt;code&gt;Axes3D.plot&lt;/code&gt;，点(散布図)を描画するには&lt;code&gt;Axes3D.scatter&lt;/code&gt;を用いる．&lt;/p&gt;

&lt;p&gt;（スクリプト例: &lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex1.py&quot;&gt;ex1.py&lt;/a&gt;）&lt;/p&gt;

&lt;h3&gt;練習2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;半径1の球に内接する直方体の体積の最大値を求めよ．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;解答は&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex2.md&quot;&gt;ex2.md&lt;/a&gt;に示す．&lt;/p&gt;

&lt;h3&gt;Lagrangeの未定乗数法の原理&lt;/h3&gt;

&lt;p&gt;厳密な証明は難しく，この分科会の内容から少々逸脱してしまうので，ここでは割愛する．
イメージを掴みたい人は，&lt;a href=&quot;https://github.com/levelfour/machine-learning-2014/wiki/%E7%AC%AC4%E5%9B%9E---Lagrange%E3%81%AE%E6%9C%AA%E5%AE%9A%E4%B9%97%E6%95%B0%E6%B3%95#%E8%A8%BC%E6%98%8E&quot;&gt;昨年の分科会の資料&lt;/a&gt;を参考にするとよい．&lt;/p&gt;

&lt;h3&gt;複数制約条件の場合&lt;/h3&gt;

&lt;p&gt;ここでは制約条件が1つの場合のみについて述べたが，制約条件が\(M\)個（\(\{g_m(\boldsymbol{x})=0\}_{m=1}^M\)）ある場合は&lt;/p&gt;

&lt;div&gt;
\[
    \tilde{f}(\boldsymbol{x},\boldsymbol{\lambda}) = f(\boldsymbol{x})+\sum_{m=1}^M\lambda_mg_m(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;というLagrange関数をつくり，以下の連立方程式を解く．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial \tilde{f}}{\partial\boldsymbol{x}} &amp;= 0 \\
    \frac{\partial \tilde{f}}{\partial\boldsymbol{\lambda}} &amp;= 0
\end{align}
&lt;/div&gt;

&lt;h3&gt;不等式制約条件の場合&lt;/h3&gt;

&lt;p&gt;ここで述べたのは制約条件が等式の場合のみであった．制約条件が不等式になっているケースも実用上は多く登場する．
その場合は少しだけ複雑になる（Karush-Kuhn-Tucker条件の導入）ので，実際に用いる際に説明する．&lt;/p&gt;

&lt;p&gt;&lt;hr /&gt;&lt;/p&gt;

&lt;h2&gt;最急降下法&lt;/h2&gt;

&lt;p&gt;最急降下法は，関数の導関数の値を用いて逐次的に関数値を最大にする解を更新するアルゴリズムである．&lt;/p&gt;

&lt;p&gt;関数\(f(\boldsymbol{x})\)に対して初期値\(x_0\)を与えて，&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}_{n+1}=\boldsymbol{x}_{n}+\eta\frac{\partial f(\boldsymbol{x}_{n})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;p&gt;で\(\boldsymbol{x}\)の値を逐次的に更新し，収束した点が最大になっている．
導関数の値が0になったら収束だが，実際にちょうど0になる時点まで探索し続けると収束が非常に遅くなるので，導関数値が十分に小さくなったら収束と見なす．
イメージとしては，関数値が増大する向き（＝導関数が正）に山を登ることになる．&lt;/p&gt;

&lt;p&gt;\(\eta(&amp;gt;)0\)は学習率で，収束の仕方を決めるパラメータである（大きいほど収束が速いわけではない）．&lt;/p&gt;

&lt;p&gt;最急降下法の更新の過程の様子は以下のアニメーションを参考にしてほしい．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/descent.gif&quot; alt=&quot;最急降下法&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最急降下法で最小値を求める際は，更新式を以下のように変更すればよい．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}_{n+1}=\boldsymbol{x}_n-\eta\frac{\partial f(\boldsymbol{x}_{n})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;h3&gt;最急降下法の問題点&lt;/h3&gt;

&lt;h4&gt;初期値依存性&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/04/overfit.png&quot; alt=&quot;初期値依存性&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上に示す図のように，最急降下法では初期値の取り方によって収束先が変わり得る．
それは，最急降下法は大域的な最大値を求めているのではなく，局所的最大値（すなわち極大値）を求めているからにすぎない．
そのため，実際には解析対象となる関数の特性を把握しつつ初期値を選択したり，複数の初期値に対して試行する必要がある．&lt;/p&gt;

&lt;h4&gt;収束速度&lt;/h4&gt;

&lt;p&gt;最急降下法は原理がシンプルな反面，収束速度が遅いことで知られる．
学習率\(\eta\)の選び方で収束速度は制御できるが，あまり大きな値にしすぎると見当違いな値に収束することもあり，あまり小さな値にしすぎると収束精度は向上するが収束速度は遅くなるといった問題がある．&lt;/p&gt;

&lt;h3&gt;例2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;#03の例1でプロットした下の関数の最大値を最急降下法で求める．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x}) = \exp\left(-\frac{(x-0.5)^2+(y-0.2)^2}{2}\right)
\]
&lt;/div&gt;

&lt;p&gt;この関数を微分すると以下のようになる．&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f}{\partial\boldsymbol{x}}
    = \left(\begin{array}{c}
        0.5-x \\
        0.2-y
    \end{array}\right) f(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;最大値を求めるPythonコードは以下のように書ける．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 学習率&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 反復回数&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 初期値ベクトル&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ini&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ini&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;x({}) = {}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ちなみに，このスクリプトのようにコメントにUnicode文字を使いたい場合は，ファイルの先頭に&lt;code&gt;# -*- coding: utf-8 -*-&lt;/code&gt;または&lt;code&gt;# coding: utf-8&lt;/code&gt;と書く必要がある．
この1行によって，Python処理系はUTF-8でスクリプトを解釈するようになる．&lt;/p&gt;

&lt;h3&gt;練習3&lt;/h3&gt;

&lt;p&gt;例2のスクリプトを実際に動かしてみよ．また，学習率，反復回数，初期値ベクトルを変化させると収束先の値がどのように変化するか，いろいろ試してみよ．&lt;/p&gt;

&lt;h3&gt;練習4&lt;/h3&gt;

&lt;p&gt;例2で，収束するまでの反復回数が最も少なくなるような学習率を探索せよ．0から2の間で試せば十分である．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tsg-ut/ml2015/blob/master/04/ex4.py&quot;&gt;ex4.py&lt;/a&gt;にスクリプト例を示す．&lt;/p&gt;
</description>
        <pubDate>Mon, 11 May 2015 21:11:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/11/lagrange-multiplier.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/11/lagrange-multiplier.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#03 多変数解析</title>
        <description>&lt;h2&gt;多変数関数&lt;/h2&gt;

&lt;p&gt;1変数関数は\(x\)に対して\(y\)を一意に対応付ける関係のことであった．この関係を\(y=f(x)\)等と書いた．&lt;/p&gt;

&lt;p&gt;多変数関数は，\(n\)個の変数\(x_1,x_2,\cdots,x_n\)を\(y\)に対応づける関係のことであり，
これを\(y=f(x_1,x_2,\cdots,x_n)\)，または\(y=f(\boldsymbol{x})\)と書いたりする．&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x} = (x_1 x_2 \cdots x_n)^{\mathrm{T}} \longmapsto f(\boldsymbol{x})
\]
&lt;/div&gt;

&lt;p&gt;というようにベクトル\(\boldsymbol{x}\)と\(f(\boldsymbol{x})\)の対応付けとして見ることもできる．
こちらの見方の方が後々微分等で直感的に理解しやすいという利点がある．&lt;/p&gt;

&lt;h4&gt;例1&lt;/h4&gt;

&lt;p&gt;多変数関数&lt;/p&gt;

&lt;div&gt;
\[
    f(\boldsymbol{x}) = \exp\left(-\frac{(x-0.5)^2+(y-0.2)^2}{2}\right)
\]
&lt;/div&gt;

&lt;p&gt;をプロットする．&lt;code&gt;matplotlib&lt;/code&gt;を用いると，以下のようなコードでプロットできる．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mpl_toolkits.mplot3d&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Axes3D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_wireframe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/ml2015/images/03/gaussian.png&quot; alt=&quot;例1&quot; style=&quot;width: 60%&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;偏微分&lt;/h2&gt;

&lt;p&gt;1変数関数と同様に多変数関数でも微分を行うが，多変数関数では引数が複数個あるので，どの変数で微分するかで導関数が変化する．&lt;/p&gt;

&lt;p&gt;関数\(f(x_1,\cdots,x_n)\)の引数のうち\(x_i\)以外を固定（定数と見なす）し，\(x_i\)で微分したときの導関数を&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial f}{\partial x_i}
\]
&lt;/div&gt;

&lt;p&gt;と書き，偏導関数という．&lt;/p&gt;

&lt;h2&gt;ベクトルによる微分&lt;/h2&gt;

&lt;p&gt;多変量解析では&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial(\boldsymbol{a}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{x}}
\]
&lt;/div&gt;

&lt;p&gt;のようにベクトルで微分を行う操作が式変形の過程で登場することがある．
これは以下の略記である．&lt;/p&gt;

&lt;div&gt;
\begin{align}   
    \frac{\partial(\boldsymbol{a}^{\mathrm{T}}\boldsymbol{x})}{\partial\boldsymbol{x}}
    &amp;= \frac{\partial(a_1x_1+\cdots+a_nx_n)}{\partial \left(\begin{array}{c} x_1 \\ : \\ x_n \end{array}\right)} \\
    &amp;= \left(\begin{array}{c} \frac{\partial}{\partial x_1}(a_1x_1+\cdots+a_nx_n) \\ : \\ \frac{\partial}{\partial x_n}(a_1x_1+\cdots+a_nx_n) \end{array}\right) \\
    &amp;= \left(\begin{array}{c} a_1 \\ : \\ a_n \end{array}\right) \\
    &amp;= \boldsymbol{a}
\end{align}
&lt;/div&gt;

&lt;p&gt;このように，ベクトルで微分する場合も結果はスカラーの場合から連想される直感的な結果になる．&lt;/p&gt;

&lt;p&gt;2次形式&lt;/p&gt;

&lt;div&gt;
\[
    \boldsymbol{x}^{\mathrm{T}}\boldsymbol{Bx}=\sum_{i=1}^n\sum_{j=1}^n b_{ij}x_ix_j
\]
&lt;/div&gt;

&lt;p&gt;の微分も頻出なので，結果のみ示しておく．興味のある人は手を動かして計算してみるとよい．&lt;/p&gt;

&lt;div&gt;
\begin{align}
    \frac{\partial\boldsymbol{x}^{\mathrm{T}}\boldsymbol{Bx}}{\partial\boldsymbol{x}}
    &amp;= (\boldsymbol{B}+\boldsymbol{B}^{\mathrm{T}})\boldsymbol{x} \\
    &amp;= 2\boldsymbol{Bx}^{\mathrm{T}} \; (\boldsymbol{B}:\mathrm{symmetric\,matrix})
\end{align}
&lt;/div&gt;
</description>
        <pubDate>Sun, 10 May 2015 00:47:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/05/10/multivariate-analysis.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/05/10/multivariate-analysis.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#02 環境構築</title>
        <description>&lt;h2&gt;統計解析とプログラミング&lt;/h2&gt;

&lt;p&gt;統計処理とプログラミングは切っても切り離せない関係にある。
プログラミングと言っても、この分野ではGUIで華やかなアプリケーションを作る技術やアセンブリをバリバリ読むということは勿論無く、単純に計算したい数式を記述するツールとして用いられることがほとんどだ。
そのため、プログラミング初心者の人も臆する必要はない（むしろプログラミングの観点から見れば難易度は低いので、プログラミングの勉強としても良いはずだ）。&lt;/p&gt;

&lt;p&gt;統計解析によく用いられるプログラミング言語・ツールには以下のようなものがある（levelfourのバイアスがかかっている）。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Excel: 言わずもがな。&lt;s&gt;方眼紙として使わなければ最強なんだけどなあ&lt;/s&gt;&lt;/li&gt;
&lt;li&gt;MatLab: 数値計算の王道ソフトウェア。ライセンスも高いので研究室向け。&lt;/li&gt;
&lt;li&gt;R: GNU発のフリーの数値解析ソフトウェア。フリーなのに高機能。&lt;/li&gt;
&lt;li&gt;Python: 汎用プログラミング言語だが、数値計算ライブラリが充実。&lt;/li&gt;
&lt;li&gt;(Julia: 2012年に初公開された、LLVMベースの高速数値計算向け汎用プログラミング言語)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この分科会ではPythonを用いることにする。統計解析だけに絞るのであれば確かにR言語の方が（特に検定回りで）ライブラリが充実しているのだが、Pythonの方が汎用プログラミング言語ともあって、他のツールの助力を借りること無くファイルアクセスしたりWeb公開できるのが強みだ。
世界的に見てもこの流れがあるようで、最近はR言語を始めるならPythonをやろうとよく言われる。&lt;/p&gt;

&lt;h2&gt;Pythonの統計解析用環境&lt;/h2&gt;

&lt;p&gt;一般的に以下のようなライブラリ群が用いられる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;numpy: 行列計算&lt;/li&gt;
&lt;li&gt;scipy: 科学計算（フィッティング、連立方程式、特殊関数、微分積分など）&lt;/li&gt;
&lt;li&gt;matplotlib: グラフプロットツール&lt;/li&gt;
&lt;li&gt;pandas: データの読み書き、表の作成（Pythonで動くExcelのような感じ）&lt;/li&gt;
&lt;li&gt;scikit-learn: 機械学習&lt;/li&gt;
&lt;li&gt;IPython(Jupyter): 高機能インタラクティブPythonシェル&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この回では、Python自体の処理系のインストールと、上記ライブラリのインストールを目指す。&lt;/p&gt;

&lt;h2&gt;pyenvのインストール&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;pyenv&lt;/code&gt;を入れておくとよい。仮想環境を使うと&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意のバージョンのPythonを簡単に入れられる&lt;/li&gt;
&lt;li&gt;システムのPythonを破壊せずにとっておける&lt;/li&gt;
&lt;li&gt;Sandbox環境を簡単に作成&amp;amp;破棄できる&lt;/li&gt;
&lt;li&gt;後述のAnacondaもあっという間にインストール&lt;/li&gt;
&lt;li&gt;Ma OS Xだと&lt;code&gt;brew install pyenv&lt;/code&gt;で入れられるっぽい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;といった風にメリットづくしなので、オススメする。ちなみに同じバージョンのPythonを複数入れたいという需要があるかもしれないが、そんなときには&lt;code&gt;virtualenv&lt;/code&gt;を用いる。&lt;/p&gt;

&lt;p&gt;以下OSごとのインストール方法を説明する。levelfourはMac OS 10.9, 10.10, Ubuntu 14.04でのインストールを確認した。
また、2014年度の分科会で他のメンバーによりWindowsでのインストールも確認された。&lt;/p&gt;

&lt;h4&gt;Windows&lt;/h4&gt;

&lt;p&gt;&lt;s&gt;Cygwinで以下のページの通りに作業するとインストールできることを確認。&lt;/s&gt;&lt;/p&gt;

&lt;p&gt;&lt;s&gt;&lt;a href=&quot;http://qiita.com/la_luna_azul/items/3f64016feaad1722805c&quot;&gt;pyenvとvirtualenvのインストールと使い方 - Qiita&lt;/a&gt;&lt;/s&gt;&lt;/p&gt;

&lt;p&gt;Windowsだと&lt;strong&gt;pyenvからanacondaをうまくインストールできない&lt;/strong&gt;ようなので、後述のAnaconda Installerを使用するとよい。&lt;/p&gt;

&lt;h4&gt;Mac OS X&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ brew install pyenv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Linux&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/yyuu/pyenv-installer&quot;&gt;pyenv-installer&lt;/a&gt;を用いるとインストールできる。
（Ubuntu 14.04で確認）&lt;/p&gt;

&lt;h3&gt;pyenvの使い方（概略）&lt;/h3&gt;

&lt;p&gt;インストール終了後に標準出力に.bashrcに環境変数設定を追記するように促されていると思うので、その通りに従う。&lt;/p&gt;

&lt;h4&gt;インストール可能バージョンを見る&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv install -l
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;インストールする&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv install [what-you-want]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;インストール済バージョン一覧を参照&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv versions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;環境の切り替え&lt;/h4&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv global [environment]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ちなみに&lt;code&gt;global&lt;/code&gt;でシステム全体で使用するPythonのバージョン、&lt;code&gt;local&lt;/code&gt;で&lt;strong&gt;そのディレクトリより下層&lt;/strong&gt;で使用するPythonのバージョンを設定することが出来る。&lt;/p&gt;

&lt;p&gt;【参考】&lt;a href=&quot;http://qiita.com/la_luna_azul/items/3f64016feaad1722805c&quot;&gt;pyenvとvirtualenvのインストールと使い方 - Qiita&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Anacondaのインストール&lt;/h2&gt;

&lt;p&gt;pyenvをインストールできたところで、次は上で挙げた数値計算ライブラリ群をインストールする。
しかし、これらライブラリのインストールには非常に手間がかかり、ビルドエラーが起こって解決が困難になることも多い。
そこで、Pythonの処理系と有用なライブラリ群をまとめたパッケージである&lt;a href=&quot;http://continuum.io/downloads#all&quot;&gt;Anaconda&lt;/a&gt;を利用する。
pyenvがインストールされていれば、Anacondaのインストールは非常に簡単である。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv install anaconda3-2.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;このインストールには20分〜30分程度の時間を要するので注意すること。
インストールの確認には&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv versions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;を実行した際にanaconda3-2.1.0が表示されるかを確認すればよい。
各種ライブラリ群が正常動作していることを確認するために、Anaconda環境に切り替える。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ pyenv global anaconda3-2.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;そしてPythonを立ち上げる。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以下の行を一行ずつ入力し、エラーが起こらなければ成功である。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Windows等pyenvが正常に動作しない環境の場合&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://continuum.io/downloads#all&quot;&gt;Anacondaのサポートページ&lt;/a&gt;からAnaconda installerをダウンロードしてインストールするのが簡単だと思われる。&lt;/p&gt;

&lt;h2&gt;Pythonの基礎知識&lt;/h2&gt;

&lt;p&gt;Pythonは汎用プログラミング言語の一つで、（多くの場合）インタプリタ上で動作する動的型付けスクリプト言語である。
Pythonの大きな特徴としては&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;インデントで関数等のブロックを表現する&lt;/li&gt;
&lt;li&gt;柔軟な動的型付け&lt;/li&gt;
&lt;li&gt;内包表記による簡潔で高速なコード&lt;/li&gt;
&lt;li&gt;関数型プログラミングも可能&lt;/li&gt;
&lt;li&gt;豊富な標準ライブラリ、活発な海外コミュニティ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;逆にデメリットとしては&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;インデントに縛られる&lt;/li&gt;
&lt;li&gt;lambda式が書きにくい&lt;/li&gt;
&lt;li&gt;正規表現が使いにくい&lt;/li&gt;
&lt;li&gt;オブジェクト指向にもかかわらずメソッドチェーンが書きにくい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;あたりだろうか（個人の感想）。&lt;/p&gt;

&lt;h3&gt;バージョン&lt;/h3&gt;

&lt;p&gt;Pythonは現在2.xから3.xへの移行期を向かえている。3.xでは2.xの後方互換性を切り捨てる形で、新たな機能を取り入れることを目指している。
（Pythonコミュニティとしては「今までよりもPythonicなコード」を目指しているようだ）&lt;/p&gt;

&lt;p&gt;そのため、3.xのコードは2.xの処理系では動かないし、その逆もまた然りだ。
昨年度の分科会では2.7の処理系を用いて説明を行ったが、3.xに対する環境も整備しつつあるこの時期が3.xへの移行タイミングと見極め、本分科会では3.xを対象として説明を行う。&lt;/p&gt;

&lt;p&gt;ちなみに、上でAnacondaをインストールしてもらった際にanaconda3-2.1.0というパッケージをインストールしてもらった。
これは2015年4月末時点の最新バージョンで、付属しているPython処理系のバージョンは3.4.1である。&lt;/p&gt;

&lt;h3&gt;サンプルスクリプト&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# coding: utf-8&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 定義域は(-3,3)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# sinカーブに正規分布ノイズをのせる&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# 3次関数フィッティング&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;polyfit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poly1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上記のスクリプトを保存してpythonで実行すると、以下のような結果が得られる。
sin関数のフィッティングである。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/02/sin.png&quot; alt=&quot;sin関数のフィッティング&quot;&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Apr 2015 21:07:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/environment.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/environment.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
      <item>
        <title>#01 機械学習の基礎</title>
        <description>&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;（釈迦に説法になる話も多分に含まれるが、限りなく前提知識を0に近づけたいのでご容赦願いたい）&lt;/p&gt;

&lt;p&gt;「機械学習」や「ビッグデータ」という言葉を昨今よく耳にするが、機械学習とは一体何だろうか。
&lt;a href=&quot;http://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&quot;&gt;Wikipedia&lt;/a&gt;から引用してみる。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;機械学習（きかいがくしゅう、英: machine learning）とは、人工知能における研究課題の一つで、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のことである。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ここで問題になるのは、どのような手法・アルゴリズムを用いれば計算機で学習を実現できるかということである。
元々人工知能に関する研究は1940年代の頃から始まっており、今日に至るまでの長い道のりの過程で工学的応用を得たのが機械学習と呼ばれる分野である。
まずは人工知能の歴史を軽く追って見ることにする。&lt;/p&gt;

&lt;h2&gt;人工知能の歴史&lt;/h2&gt;

&lt;h3&gt;Rule-based AI&lt;/h3&gt;

&lt;p&gt;人工知能というキーワードは1956年のダートマス大学における学会で初めて使われたという。
この頃から人工知能は盛んに研究されており、Alan Turingが有名な&lt;a href=&quot;http://ja.wikipedia.org/wiki/%E3%83%81%E3%83%A5%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%83%BB%E3%83%86%E3%82%B9%E3%83%88&quot;&gt;チューリングテスト&lt;/a&gt;を発表したのも1950年のことだった。
チューリングテストとは簡単に言うと、「自分がチャットしている相手が実はコンピュータだったら人工知能と言ってもいいよね」という人工知能のテストである。
実際、1964年頃にMITのJoseph Weizenbaum教授は有名な&lt;a href=&quot;http://ja.wikipedia.org/wiki/ELIZA&quot;&gt;ELIZA&lt;/a&gt;というカウンセラーを模した人工知能を実装し、この人工知能は多くの人間を騙すことに成功したという。&lt;/p&gt;

&lt;p&gt;ELIZAが実際に行った会話の記録は多く残っているし、今でもEmacsにはdoctorというELIZAのbuilt-in実装が入っている（emacsを起動してEsc - Xでdoctorを実行）。
確かにELIZAの会話は一見人間の精神科医のものかと思ってしまう。
しかし、実際にはパターンマッチングでそれらしい単語（例えば&amp;quot;anxiety&amp;quot;とか&amp;quot;headache&amp;quot;とか）にマッチした場合は用意された回答をし、マッチケースがなければオウム返しをするという、非常にシンプルな作りになっている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/eliza.png&quot; alt=&quot;ELIZAとの会話の様子&quot;&gt;&lt;/p&gt;

&lt;p&gt;この時期の人工知能は、ELIZAに限らず多くはパターンマッチング（if-then-elseをひたすら連ねたもの）を基本とした&lt;b&gt;rule-based&lt;/b&gt;なAIだった。
Rule-basedなAIでは、より高度な知能に見せるためにはマッチケースをひたすら増やすしかない。&lt;/p&gt;

&lt;p&gt;余談だが、「Lispが人工知能研究で用いられた」と言われることがあるが、これは当時まともにパターンマッチングができる言語がLispくらいしかなかったがために、Lispが用いられてきたという歴史的経緯があるようだ。
ELIZAも初期の段階でLisp実装がなされている。&lt;/p&gt;

&lt;h3&gt;Neural Network&lt;/h3&gt;

&lt;p&gt;同じ頃、1958年にアメリカの心理学者Frank RosenblattがPerceptronを発表した。
「人工知能を作るなら脳のニューロンの機構を模せばいいではないか」という発想のもと生まれたのが、Perceptronを始めとしたNeural Networkというモデルである。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/perceptron.png&quot; alt=&quot;パーセプトロンのモデル&quot;&gt;&lt;/p&gt;

&lt;p&gt;図中の\(x_1 \cdots x_n\)及び\(z\)がニューロンを形式化したもので、左側のニューロンから右側のニューロンへ信号が伝えられている様子をモデル化している。
各入力に\(w_1 \cdots w_n\)という重みをつけて、&lt;/p&gt;

&lt;p&gt;\[
  z = \sum_{i=1}^n x_i w_i
\]&lt;/p&gt;

&lt;p&gt;という入力を与える。&lt;/p&gt;

&lt;p&gt;しかし、最も単純なパーセプトロンではXOR(排他的論理和)を認識することができない。
この事実（正確には「単純パーセプトロンは線形分離不能なデータを識別できない」こと）は1969年に数学的に証明された。&lt;/p&gt;

&lt;p&gt;Neural Network自体は後に扱うので、この辺りで出た不明な単語は気にしなくて良い。&lt;/p&gt;

&lt;h3&gt;AIの冬&lt;/h3&gt;

&lt;p&gt;このように少し複雑な問題になると解けなくなってしまうことから人工知能研究への期待は薄れ、国や企業の投資も減っていった。
そのため多くの研究者は人間らしい知能を実現する人工知能(強いAI)の研究を諦め、それまでの研究を実用的なアプリケーション(弱いAI)に活かすことを考えた。
その際によく研究されたのが、光学文字認識や音声認識、機械翻訳などである。
実際、この時期に地道にアルゴリズムが改良され、これらの問題も解くことができるようになっていった。
しかし、実用化されるほどの認識精度にまでは至らなかった。
その大きな理由はデータ不足である。&lt;/p&gt;

&lt;h3&gt;ビッグデータ時代の幕開け&lt;/h3&gt;

&lt;p&gt;この状況を打開する出来事が1993年に起こった。
CERNによるWorld Wide Webの一般開放、すなわちインターネットの始まりである。
瞬く間にインターネット上にはテキスト、音声・画像をはじめ、ユーザの行動履歴、天気、株価情報といった広範囲にわたる膨大な量のデータで溢れかえることになった。
こういったデータをアルゴリズムで処理することにより、文字や音声の認識精度はますます向上した。&lt;/p&gt;

&lt;p&gt;さらに、&lt;a href=&quot;http://ja.wikipedia.org/wiki/%E3%83%A0%E3%83%BC%E3%82%A2%E3%81%AE%E6%B3%95%E5%89%87&quot;&gt;ムーアの法則&lt;/a&gt;に代表されるようにコンピュータの性能が指数関数的に向上し、大量のデータを高速に処理できるようになった。
あたかもコンピュータが大量のデータを読み込んで学習し、文字や音声を認識しているようにみえることから、機械学習と呼ばれるようになった。&lt;/p&gt;

&lt;p&gt;2006年、イギリスのコンピュータ科学者Geoffrey HintonがAuto EncoderとDeep Belief Networkを考案し、これが昨今注目を集めている&amp;quot;Deep Learning&amp;quot;の先駆けとなった。
さらに2011年、Stanford大学のAndrew NgらがGoogleと共同で研究を行った結果、コンピュータクラスタが自力でYouTubeの動画データから猫の概念を認識できるようになったと報告された。
この研究報告によって世界中に衝撃が走り、今日ではDeep Learningをはじめ機械学習の研究が世界中のコンピュータサイエンティストによって行われている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/cat.jpg&quot; alt=&quot;Googleの猫&quot; style=&quot;width:50%&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;統計と機械学習&lt;/h2&gt;

&lt;p&gt;先程パーセプトロンに触れた際に、&lt;b&gt;「あの数式のどこが脳の機構を模しているんだ」&lt;/b&gt;と感じた人も少なく無いと思う。
実際のところ、パーセプトロンを始めたNeural Networkは、ネットワーク構造を持ち、各素子がしきい値を持っていること以外はほとんど脳の機構と関係がない。
ではなぜあのようなモデルが用いられるかというと、結局コンピュータは数字を扱うことしかできないからである。
逆に言えば、適切に数字に直すことさえできればあらゆるデータを高速に処理することができるのがコンピュータの強みである。
そのため、今日の機会学習理論の多くは数理統計をベースとしている。&lt;/p&gt;

&lt;p&gt;機械学習には様々な種類があるが、その大部分は分類問題である。
先の例だと、文字認識も「与えられた画像をそこに書かれている文字に分類する」問題だし、音声認識も「入力された振幅データがどの音素に相当するのか分類する」ことになる。
その際には&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;予め答えのわかっているデータを&lt;b&gt;学習&lt;/b&gt;する&lt;/li&gt;
&lt;li&gt;未知のデータがどのクラスに分類されるのか&lt;b&gt;推定&lt;/b&gt;する&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;という2段階にわかれている。&lt;/p&gt;

&lt;h3&gt;学習&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ml2015/images/01/feature_extraction.png&quot; alt=&quot;特徴抽出の例&quot;&gt;&lt;/p&gt;

&lt;p&gt;例えば上のような元画像が与えられたとき、この画像が「れ」というひらがなであると認識するにはどうすればよいか。
先程も述べたように、結局コンピュータには数字しか扱うことができないから、なんとかしてコンピュータの扱えるような数値データとしてデータの特徴を捉え直す必要がある。
このようにデータからその特徴を数値として取り出すことを&lt;b&gt;特徴抽出&lt;/b&gt;という。&lt;/p&gt;

&lt;p&gt;一例だが、例えば文字画像を20×20で格子状に区切って各格子で平均をとり、合計400個の濃淡データとして扱う方法がある。
こうすると、例えば「れ」なら「左から30%のあたりに濃度が大きいデータが縦に並んでいる」といった特徴になるわけである。
このデータを400次元空間にmappingすれば、「&amp;quot;れ&amp;quot;は400次元空間のこのあたりに来やすい」みたいな計算をコンピュータで行うことができる。&lt;/p&gt;

&lt;h3&gt;推定&lt;/h3&gt;

&lt;p&gt;未知のデータが与えられたときも同じように特徴抽出を行い、既に学習したデータと照らし合わせればクラスに分類することができる。
このときにコンピュータが行っているのも、単純に「このデータは今までのどのデータにより近いのか」という類似度（距離）計算にすぎない（勿論類似度以外で分類するアルゴリズムもある）。&lt;/p&gt;

&lt;h2&gt;機械学習の学習&lt;/h2&gt;

&lt;h3&gt;当分科会の進め方&lt;/h3&gt;

&lt;p&gt;ご察しの通り、機械学習ではそれなりに高度な数学（主に解析学、線形代数、確率統計）が要求される。
そのため、この分科会ではおおまかに次のような流れで進めていく。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;基礎的な確率統計&lt;/li&gt;
&lt;li&gt;数理統計モデル&lt;/li&gt;
&lt;li&gt;機械学習の基本的なアルゴリズム&lt;/li&gt;
&lt;li&gt;最新のトピック（主にDeep Learning）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1年生にとっては難しいこともあると思うが、可能な限り高校レベルの数学で理解可能な丁寧な解説を心がけるし、わからない点があれば遠慮なく質問してほしい
（答えられる範囲で回答するよう努力するので）。&lt;/p&gt;

&lt;p&gt;理論だけをやると理解しづらいし実感がわきにくいと思うので、できるだけ実例・応用例を見せながら進めたい（希望）。また、回毎に簡単な練習をつけたい（希望）。&lt;/p&gt;

&lt;p&gt;この分科会の最終目標として、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;KaggleのcompetitionにTSGの有志で参加したい&lt;/li&gt;
&lt;li&gt;機械学習を使って何らかしらの認識エンジンを作りたい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;という2つを据えておく（希望）。&lt;/p&gt;

&lt;p&gt;ちなみに、&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt;というのは統計解析のコンペティションで、様々な企業が「このデータを解析して欲しい！」といって賞金をつけ、世界中の企業や団体や個人が解析してスコアを競いあうサイトである。&lt;/p&gt;

&lt;h3&gt;本&lt;/h3&gt;

&lt;p&gt;参考書というか、僕が読んだ本・読みかけの本で良かったと思う本を挙げておく。
順番に意味はない。&lt;/p&gt;

&lt;h4&gt;自然科学の統計学&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4130420674&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
&lt;s&gt;最初から積読化されている本を挙げるのもどうかと思ったが&lt;/s&gt;多方面から良書であるとのレビューを聞いている。
教養の「基礎統計」からのつなぎに最適（基礎統計で使われる教科書の続編でもあるし）。&lt;/p&gt;

&lt;h4&gt;はじめてのパターン認識&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4627849710&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
一覧性に優れた本で、某書と違ってカバンに入れて持ち運ぶのにはとても適している良書。
説明もわかりやすい。&lt;/p&gt;

&lt;h4&gt;パターン認識と機械学習&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4621061224&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4621061240&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
鉄板。だが難しい。避けては通れない一冊。
levelfourは数学が苦手なので、4章くらいまでしかまだ読んでないです。&lt;/p&gt;

&lt;h4&gt;AIの衝撃&lt;/h4&gt;

&lt;iframe src=&quot;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=levelfour-22&amp;o=9&amp;p=8&amp;l=as4&amp;m=amazon&amp;f=ifr&amp;ref=ss_til&amp;asins=4062883074&quot; style=&quot;width:120px;height:240px;&quot; scrolling=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;今度は理論書ではなくて普通の新書。機械学習のバックグラウンドやこの先の展望が読みやすく書かれていて面白い。
機械学習を勉強する前に読めばよかったなあと思ってたり。ちなみに2015年3月刊行です。&lt;/p&gt;

&lt;h3&gt;研究室&lt;/h3&gt;

&lt;p&gt;折角東大にいるのだから、最先端の研究室に行って教授に話を聞いてみるとよい。というか本当にオススメ。&lt;/p&gt;

&lt;h4&gt;山西先生&lt;/h4&gt;

&lt;p&gt;計数工学科の先生。アルゴリズム寄りの研究者で、最近の興味は「潜在情報の抽出」だとか。
つまり、機械学習で得られたパラメータはコンピュータの計算したただの数字でしかないけど、その数字には具体的にどういう意味付けができるか、ということ。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://ibis.t.u-tokyo.ac.jp/yamanishiken/&quot;&gt;http://ibis.t.u-tokyo.ac.jp/yamanishiken/&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;杉山先生&lt;/h4&gt;

&lt;p&gt;情報科学科の先生。去年までは東工大にいらっしゃった。
研究自体はアルゴリズム寄り。「機械学習のアルゴリズムは様々なアプローチからおおよそ研究し尽くされたから、一段上のレイヤから抽象的にアルゴリズムを俯瞰する（要約、曲解あり）」ことを考えているそう。
あとは色々な企業でコンサルもなさってるそうです。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://www.ms.k.u-tokyo.ac.jp/index-jp.html&quot;&gt;http://www.ms.k.u-tokyo.ac.jp/index-jp.html&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;中山先生&lt;/h4&gt;

&lt;p&gt;情報理工学研究科の創造情報学の先生。画像認識・物体認識が専門で、世界的な画像認識のアカデミックコンペに毎年参加されているそうです。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://www.nlab.ci.i.u-tokyo.ac.jp/&quot;&gt;http://www.nlab.ci.i.u-tokyo.ac.jp/&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;中川先生&lt;/h4&gt;

&lt;p&gt;情報理工学研究科の数理情報学の先生。最近はプライバシー保護データマイニング(PPDM)がメイン。
「ビッグデータ使って統計するのはいいけど、データに紐付いた個人情報を上手く匿名化しないとマズイよね」的な話。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://www.r.dl.itc.u-tokyo.ac.jp/&quot;&gt;http://www.r.dl.itc.u-tokyo.ac.jp/&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;松尾先生&lt;/h4&gt;

&lt;p&gt;ウェブマイニング、人工知能の専門家。
最近メディアにもよく顔を出して本もたくさん執筆なさっています。書籍部にも平積みにしてあるはずなので読むと良いかも。&lt;/p&gt;

&lt;p&gt;URL: &lt;a href=&quot;http://weblab.t.u-tokyo.ac.jp/&quot;&gt;http://weblab.t.u-tokyo.ac.jp/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Apr 2015 21:06:00 +0900</pubDate>
        <link>http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/machine-learning-basics.html</link>
        <guid isPermaLink="true">http://sig.tsg.ne.jp/ml2015/ml/2015/04/26/machine-learning-basics.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
  </channel>
</rss>
